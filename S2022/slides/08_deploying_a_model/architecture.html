<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>17-445: Deploying a Model</title>
    <link rel="stylesheet" href="./../css/reveal.css" />
    <link rel="stylesheet" href="./../css/theme/white.css" id="theme" />
    <link rel="stylesheet" href="./../css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./../css/print/paper.css" type="text/css" media="print" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
     <script src="./../rplugin/spreadsheet/ruleJS.all.full.min.js"></script>
   <link rel="stylesheet" href="./../rplugin/spreadsheet/spreadsheet.css">
    <link rel="stylesheet" href="./../_assets/_assets/cmu.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><div id="footer">17-445 Machine Learning in Production, Christian Kaestner</div><section  data-markdown><script type="text/template">



# Deploying a Model

(Introduction to Software Architecture of AI-enabled Systems)

Christian Kaestner

<!-- references -->

Required reading: 
* üïÆ Hulten, Geoff. "[Building Intelligent Systems: A Guide to Machine Learning Engineering.](https://www.buildingintelligentsystems.com/)" Apress, 2018, Chapter 13 (Where Intelligence Lives).
* üì∞ Daniel Smith. "[Exploring Development Patterns in Data Science](https://www.theorylane.com/2017/10/20/some-development-patterns-in-data-science/)." TheoryLane Blog Post. 2017.

Recommended reading: Rick Kazman, Paul Clements, and Len Bass. [Software architecture in practice.](https://www.oreilly.com/library/view/software-architecture-in/9780132942799/?ar) Addison-Wesley Professional, 2012, Chapter 1

</script></section><section  data-markdown><script type="text/template">
# Learning Goals

* Understand important quality considerations when deploying ML components
* Follow a design process to explicitly reason about alternative designs and their quality tradeoffs
* Gather data to make informed decisions about what ML technique to use and where and how to deploy it
* Understand the power of design patterns for codifying design knowledge
*
* Create architectural models to reason about relevant characteristics
* Critique the decision of where an AI model lives (e.g., cloud vs edge vs hybrid), considering the relevant tradeoffs 
* Deploy models locally and to the cloud
* Document model inference services




</script></section><section ><section data-markdown><script type="text/template"># Deploying a Model is Easy
</script></section><section data-markdown><script type="text/template">## Deploying a Model is Easy

Model inference component as function/library

```python
from sklearn.linear_model import LogisticRegression
model = ‚Ä¶ # learn model or load serialized model ...
def infer(feature1, feature2):
    return model.predict(np.array([[feature1, feature2]])
```
</script></section><section data-markdown><script type="text/template">## Deploying a Model is Easy

Model inference component as a service


```python
from flask import Flask, escape, request
app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = '/tmp/uploads'
detector_model = ‚Ä¶ # load model‚Ä¶

# inference API that returns JSON with classes 
# found in an image
@app.route('/get_objects', methods=['POST'])
def pred():
    uploaded_img = request.files["images"]
    coverted_img = ‚Ä¶ # feature encoding of uploaded img
    result = detector_model(converted_img)
    return jsonify({"response":
                result['detection_class_entities']})

```
</script></section><section data-markdown><script type="text/template">## Deploying a Model is Easy

Packaging a model inference service in a container


```docker
FROM python:3.8-buster
RUN pip install uwsgi==2.0.20
RUN pip install numpy==1.22.0
RUN pip install tensorflow==2.7.0
RUN pip install flask==2.0.2
RUN pip install gunicorn==20.1.0
COPY models/model.pf /model/
COPY ./serve.py /app/main.py
WORKDIR ./app
EXPOSE 4040
CMD ["gunicorn", "-b 0.0.0.0:4040", "main:app"]
```
</script></section><section data-markdown><script type="text/template">## Deploying a Model is Easy

Model inference component as a service in the cloud

* Package in container or other infrastructure
* Deploy in cloud infrastructure
* Auto-scaling with demand
* MLOps infrastructure to automate all of this
* (more on this later)
*
* Model inference is stateless and embarrassingly parallel
* Almost always deterministic
* "*Stateless Serving Functions Pattern*"
* Lots of tooling available, including 
    * [BentoML](https://github.com/bentoml/BentoML) (low code service creation, deployment, model registry), 
    * [Cortex](https://github.com/bentoml/BentoML) (automated deployment and scaling of models on AWS), 
    * [TFX model serving](https://www.tensorflow.org/tfx/guide/serving) (tensorflow GRPC services)
    * [Seldon Core](https://www.seldon.io/tech/products/core/) (no-code model service and many many additional services for monitoring and operations on Kubernetes)

</script></section><section data-markdown><script type="text/template">## But is it really easy?

* Offline use?
* Deployment at scale?
* Hardware needs and operating cost?
* Frequent updates?
* Integration of the model into a system?
* Meeting system requirements?
* Every system is different!
</script></section><section data-markdown><script type="text/template">## Every System is Different

* Personalized music recommendations for Spotify
* Transcription service startup
* Self-driving car
* Smart keyboard for mobile device
</script></section><section data-markdown><script type="text/template">## Inference is a Component within a System

![Transcription service architecture example](transcriptionarchitecture2.svg)
<!-- .element: class="stretch plain" -->




</script></section></section><section ><section data-markdown><script type="text/template"># Software Architecture 

```mermaid
graph LR;
Requirements --> m((Miracle / genius developers))
m --> Implementation
```
</script></section><section data-markdown><script type="text/template">## So far: Requirements 

* Identify goals for the system, define success metrics
* Understand requirements, specifications, and assumptions
* Consider risks, plan for mitigations to mistakes
* Approaching component requirements: Understand quality requirements and constraints for models and learning algorithms

</script></section><section data-markdown><script type="text/template">
# Software Architecture 

```mermaid
graph LR;
Requirements --> Architecture
Architecture --> Implementation
```

Focused on reasoning about tradeoffs and desired qualities
</script></section><section data-markdown><script type="text/template">## From Requirements to Design/Architecture

![overview of the lecture content](overview.png)
<!-- .element: class="plain" -->
</script></section><section data-markdown><script type="text/template">## Software Architecture

> The software architecture of a program or computing system is the **structure or structures** of the system, which comprise **software elements**, the ***externally visible properties*** of those elements, and the relationships among them.
> -- [Kazman et al. 2012](https://www.oreilly.com/library/view/software-architecture-in/9780132942799/?ar)
 </script></section><section data-markdown><script type="text/template">## Recall: Systems Thinking

![](system.svg)
<!-- .element: class="plain" -->

> A system is a set of inter-related components that work together in a particular environment to perform whatever functions are required to achieve the system's objective -- Donella Meadows
</script></section><section data-markdown><script type="text/template">
## Why Architecture? ([Kazman et al. 2012](https://www.oreilly.com/library/view/software-architecture-in/9780132942799/?ar))

* Represents earliest design decisions.
* Aids in **communication** with stakeholders
    * Shows them ‚Äúhow‚Äù at a level they can understand, raising questions about whether it meets their needs
* Defines **constraints** on implementation
    * Design decisions form ‚Äúload-bearing walls‚Äù of application
* Dictates **organizational structure**
    * Teams work on different components
* Inhibits or enables **quality attributes**
    * Similar to design patterns
* Supports **predicting** cost, quality, and schedule
    * Typically by predicting information for each component
* Aids in software **evolution**
    * Reason about cost, design, and effect of changes
* Aids in **prototyping**
    * Can implement architectural skeleton early
</script></section><section data-markdown><script type="text/template">
## Case Study: Twitter

![twitter](twitter.png)

<aside class="notes"><p>Source and additional reading: Raffi. <a href="https://blog.twitter.com/engineering/en_us/a/2013/new-tweets-per-second-record-and-how.html">New Tweets per second record, and how!</a> Twitter Blog, 2013</p>
</aside></script></section><section data-markdown><script type="text/template">
## Twitter - Caching Architecture

![twitter](twitter-caching.png)
<!-- .element: class="stretch" -->

<aside class="notes"><ul>
<li>Running one of the world‚Äôs largest Ruby on Rails installations</li>
<li>200 engineers</li>
<li>Monolithic: managing raw database, memcache, rendering the site, and * presenting the public APIs in one codebase</li>
<li>Increasingly difficult to understand system; organizationally challenging to manage and parallelize engineering teams</li>
<li>Reached the limit of throughput on our storage systems (MySQL); read and write hot spots throughout our databases</li>
<li>Throwing machines at the problem; low throughput per machine (CPU + RAM limit, network not saturated)</li>
<li>Optimization corner: trading off code readability vs performance</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">
## Twitter's Redesign Goals

* Performance
    * Improve median latency; lower outliers 
    * Reduce number of machines 10x
+ Reliability
    * Isolate failures
+ Maintainability
    * "We wanted cleaner boundaries with ‚Äúrelated‚Äù logic being in one place": 
encapsulation and modularity at the systems level (rather than at the class, module, or package level)
* Modifiability
    * Quicker release of new features: "run small and empowered engineering teams that could make local decisions and ship user-facing changes, independent of other teams"

<!-- references -->

Raffi. [New Tweets per second record, and how!](https://blog.twitter.com/engineering/en_us/a/2013/new-tweets-per-second-record-and-how.html) Twitter Blog, 2013
</script></section><section data-markdown><script type="text/template">
## Twitter: Redesign Decisions

* Ruby on Rails -> JVM/Scala 
* Monolith -> Microservices
* RPC framework with monitoring, connection pooling, failover strategies, loadbalancing, ... built in
* New storage solution, temporal clustering, "roughly sortable ids"
* Data driven decision making

<!-- split -->

![Gizzard](gizzard.png)
<!-- .element: class="stretch" -->
</script></section><section data-markdown><script type="text/template">
## Twitter Case Study: Key Insights

* Architectural decisions affect entire systems, not only individual modules
* Abstract, different abstractions for different scenarios
* Reason about quality attributes early
* Make architectural decisions explicit
* Question: **Did the original architect make poor decisions?**

</script></section></section><section ><section data-markdown><script type="text/template">
# Architectural Modeling and Reasoning</script></section><section data-markdown><script type="text/template">![](pgh.jpg)
<aside class="notes"><p>Map of Pittsburgh. Abstraction for navigation with cars.</p>
</aside></script></section><section data-markdown><script type="text/template">![](pgh-cycling.jpg)
<aside class="notes"><p>Cycling map of Pittsburgh. Abstraction for navigation with bikes and walking.</p>
</aside></script></section><section data-markdown><script type="text/template">![](pgh-firezones.png)
<aside class="notes"><p>Fire zones of Pittsburgh. Various use cases, e.g., for city planners.</p>
</aside></script></section><section data-markdown><script type="text/template">## Analysis-Specific Abstractions

* All maps were abstractions of the same real-world construct
* All maps were created with different goals in mind
    - Different relevant abstractions
    - Different reasoning opportunities
* 
* Architectural models are specific system abstractions, for reasoning about specific qualities
* No uniform notation
</script></section><section data-markdown><script type="text/template">
## What can we reason about?

![](lan-boundary.png)
<!-- .element: class="stretch" -->
</script></section><section data-markdown><script type="text/template">
## What can we reason about?

![](gfs.png)

<!-- references -->
Ghemawat, Sanjay, Howard Gobioff, and Shun-Tak Leung. "[The Google file system.](https://ai.google/research/pubs/pub51.pdf)" ACM SIGOPS operating systems review. Vol. 37. No. 5. ACM, 2003.

<aside class="notes"><p>Scalability through redundancy and replication; reliability wrt to single points of failure; performance on edges; cost</p>
</aside></script></section><section data-markdown><script type="text/template">## What can we reason about?

![Apollo Self-Driving Car Architecture](apollo.png)

<!-- references -->
Peng, Zi, Jinqiu Yang, Tse-Hsun Chen, and Lei Ma. "A first look at the integration of machine learning models in complex autonomous driving systems: a case study on Apollo." In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 1240-1250. 2020.
</script></section><section data-markdown><script type="text/template">
## Suggestions for Graphical Notations

* Use notation suitable for analysis
* Document meaning of boxes and edges in legend
* Graphical or textual both okay; whiteboard sketches often sufficient
* Formal notations available














</script></section></section><section ><section data-markdown><script type="text/template">
# Case Study: Augmented Reality Translation


![Seoul Street Signs](seoul.jpg)
<!-- .element: class="stretch" -->


<aside class="notes"><p>Image: <a href="https://pixabay.com/photos/nightlife-republic-of-korea-jongno-2162772/">https://pixabay.com/photos/nightlife-republic-of-korea-jongno-2162772/</a></p>
</aside></script></section><section data-markdown><script type="text/template">## Case Study: Augmented Reality Translation
![Google Translate](googletranslate.png)</script></section><section data-markdown><script type="text/template">## Case Study: Augmented Reality Translation
![Google Glasses](googleglasses.jpg)
<aside class="notes"><p>Consider you want to implement an instant translation service similar toGoogle translate, but run it on embedded hardware in glasses as an augmented reality service.</p>
</aside></script></section><section data-markdown><script type="text/template">## System Qualities of Interest?

<!-- discussion -->

</script></section></section><section  data-markdown><script type="text/template"># Design Decision: Selecting ML Algorithms

What ML algorithms to use and why? Tradeoffs?

![](googletranslate.png)

<aside class="notes"><p>Relate back to previous lecture about AI technique tradeoffs, including for example
Accuracy
Capabilities (e.g. classification, recommendation, clustering‚Ä¶)
Amount of training data needed
Inference latency
Learning latency; incremental learning?
Model size
Explainable? Robust?</p>
</aside></script></section><section ><section data-markdown><script type="text/template"># Design Decision: Where Should the Model Live?

(Deployment Architecture)
</script></section><section data-markdown><script type="text/template">## Where Should the Models Live?

![AR Translation Architecture Sketch](ar-architecture.svg)
<!-- .element: class="plain" -->

Cloud? Phone? Glasses?

What qualities are relevant for the decision?

<aside class="notes"><p>Trigger initial discussion</p>
</aside></script></section><section data-markdown><script type="text/template">## Considerations

* How much data is needed as input for the model?
* How much output data is produced by the model?
* How fast/energy consuming is model execution?
* What latency is needed for the application?
* How big is the model? How often does it need to be updated?
* Cost of operating the model? (distribution + execution)
* Opportunities for telemetry?
* What happens if users are offline?
</script></section><section data-markdown><script type="text/template">## Breakout: Latency and Bandwidth Analysis of AR Translation

1. Estimate latency and bandwidth requirements between components

2. Discuss tradeoffs among different deployment models

![AR Translation Architecture Sketch](ar-architecture.svg)
<!-- .element: class="plain stretch" -->

Post on Slack in `#lecture`:
* Recommended deployment for OCR (with justification):
* Recommended deployment for Translation (with justification):




<aside class="notes"><p>Identify at least OCR and Translation service as two AI components in a larger system. Discuss which system components are worth modeling (e.g., rendering, database, support forum). Discuss how to get good estimates for latency and bandwidth.</p>
<p>Some data:
200ms latency is noticable as speech pause; 
20ms is perceivable as video delay, 10ms as haptic delay;
5ms referenced as cybersickness threshold for virtual reality
20ms latency might be acceptable</p>
<p>bluetooth latency around 40ms to 200ms</p>
<p>bluetooth bandwidth up to 3mbit, wifi 54mbit, video stream depending on quality 4 to 10mbit for low to medium quality</p>
<p>google glasses had 5 megapixel camera, 640x360 pixel screen, 1 or 2gb ram, 16gb storage</p>
</aside></script></section><section data-markdown><script type="text/template">
![Example of an architectural diagram](arch-diagram-example.png)
<!-- .element: class="stretch plain" -->

</script></section><section data-markdown><script type="text/template">## From the Reading: When would one use the following designs?

* Static intelligence in the product
* Client-side intelligence (user-facing devices)
* Server-centric intelligence
* Back-end cached intelligence
* Hybrid models
*
* Consider: Offline use, inference latency, model updates, application updates, operating cost, scalability, protecting intellectual property

<!-- discussion -->

<aside class="notes"><p>From the reading:</p>
<ul>
<li>Static intelligence in the product<ul>
<li>difficult to update</li>
<li>good execution latency</li>
<li>cheap operation</li>
<li>offline operation</li>
<li>no telemetry to evaluate and improve</li>
</ul>
</li>
<li>Client-side intelligence<ul>
<li>updates costly/slow, out of sync problems</li>
<li>complexity in clients</li>
<li>offline operation, low execution latency</li>
</ul>
</li>
<li>Server-centric intelligence<ul>
<li>latency in model execution (remote calls)</li>
<li>easy to update and experiment</li>
<li>operation cost</li>
<li>no offline operation</li>
</ul>
</li>
<li>Back-end cached intelligence<ul>
<li>precomputed common results</li>
<li>fast execution, partial offline </li>
<li>saves bandwidth, complicated updates</li>
</ul>
</li>
<li>Hybrid models</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">## Where Should Feature Encoding Happen?

![Feature Encoding](featureencoding.svg)
<!-- .element: class="plain" -->

*Should feature encoding happen server-side or client-side? Tradeoffs?*

<aside class="notes"><p>When thinking of model inference as a component within a system, feature encoding can happen with the model-inference component or can be the responsibility of the client. That is, the client either provides the raw inputs (e.g., image files; dotted box in the figure above) to the inference service or the client is responsible for computing features and provides the feature vector to the inference service (dashed box). Feature encoding and model inference could even be two separate services that are called by the client in sequence. Which alternative is preferable is a design decision that may depend on a number of factors, for example, whether and how the feature vectors are stored in the system, how expensive computing the feature encoding is, how often feature encoding changes, how many models use the same feature encoding, and so forth. For instance, in our stock photo example, having feature encoding being part of the inference service is convenient for clients and makes it easy to update the model without changing clients, but we would have to send the entire image over the network instead of just the much smaller feature vector for the reduced 300 x 300 pixels.</p>
</aside></script></section><section data-markdown><script type="text/template">## Reusing Feature Engineering Code


![Feature encoding shared between training and inference](shared-feature-encoding.svg)
<!-- .element: class="plain" -->


Avoid *training‚Äìserving skew*
</script></section><section data-markdown><script type="text/template">## The Feature Store Pattern

* Central place to store, version, and describe feature engineering code
* Can be reused across projects
* Possible caching of expensive features


Many open source and commercial offerings, e.g.,  Feast, Tecton, AWS SageMaker Feature Store
</script></section><section data-markdown><script type="text/template">## Tecton Feature Store

<iframe width="560" height="315" src="https://www.youtube.com/embed/u_L_V2HQ_nQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</script></section><section data-markdown><script type="text/template">## More Considerations for Deployment Decisions

* Coupling of ML pipeline parts
* Coupling with other parts of the system
* Ability for different developers and analysts to collaborate
* Support online experiments
* Ability to monitor

</script></section><section data-markdown><script type="text/template">## Real-Time Serving; Many Models

![Apollo Self-Driving Car Architecture](apollo.png)

<!-- references -->
Peng, Zi, Jinqiu Yang, Tse-Hsun Chen, and Lei Ma. "A first look at the integration of machine learning models in complex autonomous driving systems: a case study on Apollo." In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 1240-1250. 2020.

</script></section><section data-markdown><script type="text/template">## Infrastructure Planning (Facebook Example)

![Example of Facebook‚Äôs Machine Learning Flow and Infrastructure](facebook-flow.png)

<!-- references -->

Hazelwood, Kim, Sarah Bird, David Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy et al. "Applied machine learning at facebook: A datacenter infrastructure perspective." In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 620-629. IEEE, 2018.
</script></section><section data-markdown><script type="text/template">## Capacity Planning (Facebook Example)

<!-- small -->

| Services | Relative Capacity | Compute | Memory |
|--|--|--|--|
| News Feed | 100x | Dual-Socket CPU | High |
| Facer (face recognition) | 10x | Single-Socket CPU | Low |
| Lumos (image understanding) | 10x | Single-Socket CPU | Low |
| Search | 10x | Dual-Socket CPU | High |
| Lang. Translation | 1x | Dual-Socket CPU | High |
| Sigma (anomaly and spam detection) | 1x | Dual-Socket CPU | High |
| Speech Recognition | 1x | Dual-Socket CPU | High |

Trillions of inferences per day, in real time

Preference for cheap single-CPU machines whether possible

Different latency requirements, some "nice to have" predictions

Some models run on mobile device to improve latency and reduce communication cost

<!-- references -->

Hazelwood, Kim, Sarah Bird, David Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy et al. "Applied machine learning at facebook: A datacenter infrastructure perspective." In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 620-629. IEEE, 2018.

</script></section><section data-markdown><script type="text/template">## Operational Robustness

* Redundancy for availability?
* Load balancer for scalability?
* Can mistakes be isolated?
    - Local error handling?
    - Telemetry to isolate errors to component?
* Logging and log analysis for what qualities?


</script></section></section><section ><section data-markdown><script type="text/template"># Preview: Telemetry Design
</script></section><section data-markdown><script type="text/template">## Telemetry Design

How to evaluate system performance and mistakes in production?

![](googletranslate.png)

<aside class="notes"><p>Discuss strategies to determine accuracy in production. What kind of telemetry needs to be collected?</p>
</aside></script></section><section data-markdown><script type="text/template">## The Right and Right Amount of Telemetry

* Purpose:
    - Monitor operation
    - Monitor mistakes (e.g., accuracy)
    - Improve models over time (e.g., detect new features)
*
* Challenges:
    - too much data
    - no/not enough data
    - hard to measure, poor proxy measures
    - rare events
    - cost
    - privacy
*
* **Interacts with deployment decisions**
</script></section><section data-markdown><script type="text/template">## Telemetry Tradeoffs

What data to collect? How much? When?

Estimate data volume and possible bottlenecks in system.

![](googletranslate.png)

<aside class="notes"><p>Discuss alternatives and their tradeoffs. Draw models as suitable.</p>
<p>Some data for context:
Full-screen png screenshot on Pixel 2 phone (1080x1920) is about 2mb (2 megapixel); Google glasses had a 5 megapixel camera and a 640x360 pixel screen, 16gb of storage, 2gb of RAM. Cellar cost are about $10/GB.</p>
</aside></script></section></section><section ><section data-markdown><script type="text/template"># Integrating Models into a System
</script></section><section data-markdown><script type="text/template">## Recall: Inference is a Component within a System

![Transcription service architecture example](transcriptionarchitecture2.svg)
<!-- .element: class="plain stretch" -->
</script></section><section data-markdown><script type="text/template">## Separating Models and Business Logic

![3-tier architecture integrating ML](3tier-with-ml.svg)
<!-- .element: class="stretch plain" -->

Based on: Yokoyama, Haruki. "Machine learning system architectural pattern for improving operational stability." In 2019 IEEE International Conference on Software Architecture Companion (ICSA-C), pp. 267-274. IEEE, 2019.
</script></section><section data-markdown><script type="text/template">## Separating Models and Business Logic

* Clearly divide responsibilities
* Allows largely independent and parallel work, assuming stable interfaces
* Plan location of non-ML safeguards and other processing logic


</script></section><section data-markdown><script type="text/template">## Composing Models: Ensemble and metamodels

![Ensemble models](ensemble.svg)
<!-- .element: class="plain" -->
</script></section><section data-markdown><script type="text/template">## Composing Models: Decomposing the problem, sequential

![](sequential-model-composition.svg)
<!-- .element: class="plain" -->
</script></section><section data-markdown><script type="text/template">## Composing Models: Cascade/two-phase prediction

![](2phase-prediction.svg)
<!-- .element: class="plain" -->








</script></section></section><section ><section data-markdown><script type="text/template"># Documenting Model Inference Interfaces


</script></section><section data-markdown><script type="text/template">## Why Documentation

* Model inference between teams:
  * Data scientists developing the model
  * Other data scientists using the model, evolving the model
  * Software engineers integrating the model as a component
  * Operators managing model deployment
* Will this model work for my problem?
* What problems to anticipate?
</script></section><section data-markdown><script type="text/template">## Classic API Documentation


```java
/**
 * compute deductions based on provided adjusted 
 * gross income and expenses in customer data.
 *
 * see tax code 26 U.S. Code A.1.B, PART VI
 */
float computeDeductions(float agi, Expenses expenses);
```


</script></section><section data-markdown><script type="text/template">## What to Document for Models?

<!-- discussion -->
</script></section><section data-markdown><script type="text/template">## Documenting Input/Output Types for Inference Components

```js
{
  "mid": string,
  "languageCode": string,
  "name": string,
  "score": number,
  "boundingPoly": {
    object (BoundingPoly)
  }
}
```
From Google‚Äôs public [object detection API](https://cloud.google.com/vision/docs/object-localizer).
</script></section><section data-markdown><script type="text/template">## Documentation beyond Input/Output Types

* Intended use cases, model capabilities and limitations
* Supported target distribution (vs preconditions)
* Accuracy (various measures), incl. slices, fairness
* Latency, throughput, availability (service level agreements)
* Model qualities such as explainability, robustness, calibration
* Ethical considerations (fairness, safety, security, privacy)


**Example for OCR model? How would you describe these?**
</script></section><section data-markdown><script type="text/template">## Model Cards 

* Proposal and template for documentation from Google
* 1-2 page summary
* Focused on fairness
* Includes
  * Intended use, out-of-scope use
  * Training and evaluation data
  * Considered demographic factors
  * Accuracy evaluations
  * Ethical considerations
* Widely discussed, but not frequently adopted

<!-- references -->
Mitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. "[Model cards for model reporting](https://arxiv.org/abs/1810.03993)." In *Proceedings of the conference on fairness, accountability, and transparency*, pp. 220-229. 2019.
</script></section><section data-markdown><script type="text/template">![Model card example](modelcard.png)
<!-- .element: class="stretch" -->

Example from Model Cards paper
</script></section><section data-markdown><script type="text/template">![Model card screenshot from Google](modelcard2.png)
<!-- .element: class="stretch" -->

From: https://modelcards.withgoogle.com/object-detection
</script></section><section data-markdown><script type="text/template">## FactSheets 

* Proposal and template for documentation from IBM
* Intended to communicate intended qualities and assurances
* Longer list of criteria, including
  * Service intention
  * Technical description
  * Intended use
  * Target distribution
  * Own and third-party evaluation results
  * Safety and fairness considerations
  * Explainability
  * Preparation for drift and evolution
  * Security
  * Lineage and versioning


<!-- references -->
Arnold, Matthew, Rachel KE Bellamy, Michael Hind, Stephanie Houde, Sameep Mehta, Aleksandra Mojsiloviƒá, Ravi Nair, Karthikeyan Natesan Ramamurthy, Darrell Reimer, Alexandra Olteanu, David Piorkowski, Jason Tsay, and Kush R. Varshney. "[FactSheets: Increasing trust in AI services through supplier's declarations of conformity](https://arxiv.org/pdf/1808.07261.pdf)." *IBM Journal of Research and Development* 63, no. 4/5 (2019): 6-1.
</script></section><section data-markdown><script type="text/template">## Recall: Correctness vs Fit

* Without a clear specification a model is difficult to document
* Need documentation to allow evaluation for *fit*
* Description of *target distribution* is a key challenge











</script></section></section><section ><section data-markdown><script type="text/template"># Design Patterns for AI Enabled Systems

(no standardization, *yet*)
</script></section><section data-markdown><script type="text/template">## Design Patterns are Codified Design Knowledge

Vocabulary of design problems and solutions


![Observer pattern](observer.png)

Example: *Observer pattern* object-oriented design pattern describes a solution how objects can be notified when another object changes without strongly coupling these objects to each other
</script></section><section data-markdown><script type="text/template">## Common System Structures

* Client-server architecture
* Multi-tier architecture
* Service-oriented architecture and microservices
* Event-based architecture
* Data-flow architecture
</script></section><section data-markdown><script type="text/template">## Multi-Tier Architecture

![3-tier architecture integrating ML](3tier-with-ml.svg)
<!-- .element: class="stretch plain" -->

Based on: Yokoyama, Haruki. "Machine learning system architectural pattern for improving operational stability." In 2019 IEEE International Conference on Software Architecture Companion (ICSA-C), pp. 267-274. IEEE, 2019.
</script></section><section data-markdown><script type="text/template">## Microservices

![Microservice illustration](microservice.svg)
<!-- .element: class="stretch plain" -->


(more later)


</script></section><section data-markdown><script type="text/template">## Patterns for ML-Enabled Systems

* Stateless/serverless Serving Function Pattern
* Feature-Store Pattern
* Batched/precomuted serving pattern
* Two-phase prediction pattern
* Batch Serving Pattern
* Decouple-training-from-serving pattern

</script></section><section data-markdown><script type="text/template">## Anti-Patterns

* Big Ass Script Architecture
* Dead Experimental Code Paths
* Glue code
* Multiple Language Smell
* Pipeline Jungles
* Plain-Old Datatype Smell
* Undeclared Consumers



<!-- references -->
* Washizaki, Hironori, Hiromu Uchida, Foutse Khomh, and Yann-Ga√´l Gu√©h√©neuc. "[Machine Learning Architecture and Design Patterns](http://www.washi.cs.waseda.ac.jp/wp-content/uploads/2019/12/IEEE_Software_19__ML_Patterns.pdf)." Draft, 2019
* Sculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan Dennison. "[Hidden technical debt in machine learning systems](http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)." In Advances in neural information processing systems, pp. 2503-2511. 2015.













</script></section></section><section  data-markdown><script type="text/template">
# Summary

* Model deployment seems easy, but involves many design decisions
    * What models to use?
    * Where to deploy?
    * How to design feature encoding and feature engineering?
    * How to compose with other components?
    * How to document?
    * How to collect telemetry?
* Software architecture is an established discipline to reason about design alternatives
* Understand relevant quality goals 
* Problem-specific modeling and analysis: Gather estimates, consider design alternatives, make tradeoffs explicit
* Codifying design knowledge as patterns



</script></section></div>
    </div>

    <script src="./../js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './../plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './../plugin/zoom-js/zoom.js', async: true },
        { src: './../plugin/notes/notes.js', async: true },
        { src: './../plugin/math/math.js' },
        { src: './../rplugin/embed-tweet/embed-tweet.js' },
        { src: './../rplugin/menu/menu.js', async: true },
        { src: './../rplugin/spreadsheet/spreadsheet.js' },
        { src: './../rplugin/chalkboard/chalkboard.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps,
	chalkboard: { // font-awesome.min.css must be available
		toggleChalkboardButton: { left: "80px" },
		toggleNotesButton: { left: "130px" },
	},
	keyboard: {
	    67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle notes canvas when 'c' is pressed
	    66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
	    46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
	     8: function() { RevealChalkboard.reset() },	// reset chalkboard data on current slide when 'BACKSPACE' is pressed
	    68: function() { RevealChalkboard.download() },	// downlad recorded chalkboard drawing when 'd' is pressed
	    88: function() { RevealChalkboard.colorNext() },	// cycle colors forward when 'x' is pressed
	    89: function() { RevealChalkboard.colorPrev() },	// cycle colors backward when 'y' is pressed
	}
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"theme":"white","slideNumber":true,"hash":true,"center":false}, queryOptions);
    </script>

    <script src="./../_assets/_assets/mermaid.min.js"></script>
    <script src="./../_assets/_assets/viz.js"></script>
    <script src="./../_assets/_assets/loadmymarkdown.js"></script>

    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
