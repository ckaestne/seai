<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>Software Engineering for ML-Enabled Systems</title>
    <link rel="stylesheet" href="./../css/reveal.css" />
    <link rel="stylesheet" href="./../css/theme/white.css" id="theme" />
    <link rel="stylesheet" href="./../css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./../css/print/paper.css" type="text/css" media="print" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
     <script src="./../rplugin/spreadsheet/ruleJS.all.full.min.js"></script>
   <link rel="stylesheet" href="./../rplugin/spreadsheet/spreadsheet.css">
    <link rel="stylesheet" href="./../_assets/_assets/cmu.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><div id="footer">Software Engineering for ML-Enabled Systems, Christian Kaestner</div><section  data-markdown><script type="text/template">

# Software Engineering for ML-Enabled Systems

Christian Kästner

Carnegie Mellon University


https://github.com/ckaestne/seai
</script></section><section ><section data-markdown><script type="text/template">
## Software Engineering for ML-Enabled Systems

> Building, operating, and maintaining software systems with machine-learned components

> with interdisciplinary collaborative teams of 
**data scientists** and **software engineers** 

</script></section><section data-markdown><script type="text/template">## SE for ML-Enabled Systems != Building models

![Notebook](notebook.png)

</script></section><section data-markdown><script type="text/template">

## SE for ML-Enabled Systems != coding ML frameworks

![SciKit Learn Logo](scikit.png)


</script></section><section data-markdown><script type="text/template">

## SE for ML-Enabled Systems != ML for SE Tools

![Code Completion with AI](codecompl.png)

</script></section><section data-markdown><script type="text/template">## SE for ML-Enabled (AI-ML-based, ML-infused) Systems

![](temi.png)
[temi.com](https://www.temi.com/)
</script></section><section data-markdown><script type="text/template">## SE for ML-Enabled (AI-ML-based, ML-infused) Systems

![](powerpoint.png)
</script></section><section data-markdown><script type="text/template">## SE for ML-Enabled (AI-ML-based, ML-infused) Systems

![](cancerpred.png)
![](gnuhealth.png)


</script></section></section><section ><section data-markdown><script type="text/template">
<svg version="1.1" viewBox="0.0 0.0 800 400" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg">
	<style>
    text { font: 60px sans-serif; }
  	</style>
	<circle r="180" cx="180", cy="200" fill="#b9ff00" fill-opacity="0.514" />
	<circle r="180" cx="620", cy="200" fill="#ff5500" fill-opacity="0.514" />
	<text x=180 y=160 dominant-baseline="middle" text-anchor="middle">Data</text>
	<text x=180 y=240 dominant-baseline="middle" text-anchor="middle">Scientists</text>
	<text x=620 y=160 dominant-baseline="middle" text-anchor="middle">Software</text>
	<text x=620 y=240 dominant-baseline="middle" text-anchor="middle">Engineers</text>
</svg>

and domain experts + lawyers + operators + security experts + regulators + ...
</script></section><section data-markdown><script type="text/template">## Software Engineering

> Software engineering is the branch of computer science that creates practical, cost-effective solutions to computing and information processing problems, preferentially by applying scientific knowledge, developing software systems in the service of mankind. 

Engineering judgements under limited information and resources

A focus on design, tradeoffs, and the messiness of the real world

Many qualities of concern: cost, correctness, performance, scalability, security, maintainability, ...



**"it depends..."**


<!-- references -->
Mary Shaw. ed. [Software Engineering for the 21st Century: A basis for rethinking the curriculum](https://www.cs.cmu.edu/~Compose/SEprinciples-pub-rev2.pdf). 2005.

</script></section><section data-markdown><script type="text/template">## Most ML Courses/Talks

Focus narrowly on modeling techniques or building models

Using notebooks, static datasets, evaluating accuracy

Little attention to software engineering aspects of building complete systems

![Notebook](notebook.png)
</script></section><section data-markdown><script type="text/template">## Data scientist

* Often fixed dataset for training and evaluation 
* Focused on accuracy
* Prototyping, often Jupyter notebooks or similar 
* Expert in modeling techniques and feature engineering
* Model size, updateability, implementation stability typically does not matter
* Starting to worry about fairness, robustness, ...

<!-- split -->

## Software engineer

* Builds a product
* Concerned about cost, performance, stability, release time
* Identify quality through customer satisfaction
* Must scale solution, handle large amounts of data
* Plan for mistakes and safeguards
* Maintain, evolve, and extend the product over long periods
* Consider requirements for security, safety, fairness
</script></section><section data-markdown><script type="text/template">
<svg version="1.1" viewBox="0.0 0.0 800 400" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg">
	<style>
    text { font: 60px sans-serif; }
  	</style>
	<circle r="180" cx="180", cy="200" fill="#b9ff00" fill-opacity="0.514" />
	<circle r="180" cx="620", cy="200" fill="#ff5500" fill-opacity="0.514" />
	<text x=180 y=160 dominant-baseline="middle" text-anchor="middle">Data</text>
	<text x=180 y=240 dominant-baseline="middle" text-anchor="middle">Scientists</text>
	<text x=620 y=160 dominant-baseline="middle" text-anchor="middle">Software</text>
	<text x=620 y=240 dominant-baseline="middle" text-anchor="middle">Engineers</text>
</svg>
</script></section><section data-markdown><script type="text/template">
![](temi.png)
</script></section><section data-markdown><script type="text/template">
![](transcriptionarchitecture2.png)
<!-- .element: class="plain" -->


</script></section></section><section ><section data-markdown><script type="text/template">
# A Software Engineering Perspective on ML
</script></section><section data-markdown><script type="text/template">## What's different?


* Missing specifications
* Environment is important (feedback loops, data drift)
* Nonlocal and nonmonotonic effects 
* Testing in production
* Data management, versioning, and provenance

</script></section><section data-markdown><script type="text/template">## Managing Complexity in Software

* **Abstraction**: Hide details & focus on high-level behaviors
* **Reuse**: Package into reusable libraries & APIs with well-defined _contracts_
* **Composition**: Build large components out of smaller ones

```java
/**
 * compute deductions based on provided adjusted 
 * gross income and expenses in customer data.
 *
 * see tax code 26 U.S. Code A.1.B, PART VI
 *
 * Adjusted gross income must be positive; 
 * returned deductions are not negative.
 */
float computeDeductions(float agi, Expenses expenses) {
  ...
}
```
</script></section><section data-markdown><script type="text/template">## Missing Specifications

*from deductive to inductive reasoning, from specs to examples*

```java
/**
  ????
*/
String transcribe(File audioFile);
```

```java
/**
  ????
*/
Boolean predictRecidivism(int age, 
                          List<Crime> priors, 
                          Gender gender, 
                          int timeServed,
                          ...);
```

```java
/**
  ????
*/
Boolean hasCancer(byte[][] image);
```


</script></section><section data-markdown><script type="text/template">

> All models are approximations. Assumptions, whether implied or clearly stated, are never exactly true. **All models are wrong, but some models are useful**. So the question you need to ask is not "Is the model true?" (it never is) but "Is the model good enough for this particular application?" -- George Box


<!-- references -->
See also https://en.wikipedia.org/wiki/All_models_are_wrong

</script></section><section data-markdown><script type="text/template">## Non-ML Example: Newton's Laws of Motion

> 2nd law: "the rate of change of momentum of a body over time is directly proportional to the force applied, and occurs in the same direction as the applied force" 
> ${\displaystyle \mathbf {F} ={\frac {\mathrm {d} \mathbf {p} }{\mathrm {d} t}}}$

"Newton's laws were verified by experiment and observation for over 200 years, and they are excellent approximations at the scales and speeds of everyday life."

Do not generalize for very small scales, very high speeds, or in very strong gravitational fields. Do not explain semiconductor, GPS errors, superconductivity, ... Those require general relativity and quantum field theory.

<!-- references -->
Further readings: https://en.wikipedia.org/wiki/Newton%27s_laws_of_motion
</script></section><section data-markdown><script type="text/template">
 
> "Since all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad." -- George Box, 1976


<!-- references -->
See also https://en.wikipedia.org/wiki/All_models_are_wrong



</script></section><section data-markdown><script type="text/template">## Environment is Important 

*(feedback loops, data drift, safety concerns)*

![Flatearth videos on Youtube](flatearth.png)
</script></section><section data-markdown><script type="text/template">## Nonlocal and Nonmonotonic Effects

*multiple models in most systems*

```mermaid
graph LR
Camera(Camera) --> LP[LanePrediction]
SS(SteeringStatus) --> SteeringPlanning
LP --> SteeringPlanning
SteeringPlanning --> Guardian
Guardian --> SA(SteeringActuators)
Guardian --> B(Beeper)
GyroSensor(GyroSensor) --> SteeringPlanning
GyroSensor --> Guardian
```
</script></section><section data-markdown><script type="text/template">## Testing in production

![Tay chat bot](tay.png)
</script></section><section data-markdown><script type="text/template">## Data management, versioning, and provenance

![Lambda Architecture](lambdaml.png)
<!-- .element: class="stretch plain" --> 
</script></section><section data-markdown><script type="text/template">## But Really Different?

</script></section><section data-markdown><script type="text/template">## ML: Missing Specifications

*from deductive to inductive reasoning*

```java
/**
  ????
*/
String transcribe(File audioFile);
```

```java
/**
  ????
*/
Boolean predictRecidivism(int age, 
                          List<Crime> priors, 
                          Gender gender, 
                          int timeServed,
                          ...);
```

```java
/**
  ????
*/
Boolean hasCancer(byte[][] image);
```

<!-- split -->

## Software Engineering:

vague specs very common

agile methods

safe systems from unreliable components

(["ML is requirements engineering"](https://medium.com/@ckaestne/machine-learning-is-requirements-engineering-8957aee55ef4))
</script></section><section data-markdown><script type="text/template">## ML: Environment is Important 

*(feedback loops, data drift)*

![Flatearth videos on Youtube](flatearth.png)


<!-- split -->

## Software Engineering:

*the world and the machine*

![](machine-world.png)
<!-- .element: class="plain" -->
 
(Jackson ICSE 95)
</script></section><section data-markdown><script type="text/template">## ML: Nonmonotonic Effects

*multiple models in most systems*

```mermaid
graph LR
Camera(Camera) --> LP[LanePrediction]
SS(SteeringStatus) --> SteeringPlanning
LP --> SteeringPlanning
SteeringPlanning --> Guardian
Guardian --> SA(SteeringActuators)
Guardian --> B(Beeper)
GyroSensor(GyroSensor) --> SteeringPlanning
GyroSensor --> Guardian
```

<!-- split -->

## Software Engineering:

*feature interactions*

*system testing*

![](interactiontelecom.png)


</script></section><section data-markdown><script type="text/template">## ML: Testing in production

![Tay chat bot](tay.png)

<!-- split -->

## Software Engineering:

Chaos engineering, A/B testing, continuous deployment, feature flags, canary releases

![](ab-button.png)

</script></section><section data-markdown><script type="text/template">## ML: Data management, versioning, and provenance

![Lambda Architecture](lambdaml.png)
<!-- .element: class="stretch" --> 


<!-- split -->

## SE/Database communities:

*stream processing*

*event sourcing*

*data modeling*

*data flow models*

*provenance tracking*


</script></section><section data-markdown><script type="text/template">## Software Engineers in AI-Enabled System Projects

* Missing specifications -- *implicit, vague specs very common; safe systems from unreliable components* 
* Environment is important -- *the world vs the machine* 
* Nonlocal and nonmonotonic effects -- *feature interactions, system testing* 
* Testing in production -- *continuous deployment, A/B testing*
* Data management, versioning, and provenance -- *stream processing, event sourcing, data modeling*


</script></section><section data-markdown><script type="text/template">## My View 

> While developers of simple traditional systems may get away with poor practices, most developers of ML-enabled systems will not.

</script></section></section><section ><section data-markdown><script type="text/template"># Quality Assurance for ML-enabled Systems

> Illustrating software engineering and systems concerns by diving into one problem
</script></section><section data-markdown><script type="text/template">## Traditional Focus: Model Accuracy

* Train and evaluate model on fixed labled data set
* Compare prediction with labels

![Cancer prognosis](cancerpred.png)
<!-- .element: class="plain" -->

</script></section><section data-markdown><script type="text/template">## Traditional Focus: Model Accuracy

| | **Actually A** | **Actually not A** |
| --- | --- | --- |
|**AI predicts A** | True Positive (TP) | False Positive (FP) |
|**AI predicts not A** | False Negative (FN) | True Negative (TN) |

Accuary, Recall, Precision, F1-Score
</script></section><section data-markdown><script type="text/template">## More Traditional Model Quality Discussions

<!-- colstart -->
Many model quality metrics 
(recall, MAPE, ROC, log loss, top-k, ...)

![Recall/Precision Plot](prcurve.png)
<!-- col -->
Generalization/overfitting (train/test/eval split, crossvalidation)

![Overfitting example](overfitting-error.png)
<small>(CC SA 3.0 by [Dake](https://commons.wikimedia.org/wiki/File:Overfitting.png))</small>

<!-- colend -->

</script></section><section data-markdown><script type="text/template">## Not all Mistakes are Equal

* False positives vs false negatives (e.g., cancer detection)
* Fairness across subpopulations
* Generalization beyond one device and one hospital?
* 
* Learn from black-box testing:
	- Equivalence classes
	- Boundary conditions
	- Critical test cases ("call mom")
	- Combinatorial testing
	- Fuzzing
</script></section><section data-markdown><script type="text/template">## Automating Model Evaluation

* Continuous integration, automated measurement, tracking of results
* Data and model versioning, provenance

![Uber dashboard](uber-dashboard.png)

<!-- references  -->

Jeremy Hermann and Mike Del Balso. [Meet Michelangelo: Uber’s Machine Learning Platform](https://eng.uber.com/michelangelo/). Blog, 2017

</script></section><section data-markdown><script type="text/template">## Beyond Accuracy: 
## Quality concerns for ML-Enabled Systems

* Learning time, cost and scalability
* Update cost, incremental learning
* Inference cost
* Size of models learned
* Amount of training data needed
* Fairness
* Robustness
* Safety, security, privacy
* Explainability, reproducibility
* Time to market
* Overall operating cost (cost per prediction)
</script></section><section data-markdown><script type="text/template">![Temi](temi.png)










</script></section></section><section ><section data-markdown><script type="text/template"># Infrastructure Quality
</script></section><section data-markdown><script type="text/template">## Think of Pipelines, not Models, not Notebooks

![Pipeline](pipeline.png)
<!-- .element: class="plain" -->

Many steps: Data collection, data cleaning, labeling, feature engineering, training, evaluation, deployment, monitoring

Automate each step -- test each step

<!-- references -->

Graphic: Amershi, Saleema, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. "[Software engineering for machine learning: A case study](https://www.microsoft.com/en-us/research/uploads/prod/2019/03/amershi-icse-2019_Software_Engineering_for_Machine_Learning.pdf)." In 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), pp. 291-300. IEEE, 2019.
</script></section><section data-markdown><script type="text/template">## Possible Mistakes in ML Pipelines

Danger of "silent" mistakes in many phases:

* Dropped data after format changes
* Failure to push updated model into production
* Incorrect feature extraction
* Use of stale dataset, wrong data source
* Data source no longer available (e.g web API)
* Telemetry server overloaded
* Negative feedback (telemtr.) no longer sent from app
* Use of old model learning code, stale hyperparameter
* Data format changes between ML pipeline steps
* ...


</script></section><section data-markdown><script type="text/template">## Quality Assurance for the Entire Pipeline

![](mltestingandmonitoring.png)
<!-- .element: class="plain" -->

<!-- references -->

Source: Eric Breck, Shanqing Cai, Eric Nielsen, Michael Salib, D. Sculley. [The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction](https://research.google.com/pubs/archive/46555.pdf). Proceedings of IEEE Big Data (2017)


</script></section><section data-markdown><script type="text/template">## Pipeline Testing

* Unit tests (e.g., data cleaning)
* End to end pipeline tests
* Testing with stubs, test error handling (e.g., test model redeployment after  dropped connection)
* Test monitoring infrastructure (e.g., "fire drills")


</script></section><section data-markdown><script type="text/template">![Temi](temi.png)









</script></section></section><section ><section data-markdown><script type="text/template"># Thinking of the Entire System

> ML models are "just" one component
</script></section><section data-markdown><script type="text/template">## Living with Mistakes

> The smart toaster may occasionally burn my toast, but it should not burn down my kitchen.

![Toaster](toaster.jpg)




<aside class="notes"><p>A smart toaster may occasionally burn the toast, but it should never burn down the kitchen. The latter can be achieved without relying on perfect accuarcy of a smart component, just stop it when it&#39;s overheating.</p>
<p>Plan for mistakes: User interaction, undo, safeguards</p>
</aside></script></section><section data-markdown><script type="text/template">## Model Accuracy vs System Goals 

* System goals are supported by AI components, e.g.,
    - maximizing sales
    - minimizing loss
    - maximizing community growth
    - retaining customers
    - maximizing engagement time
* A better model will support system goals better
    - more accurate
    - faster answers
    - fewer bad mistakes
    - more explainable
    - easier to evolve
</script></section><section data-markdown><script type="text/template">![Temi](temi.png)

</script></section><section data-markdown><script type="text/template">
## Testing in Production

> Production data = ultimate unseen data

Focus on system goals, not model accuracy

Monitoring performance over time, canary releases

Finding and debugging common mistakes

Experimentation with A/B tests

</script></section><section data-markdown><script type="text/template">## How are we doing in production?


![](temi.png)
</script></section><section data-markdown><script type="text/template">## How are we doing in production?


![](powerpoint.png)
</script></section><section data-markdown><script type="text/template">## How are we doing in production?

![](cancerpred.png)
</script></section><section data-markdown><script type="text/template">## Key Design Challenge: Telemetry

* Identify mistakes in production (“what would have been the right prediction?”)
* Many challenges:
	* How can we identify mistakes? Both false positives and false negatives?
	* How can we collect feedback without being intrusive (e.g., asking users about every interactions)?
	* How much data are we collecting? Can we manage telemetry at scale? How to sample properly?
	* How do we isolate telemetry for specific AI components and versions?
</script></section><section data-markdown><script type="text/template">![Skype feedback dialog](skype1.jpg)
<!-- split -->
![Skype report problem button](skype2.jpg)

<aside class="notes"><p>Expect only sparse feedback and expect negative feedback over-proportionally</p>
</aside></script></section><section data-markdown><script type="text/template">## Manually Label Production Samples

![Amazon mechanical turk](mturk.jpg)
</script></section><section data-markdown><script type="text/template">![Flight cost forcast](flightforcast.jpg)

<aside class="notes"><p>Can just wait 7 days to see actual outcome for all predictions</p>
</aside></script></section><section data-markdown><script type="text/template">![Temi Transcription Service Editor](temi.png)

<aside class="notes"><p>Clever UI design allows users to edit transcripts. UI already highlights low-confidence words, can observe changes in editor (UI design encourages use of editor). In addition 5 star rating for telemetry.</p>
</aside></script></section><section data-markdown><script type="text/template">## Measuring Model Quality with Telemetry

* Telemetry can provide insights for correctness
    - sometimes very accurate labels for real unseen data
    - sometimes only mistakes
    - sometimes indicates severity of mistakes
    - sometimes delayed
    - often just samples, may be hard to catch rare events
    - often just weak proxies for correctness
* Often sufficient to approximate precision/recall or other measures
* Mismatch to (static) evaluation set may indicate stale or unrepresentative test data
* Trend analysis can provide insights even for inaccurate proxy measures
</script></section><section data-markdown><script type="text/template">## Monitoring Model Quality in Production

* Watch for jumps after releases
    - roll back after negative jump
* Watch for slow degradation
    - Stale models, data drift, feedback loops, adversaries
* Debug common or important problems
    - Mistakes uniform across populations?
    - Challenging problems -> refine training, add regression tests
</script></section><section data-markdown><script type="text/template">## Engineering Challenges for Telemetry
![Amazon news story](alexa.png)



</script></section><section data-markdown><script type="text/template">
<svg version="1.1" viewBox="0.0 0.0 800 400" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg">
	<style>
    text { font: 60px sans-serif; }
  	</style>
	<circle r="180" cx="180", cy="200" fill="#b9ff00" fill-opacity="0.514" />
	<circle r="180" cx="620", cy="200" fill="#ff5500" fill-opacity="0.514" />
	<text x=180 y=160 dominant-baseline="middle" text-anchor="middle">Data</text>
	<text x=180 y=240 dominant-baseline="middle" text-anchor="middle">Scientists</text>
	<text x=620 y=160 dominant-baseline="middle" text-anchor="middle">Software</text>
	<text x=620 y=240 dominant-baseline="middle" text-anchor="middle">Engineers</text>
</svg>
</script></section></section><section ><section data-markdown><script type="text/template">
<svg version="1.1" viewBox="0.0 0.0 800 400" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg">
	<style>
    text { font: 60px sans-serif; }
  	</style>
	<circle r="180" cx="250", cy="200" fill="#b9ff00" fill-opacity="0.514" />
	<circle r="180" cx="550", cy="200" fill="#ff5500" fill-opacity="0.514" />
	<text x=230 y=160 dominant-baseline="middle" text-anchor="middle">Data</text>
	<text x=230 y=240 dominant-baseline="middle" text-anchor="middle">Scientists</text>
	<text x=570 y=160 dominant-baseline="middle" text-anchor="middle">Software</text>
	<text x=570 y=240 dominant-baseline="middle" text-anchor="middle">Engineers</text>
</svg>



</script></section><section data-markdown><script type="text/template">![Unicorn](unicorn.jpg)
<!-- .element: class="stretch" -->

</script></section><section data-markdown><script type="text/template">
![Roles Venn Diagram](roles_venn.svg)
<!-- .element: class="stretch plain" -->

By Steven Geringer, via Ryan Orban. [Bridging the Gap Between Data Science & Engineer: Building High-Performance Teams](https://www.slideshare.net/ryanorban/bridging-the-gap-between-data-science-engineer-building-highperformance-teams/3-Software_Engineer_Data_Engineer_Data). 2016


</script></section><section data-markdown><script type="text/template">## T-Shaped People

*Broad-range generalist + Deep expertise*

![T-Shaped](tshaped.png)
<!-- .element: class="stretch plain" -->

Figure: Jason Yip. [Why T-shaped people?](https://medium.com/@jchyip/why-t-shaped-people-e8706198e437). 2018
 
</script></section><section data-markdown><script type="text/template">
## Let's Learn from DevOps

![DevOps](devops.png)

Distinct roles and expertise, but joint responsibilities, joint tooling
</script></section><section data-markdown><script type="text/template">## Toward Better ML-Systems Engineering

Interdisciplinary teams, split expertise, but joint responsibilities

Joint vocabulary and tools

Foster system thinking

Awareness of production quality concerns

Perform risk + hazard analysis


</script></section><section data-markdown><script type="text/template">

<svg version="1.1" viewBox="0.0 0.0 800 400" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg">
	<style>
    text { font: 60px sans-serif; }
  	</style>
	<circle r="180" cx="250", cy="200" fill="#b9ff00" fill-opacity="0.514" />
	<circle r="180" cx="550", cy="200" fill="#ff5500" fill-opacity="0.514" />
	<text x=230 y=160 dominant-baseline="middle" text-anchor="middle">Data</text>
	<text x=230 y=240 dominant-baseline="middle" text-anchor="middle">Scientists</text>
	<text x=570 y=160 dominant-baseline="middle" text-anchor="middle">Software</text>
	<text x=570 y=240 dominant-baseline="middle" text-anchor="middle">Engineers</text>
</svg>

</script></section></section><section  data-markdown><script type="text/template">## Readings

All lecture material: https://github.com/ckaestne/seai

Annotated bibliography: https://github.com/ckaestne/seaibib


<!-- split -->
![DIS](book.webp)
</script></section><section  data-markdown><script type="text/template">## Summary: Software Engineering for ML-Enabled Systems

* Building, operating, and maintaining systems with ML component
* Data scientists and software engineers have different expertise, both needed
* Quality assurance beyond model accuracy
	- Blackbox testing, test automation
	- Testing the entire ML pipeline
	- Consider whole system
	- Testing in production with telemetry
* Interdisciplinary teams, joint vocabulary, and awareness


kaestner@cs.cmu.edu -- [@p0nk](https://twitter.com/p0nk) -- https://github.com/ckaestne/seai/
</script></section></div>
    </div>

    <script src="./../js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './../plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './../plugin/zoom-js/zoom.js', async: true },
        { src: './../plugin/notes/notes.js', async: true },
        { src: './../plugin/math/math.js' },
        { src: './../rplugin/embed-tweet/embed-tweet.js' },
        { src: './../rplugin/menu/menu.js', async: true },
        { src: './../rplugin/spreadsheet/spreadsheet.js' },
        { src: './../rplugin/chalkboard/chalkboard.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps,
	chalkboard: { // font-awesome.min.css must be available
		toggleChalkboardButton: { left: "80px" },
		toggleNotesButton: { left: "130px" },
	},
	keyboard: {
	    67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle notes canvas when 'c' is pressed
	    66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
	    46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
	     8: function() { RevealChalkboard.reset() },	// reset chalkboard data on current slide when 'BACKSPACE' is pressed
	    68: function() { RevealChalkboard.download() },	// downlad recorded chalkboard drawing when 'd' is pressed
	    88: function() { RevealChalkboard.colorNext() },	// cycle colors forward when 'x' is pressed
	    89: function() { RevealChalkboard.colorPrev() },	// cycle colors backward when 'y' is pressed
	}
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"theme":"white","slideNumber":true,"hash":true,"center":false}, queryOptions);
    </script>

    <script src="./../_assets/_assets/viz.js"></script>
    <script src="./../_assets/_assets/loadmymarkdown.js"></script>

    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
