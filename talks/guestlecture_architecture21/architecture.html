<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>Guest Lecture: Software Architecture of AI-Enabled Systems</title>
    <link rel="stylesheet" href="./../css/reveal.css" />
    <link rel="stylesheet" href="./../css/theme/white.css" id="theme" />
    <link rel="stylesheet" href="./../css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./../css/print/paper.css" type="text/css" media="print" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
     <script src="./../rplugin/spreadsheet/ruleJS.all.full.min.js"></script>
   <link rel="stylesheet" href="./../rplugin/spreadsheet/spreadsheet.css">
    <link rel="stylesheet" href="./../_assets/_assets/cmu.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><div id="footer">17-445 Machine Learning in Production</div><section  data-markdown><script type="text/template">



# Software Architecture of AI-Enabled Systems

Guest Lecture by Christian Kaestner

<!-- references -->

Required reading: 
* Vogelsang, Andreas, and Markus Borg. "[Requirements Engineering for Machine Learning: Perspectives from Data Scientists](https://arxiv.org/pdf/1908.04674.pdf)." In Proc. of the 6th International Workshop on Artificial Intelligence for Requirements Engineering (AIRE), 2019. 

</script></section><section ><section data-markdown><script type="text/template"># Machine Learning in Software Systems
</script></section><section data-markdown><script type="text/template">## Machine Learning

Function making predictions for inputs

$f(x_1, x_2, x_3) \rightarrow y$


No specification, function learned by generalizing from example data (inductive reasoning)

</script></section><section data-markdown><script type="text/template">## Running Example: Transcription Service

![competitor](transcription.png)

</script></section><section data-markdown><script type="text/template">
## The startup idea

PhD research on domain-specific speech recognition, that can detect technical jargon

DNN trained on public PBS interviews + transfer learning on smaller manually annotated domain-specific corpus

Research has shown amazing accuracy for talks in medicine, poverty and inequality research, and talks at Ruby programming conferences; published at top conferences

Idea: Let's commercialize the software and sell to academics and conference organizers


</script></section><section data-markdown><script type="text/template">
## What qualities are important for a good commercial transcription product?

<!-- discussion -->

</script></section><section data-markdown><script type="text/template">## ML in a Production System


![Architecture diagram of transcription service; many components, not just ML](transcriptionarchitecture.png)

</script></section><section data-markdown><script type="text/template">## ML in a Production System


![Architecture diagram of transcription service; many components, not just ML](transcriptionarchitecture2.png)





</script></section></section><section ><section data-markdown><script type="text/template"># Accuracy, Correctness, and Other Qualities

</script></section><section data-markdown><script type="text/template">## Traditional ML Focus: Model Accuracy

* Train and evaluate model on fixed labled data set
* Compare prediction with labels

![Architecture diagram of transcription service; many components, not just ML](transcriptionarchitecture.png)
</script></section><section data-markdown><script type="text/template">## Traditional SE Focus: Functional Correctness

*Given a specification, do outputs match inputs?*

```java
/**
 * compute deductions based on provided adjusted 
 * gross income and expenses in customer data.
 *
 * see tax code 26 U.S. Code A.1.B, PART VI
 */
float computeDeductions(float agi, Expenses expenses);
```

**Each mismatch is considered a bug, should to be fixed*.**

(*=not every bug is economical to fix, may accept some known bugs)


</script></section><section data-markdown><script type="text/template">## No specification!

We use ML precisely because we do not have a specification (too complex, rules unknown)

![Architecture diagram of transcription service; many components, not just ML](transcriptionarchitecture.png)

We are usually okay with some wrong predictions

</script></section><section data-markdown><script type="text/template">

> All models are approximations. Assumptions, whether implied or clearly stated, are never exactly true. **All models are wrong, but some models are useful**. So the question you need to ask is not "Is the model true?" (it never is) but "Is the model good enough for this particular application?" -- George Box


<!-- references -->
See also https://en.wikipedia.org/wiki/All_models_are_wrong

</script></section><section data-markdown><script type="text/template">## Non-ML Example: Newton's Laws of Motion

> 2nd law: "the rate of change of momentum of a body over time is directly proportional to the force applied, and occurs in the same direction as the applied force" 
> ${\displaystyle \mathbf {F} ={\frac {\mathrm {d} \mathbf {p} }{\mathrm {d} t}}}$

"Newton's laws were verified by experiment and observation for over 200 years, and they are excellent approximations at the scales and speeds of everyday life."

Do not generalize for very small scales, very high speeds, or in very strong gravitational fields. Do not explain semiconductor, GPS errors, superconductivity, ... Those require general relativity and quantum field theory.

<!-- references -->
Further readings: https://en.wikipedia.org/wiki/Newton%27s_laws_of_motion

</script></section><section data-markdown><script type="text/template">## Limitations of Offline Model Evaluation

* Training and test data drawn from the same population 
    * **i.i.d.: independent and identically distributed**
* Is the population representative of production data?
* If not or only partially or not anymore: Does the model generalize beyond training data?
</script></section><section data-markdown><script type="text/template">## Testing in Production

<div class="tweet" data-src="https://twitter.com/changelog/status/1137359428632621060"></div>

</script></section><section data-markdown><script type="text/template">## Quality concerns for ML-Enabled Systems

* Learning time, cost and scalability
* Update cost, incremental learning
* Inference cost
* Size of models learned
* Amount of training data needed
* Fairness
* Robustness
* Safety, security, privacy
* Explainability, reproducibility
* Time to market
* Overall operating cost (cost per prediction)



</script></section></section><section ><section data-markdown><script type="text/template"># Deploying ML Models
</script></section><section data-markdown><script type="text/template">## Accessibility: Live Subtitles 

![Google Glasses](googleglasses.jpg)
</script></section><section data-markdown><script type="text/template">## Where to Deploy the Transcription Model?

```mermaid
graph LR
Cloud ---|Mobile data| Phone
Phone ---|Bluetooth| Glasses
```

<!-- discussion -->
</script></section><section data-markdown><script type="text/template">## Where to Deploy the Transcription Model?

```mermaid
graph LR
Cloud ---|Mobile data| Phone
Phone ---|Bluetooth| Glasses
```

Which qualities and tradeoffs to consider?

<!-- discussion -->
</script></section><section data-markdown><script type="text/template">## Where to Deploy the Transcription Model?

```mermaid
graph LR
Cloud ---|Mobile data| Phone
Phone ---|Bluetooth| Glasses
```

* Amount of data, bandwidth, bandwidth cost
* Latency
* Energy/battery cost
* Available memory, CPU capacity
* Ability to debug 
* Offline functioning
* Privacy, security
* Accuracy
* Frequency of model updates




</script></section></section><section ><section data-markdown><script type="text/template"># Telemetry Design
</script></section><section data-markdown><script type="text/template">## Goals 1: Evaluate model and system quality in production

![Windows 95 Crash Report](wincrashreport_windows_xp.png)

</script></section><section data-markdown><script type="text/template">## Goal 2: Experimenting in Production

![A/B test example](ab-groove.jpg)

</script></section><section data-markdown><script type="text/template">## Goal 3: Gather more training data

![The ML Flywheel](flywheel.png)

 <!-- references -->

 graphic by [CBInsights](https://www.cbinsights.com/research/team-blog/data-network-effects/)

</script></section><section data-markdown><script type="text/template">## Discussion: Was the Transcription any good?

* Gather feedback without being intrusive (i.e., labeling outcomes), without harming user experience
* What data can we collect to evaluate our transcription service?
    - Evaluate business goals
    - Evaluate system quality
    - Evaluate model quality

<!-- discussion -->

</script></section><section data-markdown><script type="text/template">## Typical Telemetry Strategies

* Wait and see
* Ask users
* Manual/crowd-source labeling, shadow execution
* Allow users to complain
* Observe user reaction

</script></section><section data-markdown><script type="text/template">![Flight cost forcast](flightforcast.jpg)

<aside class="notes"><p>Can just wait 7 days to see actual outcome for all predictions</p>
</aside></script></section><section data-markdown><script type="text/template">![Temi Transcription Service Editor](temi.png)

<aside class="notes"><p>Clever UI design allows users to edit transcripts. UI already highlights low-confidence words, can</p>
</aside></script></section><section data-markdown><script type="text/template">![Skype feedback dialog](skype1.jpg)
<!-- split -->
![Skype report problem button](skype2.jpg)

<aside class="notes"><p>Expect only sparse feedback and expect negative feedback over-proportionally</p>
</aside></script></section><section data-markdown><script type="text/template">## Manually Label Production Samples

Similar to labeling learning and testing data, have human annotators

![Amazon mechanical turk](mturk.jpg)



</script></section><section data-markdown><script type="text/template">## Clever UI Design: Transcription Service

![Screenshot of Temi transcription service](temi.png)

</script></section><section data-markdown><script type="text/template">## ML in a Production System


![Architecture diagram of transcription service; many components, not just ML](transcriptionarchitecture2.png)

</script></section><section data-markdown><script type="text/template">## Discussion 2: Google Tagging uploaded photos with friends' names

* Gather feedback without being intrusive (i.e., labeling outcomes), without harming user experience
* What data can we collect to evaluate our transcription service?
    - Evaluate business goals
    - Evaluate system quality
    - Evaluate model quality


<!-- discussion -->

</script></section><section data-markdown><script type="text/template">## Monitoring Model Quality in Production

* Monitor model quality together with other quality attributes (e.g., uptime, response time, load)
* Set up automatic alerts when model quality drops
* Watch for jumps after releases
    - roll back after negative jump
* Watch for slow degradation
    - Stale models, data drift, feedback loops, adversaries
* Debug common or important problems
    - Monitor characteristics of requests 
    - Mistakes uniform across populations?
    - Challenging problems -> refine training, add regression tests

</script></section><section data-markdown><script type="text/template">![Grafana screenshot from Movie Recommendation Service](grafana.png)

</script></section><section data-markdown><script type="text/template">## Detecting Drift

![Drift](drift.jpg)

<!-- references -->
Image source: Joel Thomas and Clemens Mewald. [Productionizing Machine Learning: From Deployment to Drift Detection](https://databricks.com/blog/2019/09/18/productionizing-machine-learning-from-deployment-to-drift-detection.html). Databricks Blog, 2019

</script></section><section data-markdown><script type="text/template">## Model Quality vs System Quality

![Model accuracy does not need to correlate with business metric](bookingcom2.png)
<!-- .element: class="stretch" --> 

**Possible causes?**

Bernardi et al. "150 successful machine learning models: 6 lessons learned at Booking.com." In Proc KDD, 2019.

<aside class="notes"><p>hypothesized </p>
<ul>
<li>model value saturated, little more value to be expected</li>
<li>segment saturation: only very few users benefit from further improvement</li>
<li>overoptimization on proxy metrics not real target metrics</li>
<li>uncanny valley effect from &quot;creepy AIs&quot;</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">## Engineering Challenges for Telemetry
![Amazon news story](alexa.png)
</script></section><section data-markdown><script type="text/template">## Engineering Challenges for Telemetry
* Data volume and operating cost
    - e.g., record "all AR live translations"?
    - reduce data through sampling
    - reduce data through summarization (e.g., extracted features rather than raw data; extraction client vs server side)
* Adaptive targeting
* Biased sampling
* Rare events
* Privacy
* Offline deployments?

</script></section><section data-markdown><script type="text/template">## Exercise: Design Telemetry in Production

Discuss: Quality measure, telemetry, operationalization, cost, privacy, rare events

**Google: Tagging uploaded photos with friends' names**

<!-- discussion -->

</script></section></section><section ><section data-markdown><script type="text/template">
# Summary

* Machine learning is a component of a larger system
* It brings new concerns, qualities, and design options
* Telemetry design is key for ML systems in production
* Many qualities and tradeoffs to consider

</script></section><section data-markdown><script type="text/template">## Further pointers

* Full lecture (with videos, readings, assignments): https://ckaestne.github.io/seai/
* Annotated bibliography: https://github.com/ckaestne/seaibib
* Hulten, Geoff. Building Intelligent Systems: A Guide to Machine Learning Engineering. Apress. 2018
* Yokoyama, Haruki. "Machine learning system architectural pattern for improving operational stability." In 2019 IEEE International Conference on Software Architecture Companion (ICSA-C), pp. 267-274. IEEE, 2019.
* Hazelwood, Kim, Sarah Bird, David Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy et al. "Applied machine learning at facebook: A datacenter infrastructure perspective." In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 620-629. IEEE, 2018.

</script></section></section></div>
    </div>

    <script src="./../js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './../plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './../plugin/zoom-js/zoom.js', async: true },
        { src: './../plugin/notes/notes.js', async: true },
        { src: './../plugin/math/math.js' },
        { src: './../rplugin/embed-tweet/embed-tweet.js' },
        { src: './../rplugin/menu/menu.js', async: true },
        { src: './../rplugin/spreadsheet/spreadsheet.js' },
        { src: './../rplugin/chalkboard/chalkboard.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps,
	chalkboard: { // font-awesome.min.css must be available
		toggleChalkboardButton: { left: "80px" },
		toggleNotesButton: { left: "130px" },
	},
	keyboard: {
	    67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle notes canvas when 'c' is pressed
	    66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
	    46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
	     8: function() { RevealChalkboard.reset() },	// reset chalkboard data on current slide when 'BACKSPACE' is pressed
	    68: function() { RevealChalkboard.download() },	// downlad recorded chalkboard drawing when 'd' is pressed
	    88: function() { RevealChalkboard.colorNext() },	// cycle colors forward when 'x' is pressed
	    89: function() { RevealChalkboard.colorPrev() },	// cycle colors backward when 'y' is pressed
	}
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"theme":"white","slideNumber":true,"hash":true,"center":false}, queryOptions);
    </script>

    <script src="./../_assets/_assets/viz.js"></script>
    <script src="./../_assets/_assets/loadmymarkdown.js"></script>

    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
