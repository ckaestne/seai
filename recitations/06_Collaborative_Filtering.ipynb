{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recitation - Collaborative Filtering.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgK7_mMVeqde",
        "colab_type": "text"
      },
      "source": [
        "## Recommender systems\n",
        "\n",
        "Recommender systems are ***active information filtering systems*** that personalize the information coming to a user based on his interests, relevance of the information, etc.\n",
        "\n",
        "There are typically 2 ways that these systems are built.\n",
        "\n",
        "1. **Content based filtering**\n",
        "\n",
        "![The algorithm infers you like the brand duff!](https://miro.medium.com/max/1334/1*oYpMnPQFZaiZQizgVWBpoA.png)\n",
        "\n",
        "  * users are recommended items based on those they have already consumed. For example, if you bought a blue pen, amazon may recommend you a red pen as they both are similar. This is not the scope of our discussion today.\n",
        "  * Problems include creation of a filter bubble, recommending items you have already purchased\n",
        "\n",
        "\n",
        "2. **Collaborative filtering**\n",
        "\n",
        "![](https://miro.medium.com/max/1374/1*-Jr1l2rlj9SBcCzlDHtN5g.jpeg)\n",
        "\n",
        "  * CF accumulates customer product ratings, identifies customers with common ratings, and offers recommendations based on inter-customer comparisons. It’s based on the idea that people who agree in their evaluations of certain items in the past are likely to agree again in the future. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_yJn9tIJcdR",
        "colab_type": "text"
      },
      "source": [
        "## Collaborative Filtering\n",
        "\n",
        "There are 2 ways to build collaborative filtering systems.\n",
        "\n",
        "### Memory based CF\n",
        "\n",
        "* User based:\n",
        "  * User 1 rated movie A highly\n",
        "  * User 2 rated movie A highly -> User 1 and User 2 are similar.\n",
        "  * Hence, if User 1 rates movie B low, user 2 will also rate movie B low\n",
        "\n",
        "* Item-based:\n",
        "  * Instead of measuring the similarity between users, the item-based CF recommends items based on their similarity with the items that the target user rated\n",
        "\n",
        "  ![](https://miro.medium.com/max/1400/1*7bFc9R97Z4jKK6J2jaSUlw.jpeg)\n",
        "\n",
        "* Similarity Matrix - Here, item(i, j) represents the rating user i gives movie j\n",
        "![](https://hackernoon.com/hn-images/1*9TC6BrfxYttJwiATFAIFBg.png)\n",
        "\n",
        "* Similarity between users x and y is calculated using cosine similarity where xi is the rating user x gives movie i and yi is the rating user y gives movie i. If user x has rated a movie and user y has not, you can assume the rating to be 0, use a constant or even skip it.\n",
        "\n",
        "![](https://i.imgur.com/I9T81nG.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTHJ9VG_Jgos",
        "colab_type": "text"
      },
      "source": [
        "### Code Example - User based CF\n",
        "\n",
        "\n",
        "We use the [Surprise Scikit](http://surpriselib.com/) (Simple Python RecommendatIon System Engine) library which is a scikit-learn type collection of ML recommendation algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkMslo5mpKt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f6d82e64-e19d-47f0-e3ff-447dfa87c1fc"
      },
      "source": [
        "%pip install scikit-surprise\n",
        "from surprise import Dataset\n",
        "\n",
        "# Loads the builtin Movielens-100k data\n",
        "movielens = Dataset.load_builtin('ml-100k')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (0.15.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lqs3iePujOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from surprise import Reader\n",
        "from surprise import KNNWithMeans\n",
        "\n",
        "# This is the same data that was plotted for similarity earlier\n",
        "# with one new user \"E\" who has rated only movie 1\n",
        "ratings_dict = {\n",
        "    \"item\": [1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6],\n",
        "    \"user\": ['A', 'B', 'F', 'D', 'F', 'A', 'B', 'C', 'F', 'A', 'D', 'F', 'A', 'C', 'B', 'D', 'E'],\n",
        "    \"rating\": [2, 5, 4, 1, 5, 2, 4, 5, 4, 4, 5, 1, 5, 2, 1, 4, 2],\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(ratings_dict)\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "# Loads Pandas dataframe\n",
        "data = Dataset.load_from_df(df[[\"user\", \"item\", \"rating\"]], reader)\n",
        "\n",
        "# To use user-based cosine similarity\n",
        "sim_options = {\n",
        "    \"name\": \"cosine\",\n",
        "    \"user_based\": True,\n",
        "}\n",
        "algo = KNNWithMeans(sim_options=sim_options)\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX5xG8zysJPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6049d58c-5263-4f9b-f468-11ee62a418de"
      },
      "source": [
        "# Train\n",
        "trainingSet = data.build_full_trainset()\n",
        "algo.fit(trainingSet)\n",
        "\n",
        "# Predict\n",
        "prediction = algo.predict('E', 1)\n",
        "print (prediction.est)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "3.6666666666666665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni_PLD-n4eHc",
        "colab_type": "text"
      },
      "source": [
        "### Model based CF\n",
        "\n",
        "\n",
        "* Typically uses matrix based factorization, which is useful for deriving meaningful inferences from sparse matrices, primarily through latent features.\n",
        "* Matrix factorization can be seen as breaking down a large matrix into a product of smaller ones. A matrix A with dimensions `m x n` can be reduced to a product of two matrices X and Y with dimensions `m x p` and `p x n` respectively.\n",
        "\n",
        "#### Concrete Example \n",
        "\n",
        "![](https://files.realpython.com/media/dimensionality-reduction.f8686dd52b9c.jpg)\n",
        "\n",
        "Here, the left matrix and top matrix map `m` users to `2` latent features and `n` movies to the same 2 latent features respectively. Here is an interpretation \n",
        "\n",
        "* Assume that in a user vector (u, v), u represents how much a user likes the Horror genre, and v represents how much they like the Romance genre.\n",
        "\n",
        "  * The user vector (2, -1) thus represents a user who likes horror movies and rates them positively and dislikes movies that have romance and rates them negatively.\n",
        "\n",
        "* Assume that in an item vector (i, j), i represents how much a movie belongs to the Horror genre, and j represents how much that movie belongs to the Romance genre.\n",
        "\n",
        "  * The movie (2.5, 1) has a Horror rating of 2.5 and a Romance rating of 1. Multiplying it by the user vector using matrix multiplication rules gives you (2 * 2.5) + (-1 * 1) = 4.\n",
        "\n",
        "* **So, the movie belonged to the Horror genre, and the user could have rated it 5, but the slight inclusion of Romance caused the final rating to drop to 4.**\n",
        "\n",
        "\n",
        "#### **Singular Value Decomposition**\n",
        "\n",
        "* Simply a way to factorize a matrix, most popularly used for collaborative filtering and recommendation algorithms. \n",
        "* Users RMSE as the metric to ensure that the closest approximation is reached. We essentially turn the recommendation problem into an optimization problem.\n",
        "* The canonical equation is as shown. The way you derive inferences is just like the example we discussed\n",
        "\n",
        "![](https://hackernoon.com/hn-images/1*haUDjEiQmG0RapR0SHos6Q.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "### Code Example - SVD based CF\n",
        "\n",
        "We use `GridSearchCV` in conjucntion with the `SVD` algorithm, to give us the most optimal results by minimizing RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yELX8WpACw2z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f55c856-060d-4b6a-fae5-5bf31993a5e1"
      },
      "source": [
        "from surprise import SVD\n",
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "\"\"\"\n",
        "lr_all is the learning rate for all parameters (how much the parameters are adjusted in each iteration)\n",
        "reg_all is the regularization term for all parameters, which is a penalty term added to prevent overfitting.\n",
        "\"\"\"\n",
        "param_grid = {\n",
        "    \"n_epochs\": [5, 10],\n",
        "    \"lr_all\": [0.002, 0.005],\n",
        "    \"reg_all\": [0.4, 0.6]\n",
        "}\n",
        "\n",
        "# Get the best params using GridSearchCV\n",
        "gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\"], cv=3)\n",
        "gs.fit(data)\n",
        "best_params = gs.best_params[\"rmse\"]\n",
        "\n",
        "# Extract and train model with best params\n",
        "svd_algo = SVD(n_epochs=best_params['n_epochs'],\n",
        "               lr_all=best_params['lr_all'],\n",
        "               reg_all=best_params['reg_all'])\n",
        "svd_algo.fit(trainingSet)\n",
        "\n",
        "# Predict\n",
        "prediction = svd_algo.predict('E', 1)\n",
        "print (prediction.est)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.449428534520331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiePgqDmHMds",
        "colab_type": "text"
      },
      "source": [
        "## Drawbacks\n",
        "\n",
        "* Visibility - The main drawback of SVD is that there is no to little explanation to the reason that we recommend an item to an user. This can be a huge problem if users are eager to know why a specific item is recommended to them.\n",
        "* Missing values - In the case of SVD, it doesn’t assume anything about missing values. So you need to give some missing value imputation for SVD. This might bring in unnecessary noise into the model.\n",
        "* Recommending new items - Collaborative filtering can lead to some problems like cold start for new items that are added to the list. Until someone rates them, they don’t get recommended.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIcwewQeIyM0",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "* [Building collaborative filtering systems](https://realpython.com/build-recommendation-engine-collaborative-filtering/#model-based)\n",
        "* [Collaborative filtering using KNN](https://heartbeat.fritz.ai/recommender-systems-with-python-part-ii-collaborative-filtering-k-nearest-neighbors-algorithm-c8dcd5fd89b2)\n",
        "* [Collaborative filtering using SVD](https://heartbeat.fritz.ai/recommender-systems-with-python-part-iii-collaborative-filtering-singular-value-decomposition-5b5dcb3f242b)\n",
        "* [Beginner's guide to creating an SVD Recommender System](https://towardsdatascience.com/beginners-guide-to-creating-an-svd-recommender-system-1fd7326d1f65)"
      ]
    }
  ]
}