<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>17-445: Data Quality and Data Programming</title>
    <link rel="stylesheet" href="./../css/reveal.css" />
    <link rel="stylesheet" href="./../css/theme/white.css" id="theme" />
    <link rel="stylesheet" href="./../css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./../css/print/paper.css" type="text/css" media="print" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
     <script src="./../rplugin/spreadsheet/ruleJS.all.full.min.js"></script>
   <link rel="stylesheet" href="./../rplugin/spreadsheet/spreadsheet.css">
    <link rel="stylesheet" href="./../_assets/_assets/cmu.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><div id="footer">17-445 Software Engineering for AI-Enabled Systems, Christian Kaestner</div><section  data-markdown><script type="text/template">

# Data Quality and Data Programming


> "Data cleaning and repairing account for about 60% of the work of data scientists."


Christian Kaestner




<!-- references -->

Required reading: 
* üóé Schelter, S., Lange, D., Schmidt, P., Celikel, M., Biessmann, F. and Grafberger, A., 2018. [Automating large-scale data quality verification](http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf). Proceedings of the VLDB Endowment, 11(12), pp.1781-1794.
* üóé Nick Hynes, D. Sculley, Michael Terry. "[The Data Linter: Lightweight Automated Sanity Checking for ML Data Sets](http://learningsys.org/nips17/assets/papers/paper_19.pdf)."  NIPS Workshop on ML Systems (2017)
</script></section><section  data-markdown><script type="text/template">
# Learning Goals

* Design and implement automated quality assurance steps that check data schema conformance and distributions 
* Devise thresholds for detecting data drift and schema violations
* Describe common data cleaning steps and their purpose and risks
* Evaluate the robustness of AI components with regard to noisy or incorrect data
* Understanding the better models vs more data tradeoffs
* Programatically collect, manage, and enhance training data

</script></section><section ><section data-markdown><script type="text/template">
# Data-Quality Challenges
</script></section><section data-markdown><script type="text/template">## Case Study: Inventory Management

![Shelves in a warehouse](warehouse.jpg)
<!-- .element: class="stretch" -->
</script></section><section data-markdown><script type="text/template">## Inventory Database

Product Database:

| ID | Name | Weight | Description | Size | Vendor |
| - |  - |  - |  - |  - | - | 
| ... |  ... |  ... |  ... |  ... |  ... | 

Stock:

| ProductID | Location | Quantity |
| - |  - |  - |
| ... |  ... |  ... |

Sales history:

| UserID | ProductId | DateTime | Quantity | Price | 
| - |  - |  - | - |- |
| ... |  ... |  ... |... |... |
</script></section><section data-markdown><script type="text/template">## What makes good quality data?

* Accuracy
  * The data was recorded correctly.
* Completeness
  * All relevant data was recorded.
* Uniqueness
  * The entries are recorded once.
* Consistency
  * The data agrees with itself.
* Timeliness
  * The data is kept up to date.
</script></section><section data-markdown><script type="text/template">## Data is noisy

* Unreliable sensors or data entry
* Wrong results and computations, crashes
* Duplicate data, near-duplicate data
* Out of order data
* Data format invalid
*
* **Examples?**
</script></section><section data-markdown><script type="text/template">## Data changes

* System objective changes over time
* Software components are upgraded or replaced
* Prediction models change
* Quality of supplied data changes
* User behavior changes
* Assumptions about the environment no longer hold
*
* **Examples?**
</script></section><section data-markdown><script type="text/template">## Users may deliberately change data

* Users react to model output
* Users try to game/deceive the model
*
* **Examples?**
</script></section><section data-markdown><script type="text/template">## Many Data Sources

```mermaid
graph TD;
Twitter --> SalesTrends
AdNetworks --> SalesTrends
SalesTrends --> Prediction
VendorSales --> Prediction
ProductData --> Prediction
Marketing --> Prediction
Theft[Expired/Lost/Theft] --> Prediction
PastSales --> Prediction((Inventory ML))
```

*sources of different reliability and quality*
</script></section><section data-markdown><script type="text/template">## Accuracy vs Precision

* Accuracy: Reported values (on average) represent real value
* Precision: Repeated measurements yield the same result
* 
* Accurate, but imprecise: Average over multiple measurements
* Inaccurate, but precise: Systematic measurement problem, misleading

<!-- split -->

![Accuracy-vs-precision visualized](Accuracy_and_Precision.svg)

(CC-BY-4.0 by [Arbeck](https://commons.wikimedia.org/wiki/File:Accuracy_and_Precision.svg))
</script></section><section data-markdown><script type="text/template">## Accuracy and Precision in Training Data?

![Shelves in a warehouse](warehouse.jpg)
<!-- .element: class="stretch" -->

</script></section><section data-markdown><script type="text/template">## Data Quality and Machine Learning

* More data -> better models (up to a point, diminishing effects)
* Noisy data (imprecise) -> less confident models, more data needed 
  * some ML techniques are more or less robust to noise (more on robustness in a later lecture)
* Inaccurate data -> misleading models, biased models
* 
* Need the "right" data
* Invest in data quality, not just quantity






</script></section></section><section ><section data-markdown><script type="text/template"># Exploratory Data Analysis
</script></section><section data-markdown><script type="text/template">## Exploratory Data Analysis in Data Science

* Before learning, understand the data
* Understand types, ranges, distributions
* Important for understanding data and assessing quality
* Plot data distributions for features
  - Visualizations in a notebook
  - Boxplots, histograms, density plots, scatter plots, ...
* Explore outliers
* Look for correlations and dependencies
  - Association rule mining
  - Principal component analysis

Examples: https://rpubs.com/ablythe/520912 and https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15
</script></section><section data-markdown><script type="text/template">## SE Perspective: Understanding Data for Quality Assurance

* Understand input and output data
* Understand expected distributions
* Understand assumptions made on data for modeling
  - ideally document those
* Check assumptions at runtime






</script></section></section><section ><section data-markdown><script type="text/template">
# Data Cleaning

> Data cleaning and repairing account for about 60% of the work of data scientists.


<!-- references -->
Quote: Gil Press. ‚Äú[Cleaning Big Data: Most Time-Consuming, Least Enjoyable Data Science Task, Survey Says](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/).‚Äù Forbes Magazine, 2016.
</script></section><section data-markdown><script type="text/template">
![Quality Problems Taxonomy](qualityproblems.png)

<!-- references -->

Source: Rahm, Erhard, and Hong Hai Do. [Data cleaning: Problems and current approaches](http://dc-pubs.dbs.uni-leipzig.de/files/Rahm2000DataCleaningProblemsand.pdf). IEEE Data Eng. Bull. 23.4 (2000): 3-13.

</script></section><section data-markdown><script type="text/template">## Single-Source Problem Examples

* Schema level:
<!-- .element: class="fragment" -->
    * Illegal attribute values: `bdate=30.13.70`
    * Violated attribute dependencies: `age=22, bdate=12.02.70`
    * Uniqueness violation: `(name=‚ÄùJohn Smith‚Äù, SSN=‚Äù123456‚Äù), (name=‚ÄùPeter Miller‚Äù, SSN=‚Äù123456‚Äù)`
    * Referential integrity violation: `emp=(name=‚ÄùJohn Smith‚Äù, deptno=127)` if department 127 not defined
* Instance level:
<!-- .element: class="fragment" -->
	* Missing values: `phone=9999-999999`
    * Misspellings: `city=Pittsburg`
    * Misfielded values: `city=USA`
    * Duplicate records: `name=John Smith, name=J. Smith`
    * Wrong reference: `emp=(name=‚ÄùJohn Smith‚Äù, deptno=127)` if department 127 defined but wrong

<!-- references -->

Further readings: Rahm, Erhard, and Hong Hai Do. [Data cleaning: Problems and current approaches](http://dc-pubs.dbs.uni-leipzig.de/files/Rahm2000DataCleaningProblemsand.pdf). IEEE Data Eng. Bull. 23.4 (2000): 3-13.
</script></section><section data-markdown><script type="text/template">## Dirty Data: Example

![Dirty data](dirty-data-example.jpg)

*Problems with the data?*

</script></section><section data-markdown><script type="text/template">## Discussion: Potential Data Quality Problems?

![Shelves in a warehouse](warehouse.jpg)
<!-- .element: class="stretch" -->

</script></section><section data-markdown><script type="text/template">## Data Cleaning Overview

* Data analysis / Error detection
  * Error types: e.g. schema constraints, referential integrity, duplication 
  * Single-source vs multi-source problems
  * Detection in input data vs detection in later stages (more context)
* Error repair
  * Repair data vs repair rules, one at a time or holistic
  * Data transformation or mapping
  * Automated vs human guided
</script></section><section data-markdown><script type="text/template">## Error Detection

* Illegal values: min, max, variance, deviations, cardinality
* Misspelling: sorting + manual inspection, dictionary lookup
* Missing values: null values, default values
* Duplication: sorting, edit distance, normalization
</script></section><section data-markdown><script type="text/template">## Error Detection: Example

![Dirty data](dirty-data-example.jpg)

Q. Can we (automatically) detect errors? Which errors are problem-dependent?
</script></section><section data-markdown><script type="text/template">## Common Strategies

* Enforce schema constraints
  * e.g., delete rows with missing data or use defaults
* Explore sources of errors 
  * e.g., debugging missing values, outliers
* Remove outliers
  * e.g., Testing for normal distribution, remove > 2œÉ
* Normalization
  * e.g., range [0, 1], power transform
* Fill in missing values
</script></section><section data-markdown><script type="text/template">## Data Cleaning Tools

![Open Refine](openrefine.jpg)

OpenRefine (formerly Google Refine), Trifacta Wrangler, Drake, etc.,
</script></section><section data-markdown><script type="text/template">## Different Cleaning Tools

* Outlier detection
* Data deduplication
* Data transformation
* Rule-based data cleaning and rule discovery
  - (conditional) functional dependencies and other constraints
* Probabilistic data cleaning

<!-- references -->

Further reading: Ilyas, Ihab F., and Xu Chu. [Data cleaning](https://dl.acm.org/doi/book/10.1145/3310205). Morgan & Claypool, 2019.


</script></section></section><section ><section data-markdown><script type="text/template">
# Data Schema
</script></section><section data-markdown><script type="text/template">## Data Schema

* Define expected format of data
  * expected fields and their types
  * expected ranges for values
  * constraints among values (within and across sources)
* Data can be automatically checked against schema
* Protects against change; explicit interface between components
</script></section><section data-markdown><script type="text/template">## Schema in Relational Databases

```sql
CREATE TABLE employees (
    emp_no      INT             NOT NULL,
    birth_date  DATE            NOT NULL,
    name        VARCHAR(30)     NOT NULL,
    PRIMARY KEY (emp_no));
CREATE TABLE departments (
    dept_no     CHAR(4)         NOT NULL,
    dept_name   VARCHAR(40)     NOT NULL,
    PRIMARY KEY (dept_no), UNIQUE  KEY (dept_name));
CREATE TABLE dept_manager (
   dept_no      CHAR(4)         NOT NULL,
   emp_no       INT             NOT NULL,
   FOREIGN KEY (emp_no)  REFERENCES employees (emp_no),
   FOREIGN KEY (dept_no) REFERENCES departments (dept_no),
   PRIMARY KEY (emp_no,dept_no)); 
```
</script></section><section data-markdown><script type="text/template">## Schema-Less Data Exchange

* CSV files
* Key-value stores (JSon, XML, Nosql databases)
* Message brokers
* REST API calls
* R/Pandas Dataframes

```csv
1::Toy Story (1995)::Animation|Children's|Comedy
2::Jumanji (1995)::Adventure|Children's|Fantasy
3::Grumpier Old Men (1995)::Comedy|Romance
```

```csv
10|53|M|lawyer|90703
11|39|F|other|30329
12|28|F|other|06405
13|47|M|educator|29206
```
</script></section><section data-markdown><script type="text/template">## Example: Apache Avro

```json
{   "type": "record",
    "namespace": "com.example",
    "name": "Customer",
    "fields": [{
            "name": "first_name",
            "type": "string",
            "doc": "First Name of Customer"
        },        
        {
            "name": "age",
            "type": "int",
            "doc": "Age at the time of registration"
        }
    ]
}
```
</script></section><section data-markdown><script type="text/template">## Example: Apache Avro

* Schema specification in JSON format
* Serialization and deserialization with automated checking
* Native support in Kafka
* 
* Benefits
  * Serialization in space efficient format
  * APIs for most languages (ORM-like)
  * Versioning constraints on schemas
* Drawbacks
  * Reading/writing overhead
  * Binary data format, extra tools needed for reading
  * Requires external schema and maintenance
  * Learning overhead

<aside class="notes"><p>Further readings eg <a href="https://medium.com/@stephane.maarek/introduction-to-schemas-in-apache-kafka-with-the-confluent-schema-registry-3bf55e401321">https://medium.com/@stephane.maarek/introduction-to-schemas-in-apache-kafka-with-the-confluent-schema-registry-3bf55e401321</a>, <a href="https://www.confluent.io/blog/avro-kafka-data/">https://www.confluent.io/blog/avro-kafka-data/</a>, <a href="https://avro.apache.org/docs/current/">https://avro.apache.org/docs/current/</a></p>
</aside></script></section><section data-markdown><script type="text/template">## Many Schema Formats

Examples
* Avro
* XML Schema
* Protobuf
* Thrift
* Parquet
* ORC
</script></section><section data-markdown><script type="text/template">## Discussion: Data Schema for Inventory System?

Product Database:

| ID | Name | Weight | Description | Size | Vendor |
| - |  - |  - |  - |  - | - | 
| ... |  ... |  ... |  ... |  ... |  ... | 

Stock:

| ProductID | Location | Quantity |
| - |  - |  - |
| ... |  ... |  ... |

Sales history:

| UserID | ProductId | DateTime | Quantity | Price | 
| - |  - |  - | - |- |
| ... |  ... |  ... |... |... |



</script></section></section><section ><section data-markdown><script type="text/template"># Detecting Inconsistencies 

![Data Inconsistency Examples](errors_chicago.jpg)

<!-- references -->
Image source: Theo Rekatsinas, Ihab Ilyas, and Chris R√©, ‚Äú[HoloClean - Weakly Supervised Data Repairing](https://dawn.cs.stanford.edu/2017/05/12/holoclean/).‚Äù Blog, 2017.
</script></section><section data-markdown><script type="text/template">## Data Quality Rules

* Invariants on data that must hold
* Typically about relationships of multiple attributes or data sources, eg.
  - ZIP code and city name should correspond
  - User ID should refer to existing user
  - SSN should be unique
  - For two people in the same state, the person with the lower income should not have the higher tax rate
* Classic integrity constraints in databases or conditional constraints
* Rules can be used to reject data or repair it
</script></section><section data-markdown><script type="text/template">## Discovery of Data Quality Rules

+ Rules directly taken from external databases
  * e.g. zip code directory
+ Given clean data, 
  * several algorithms that find functional relationships ($X\Rightarrow Y$) among columns
  * algorithms that find conditional relationships (if $Z$ then $X\Rightarrow Y$)
  * algorithms that find denial constraints ($X$ and $Y$ cannot cooccur in a row)
+ Given mostly clean data (probabilistic view),
  * algorithms to find likely rules (e.g., association rule mining)
  * outlier and anomaly detection
+ Given labeled dirty data or user feedback,
  * supervised and active learning to learn and revise rules
  * supervised learning to learn repairs (e.g., spell checking)

<!-- references -->

Further reading: Ilyas, Ihab F., and Xu Chu. [Data cleaning](https://dl.acm.org/doi/book/10.1145/3310205). Morgan & Claypool, 2019.
</script></section><section data-markdown><script type="text/template">## Association rule mining

* Sale 1: Bread, Milk
* Sale 2: Bread, Diaper, Beer, Eggs
* Sale 3: Milk, Diaper, Beer, Coke
* Sale 4: Bread, Milk, Diaper, Beer
* Sale 5: Bread, Milk, Diaper, Coke

Rules
* {Diaper, Beer} -> Milk (40% support, 66% confidence)
* Milk -> {Diaper, Beer} (40% support, 50% confidence)
* {Diaper, Beer} -> Bread (40% support, 66% confidence)

*(also useful tool for exploratory data analysis)*

<!-- references -->
Further readings: Standard algorithms and many variations, see [Wikipedia](https://en.wikipedia.org/wiki/Association_rule_learning)

</script></section><section data-markdown><script type="text/template">## Excursion: Daikon for dynamic detection of likely invariants

* Software engineering technique to find invariants
  * e.g., `i>0`, `a==x`, `this.stack != null`, `db.query() after db.prepare()`
  * Pre- and post-conditions of functions, local variables
* Uses for documentation, avoiding bugs, debugging, testing, verification, repair
* Idea: Observe many executions (instrument code), log variable values, look for relationships (test many possible invariants)</script></section><section data-markdown><script type="text/template">## Daikon Example
<!-- colstart -->
```c
int ABS(int x) {
    if (x>0) return x;
    else return (x*(-1));
}
int main () {
    int i=0;
    int abs_i;
    for (i=-5000;i<5000;i++)
        abs_i=ABS(i);
}
```

Expected: `Return value of ABS(x) == (x>0) ? x: -x;`
<!-- col -->
```text
==================
std.ABS(int;):::ENTER
==================
std.ABS(int;):::EXIT1
x == return
==================
std.ABS(int;):::EXIT2
return == - x
==================
std.ABS(int;):::EXIT
x == orig(x)
x <= return
==================
```
<!-- colend -->

<aside class="notes"><p>many examples in <a href="https://www.cs.cmu.edu/~aldrich/courses/654-sp07/tools/kim-daikon-02.pdf">https://www.cs.cmu.edu/~aldrich/courses/654-sp07/tools/kim-daikon-02.pdf</a></p>
</aside></script></section><section data-markdown><script type="text/template">## Probabilistic Repair

* Use rules to identify inconsistencies and the more likely fix
* If confidence high enough, apply automatically
* Show suggestions to end users (like spell checkers) or data scientists
* Many tools in this area
</script></section><section data-markdown><script type="text/template">## [HoloClean](http://www.holoclean.io/)

![HoloClean](holoclean.jpg)


[HoloClean: Data Quality Management with Theodoros Rekatsinas](https://softwareengineeringdaily.com/2020/06/02/holoclean-data-quality-management-with-theodoros-rekatsinas/), SEDaily Podcast, 2020
</script></section><section data-markdown><script type="text/template">## Discussion: Data Quality Rules in Inventory System

![Shelves in a warehouse](warehouse.jpg)
<!-- .element: class="stretch" -->
 



</script></section></section><section ><section data-markdown><script type="text/template"># Data Linter

<!-- references -->
Further readings: Nick Hynes, D. Sculley, Michael Terry. "[The Data Linter: Lightweight Automated Sanity Checking for ML Data Sets](http://learningsys.org/nips17/assets/papers/paper_19.pdf)."  NIPS Workshop on ML Systems (2017)</script></section><section data-markdown><script type="text/template">## Excursion: Static Analysis and Code Linters

*Automate routine inspection tasks*

```js
if (user.jobTitle = "manager") {
   ...
}
```

```js
function fn() {
    x = 1;
    return x;
    x = 3; // dead code
}
```

```java
PrintWriter log = null;
if (anyLogging) log = new PrintWriter(...);
if (detailedLogging) log.println("Log started");
```
</script></section><section data-markdown><script type="text/template">## Static Analysis

* Analyzes the structure/possible executions of the code, without running it
* Different levels of sophistication
  * Simple heuristic and code patterns (linters)
  * Sound reasoning about all possible program executions
* Tradeoff between false positives and false negatives
* Often supporting annotations needed (e.g., `@Nullable`)
* Tools widely available, open source and commercial

![Eslint IDE integration](eslint.png)
</script></section><section data-markdown><script type="text/template">## A Linter for Data?

<!-- discussion -->
</script></section><section data-markdown><script type="text/template">## Data Linter at Google

* Miscoding
    * Number, date, time as string
    * Enum as real
    * Tokenizable string (long strings, all unique)
    * Zip code as number
* Outliers and scaling
    * Unnormalized feature (varies widely)
    * Tailed distributions
    * Uncommon sign
* Packaging
    * Duplicate rows
    * Empty/missing data

<!-- references -->
Further readings: Hynes, Nick, D. Sculley, and Michael Terry. [The data linter: Lightweight, automated sanity checking for ML data sets](http://learningsys.org/nips17/assets/papers/paper_19.pdf). NIPS MLSys Workshop. 2017.











</script></section></section><section ><section data-markdown><script type="text/template">
# Detecting Drift</script></section><section data-markdown><script type="text/template">## Drift & Model Decay

*in all cases, models are less effective over time*

* Concept drift
  * properties to predict change over time (e.g., what is credit card fraud)
  * over time: different expected outputs for same inputs
  * model has not learned the relevant concepts
* Data drift 
  * characteristics of input data changes (e.g., customers with face masks)
  * input data differs from training data 
  * over time: predictions less confident, further from training data
* Upstream data changes 
  * external changes in data pipeline (e.g., format changes in weather service)
  * model interprets input data incorrectly
  * over time: abrupt changes due to faulty inputs

**how to fix?**

<aside class="notes"><ul>
<li>fix1: retrain with new training data or relabeled old training data<ul>
<li>fix2: retrain with new data</li>
<li>fix3: fix pipeline, retrain entirely</li>
</ul>
</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">## On Terminology

* Concept and data drift are separate concepts
* In practice and literature not always clearly distinguished
* Colloquially encompasses all forms of model degradations and environment changes
* Define term for target audience


</script></section><section data-markdown><script type="text/template">## Watch for Degradation in Prediction Accuracy

![Model Drift](model_drift.jpg)

<!-- references -->
Image source: Joel Thomas and Clemens Mewald. [Productionizing Machine Learning: From Deployment to Drift Detection](https://databricks.com/blog/2019/09/18/productionizing-machine-learning-from-deployment-to-drift-detection.html). Databricks Blog, 2019


</script></section><section data-markdown><script type="text/template">## Indicators of Concept Drift

*How to detect concept drift in production?*

<!-- discussion -->
</script></section><section data-markdown><script type="text/template">## Indicators of Concept Drift

* Model degradations observed with telemetry
* Telemetry indicates different outputs over time for similar inputs
* Relabeling training data changes labels
* Interpretable ML models indicate rules that no longer fit

*(many papers on this topic, typically on statistical detection)*


</script></section><section data-markdown><script type="text/template">## Dealing with Drift

* Regularly retrain model on recent data
  - Use evaluation in production to detect decaying model performance
* Involve humans when increasing inconsistencies detected
  - Monitoring thresholds, automation
* Monitoring, monitoring, monitoring!

</script></section><section data-markdown><script type="text/template">## Different forms of Data Drift

* Structural drift
  * Data schema changes, sometimes by infrastructure changes
  * e.g., `4124784115` -> `412-478-4115`
* Semantic drift
  * Meaning of data changes, same schema
  * e.g., Netflix switches from 5-star to +/- rating, but still uses 1 and 5
* Distribution changes
  * e.g., credit card fraud differs to evade detection
  * e.g., marketing affects sales of certain items
*
* **Other examples?**


</script></section><section data-markdown><script type="text/template">## Detecting Data Drift

* Compare distributions over time (e.g., t-test)
* Detect both sudden jumps and gradual changes
* Distributions can be manually specified or learned (see invariant detection)

<!-- colstart -->
![Two distributions](twodist.png)
<!-- col -->
![Time series with confidence intervals](timeseries.png)
<!-- colend -->

</script></section><section data-markdown><script type="text/template">## Data Distribution Analysis

* Plot distributions of features (histograms, density plots, kernel density estimation)
  - identify which features drift
* Define distance function between inputs and identify distance to closest training data (eg., wasserstein and energy distance, see also kNN)
* Formal models for *data drift contribution* etc exist
* Anomaly detection and "out of distribution" detection
* Observe distribution of output labels
</script></section><section data-markdown><script type="text/template">## Data Distribution Example

https://rpubs.com/ablythe/520912

</script></section><section data-markdown><script type="text/template">## Microsoft Azure Data Drift Dashboard

![Dashboard](drift-ui-expanded.png)

<!-- references -->
Image source and further readings: [Detect data drift (preview) on models deployed to Azure Kubernetes Service (AKS)](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-data-drift)
</script></section><section data-markdown><script type="text/template">## Discussion: Inventory System

*What kind of drift might be expected? What kind of detection/monitoring?*

![Shelves in a warehouse](warehouse.jpg)
<!-- .element: class="stretch" -->









</script></section></section><section ><section data-markdown><script type="text/template"># Data Programming & Weakly-Supervised Learning

*Programmatically Build and Manage Training Data*

![Snorkel Logo](snorkel.png)

</script></section><section data-markdown><script type="text/template">## Weak Supervision -- Key Ideas

* Labeled data is expensive, unlabled data is often widely available
* Different labelers with different cost and accuracy/precision
  - crowd sourcing vs. med students vs. trained experts in labeling cancer diagnoses
* Often heuristics can define labels for some data (*labeling functions*)
  - hard coded heuristics (e.g., regular expressions)
  - distant supervision with external knowledge bases
  - noisy manual labels with crowd sourcing
  - external models providing some predictions
* Combine signals from labeling functions to automatically label training data
</script></section><section data-markdown><script type="text/template">## Labeling Function

For binary label, vote 1 (spam) or 0 (not spam) or -1 (abstain).

```py
from snorkel.labeling import labeling_function


@labeling_function()
def lf_keyword_my(x):
    """Many spam comments talk about 'my channel', 'my video'."""
    return SPAM if "my" in x.text.lower() else ABSTAIN
```

```py
@labeling_function()
def lf_textblob_polarity(x):
    """We use a third-party sentiment classification model."""
    return NOT_SPAM if 
      TextBlob(x.text).sentiment.polarity > 0.3 else ABSTAIN
```

Can also represent constraints and invariants if known

<aside class="notes"><p>More details: <a href="https://www.snorkel.org/get-started/">https://www.snorkel.org/get-started/</a></p>
</aside></script></section><section data-markdown><script type="text/template">## Snorkel



![Snorkel Overview](snorkel-steps.png)

*Generative model* learns which labeling functions to trust and when (~ from correlations). Learns "expertise" of labeling functions.

Generative model used to provide *probabilistic* training labels. *Discriminative model* learned from labeled training data; generalizes beyond label functions. 


<!-- references -->

https://www.snorkel.org/, https://www.snorkel.org/blog/snorkel-programming; 
Ratner, Alexander, et al. "[Snorkel: rapid training data creation with weak supervision](https://link.springer.com/article/10.1007/s00778-019-00552-1)." The VLDB Journal 29.2 (2020): 709-730.

<aside class="notes"><p>Emphasize the two different models. One could just let all labelers vote, but generative model identifies common correlations and disagreements and judges which labelers to trust when (also provides feedback to label function authors), resulting in better labels.</p>
<p>The generative model could already make predictions, but it is coupled tightly to the labeling functions. The discriminative model is a traditional model learned on labeled training data and thus (hopefully) generalizes beyond the labeling functions. It may actually pick up on very different signals. Typically this is more general and robust for unseen data.</p>
</aside></script></section><section data-markdown><script type="text/template">## Data Programming Beyond Labeling Training Data

* Potentially useful in many other scenarios
* Data cleaning
* Data augmentation
* Identifying important data subsets
<!-- split -->

![Snorkel Applications](https://www.snorkel.org/doks-theme/assets/images/layout/Overview.png)

</script></section><section data-markdown><script type="text/template">## Data Programming in Inventory System?

![Shelves in a warehouse](warehouse.jpg)
<!-- .element: class="stretch" -->
</script></section><section data-markdown><script type="text/template">## Data Programming for Detecting Toxic Comments in Youtube?

<!-- discussion -->






</script></section></section><section ><section data-markdown><script type="text/template"># Quality Assurance for the Data Processing Pipelines
</script></section><section data-markdown><script type="text/template">## Error Handling and Testing in Pipeline

Avoid silent failures!

* Write testable data acquisition and feature extraction code
* Test this code (unit test, positive and negative tests)
* Test retry mechanism for acquisition + error reporting
* Test correct detection and handling of invalid input
* Catch and report errors in feature extraction
* Test correct detection of data drift
* Test correct triggering of monitoring system
* Detect stale data, stale models

*More in a later lecture.*
</script></section></section><section  data-markdown><script type="text/template"># Summary

* Data and data quality are essential
* Data from many sources, often inaccurate, imprecise, inconsistent, incomplete, ... -- many different forms of data quality problems 
* Understand the data with exploratory data analysis
* Many mechanisms for enforcing consistency and cleaning 
  * Data schema ensures format consistency
  * Data quality rules ensure invariants across data points
  * Data linter detects common problems
* Concept and data drift are key challenges -- monitor
* Data programming to create training labels at scale with weak supervision
* Quality assurance for the data processing pipelines

 
</script></section></div>
    </div>

    <script src="./../js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './../plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './../plugin/zoom-js/zoom.js', async: true },
        { src: './../plugin/notes/notes.js', async: true },
        { src: './../plugin/math/math.js' },
        { src: './../rplugin/embed-tweet/embed-tweet.js' },
        { src: './../rplugin/menu/menu.js', async: true },
        { src: './../rplugin/spreadsheet/spreadsheet.js' },
        { src: './../rplugin/chalkboard/chalkboard.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps,
	chalkboard: { // font-awesome.min.css must be available
		toggleChalkboardButton: { left: "80px" },
		toggleNotesButton: { left: "130px" },
	},
	keyboard: {
	    67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle notes canvas when 'c' is pressed
	    66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
	    46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
	     8: function() { RevealChalkboard.reset() },	// reset chalkboard data on current slide when 'BACKSPACE' is pressed
	    68: function() { RevealChalkboard.download() },	// downlad recorded chalkboard drawing when 'd' is pressed
	    88: function() { RevealChalkboard.colorNext() },	// cycle colors forward when 'x' is pressed
	    89: function() { RevealChalkboard.colorPrev() },	// cycle colors backward when 'y' is pressed
	}
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"theme":"white","slideNumber":true,"hash":true,"center":false}, queryOptions);
    </script>

    <script src="./../_assets/_assets/mermaid.min.js"></script>
    <script src="./../_assets/_assets/loadmymarkdown.js"></script>

    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
