<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>17-445: Software Architecture of AI-Enabled Systems</title>
    <link rel="stylesheet" href="./../css/reveal.css" />
    <link rel="stylesheet" href="./../css/theme/white.css" id="theme" />
    <link rel="stylesheet" href="./../css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./../css/print/paper.css" type="text/css" media="print" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
     <script src="./../rplugin/spreadsheet/ruleJS.all.full.min.js"></script>
   <link rel="stylesheet" href="./../rplugin/spreadsheet/spreadsheet.css">
    <link rel="stylesheet" href="./../_assets/_assets/cmu.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><div id="footer">17-445 Software Engineering for AI-Enabled Systems, Christian Kaestner</div><section  data-markdown><script type="text/template">



# Software Architecture of AI-enabled Systems

Christian Kaestner

<!-- references -->

Required reading: 
* üïÆ Hulten, Geoff. "[Building Intelligent Systems: A Guide to Machine Learning Engineering.](https://www.buildingintelligentsystems.com/)" Apress, 2018, Chapter 13 (Where Intelligence Lives).
* üì∞ Daniel Smith. "[Exploring Development Patterns in Data Science](https://www.theorylane.com/2017/10/20/some-development-patterns-in-data-science/)." TheoryLane Blog Post. 2017.

</script></section><section  data-markdown><script type="text/template">
# Learning Goals


* Create architectural models to reason about relevant characteristics
* Critique the decision of where an AI model lives (e.g., cloud vs edge vs hybrid), considering the relevant tradeoffs 
* Deliberate how and when to update models and how to collect telemetry
</script></section><section ><section data-markdown><script type="text/template">
# Software Architecture 

```mermaid
graph LR;
Requirements --> m((Miracle / genius developers))
m --> Implementation
```

</script></section><section data-markdown><script type="text/template">
# Software Architecture 

```mermaid
graph LR;
Requirements --> Architecture
Architecture --> Implementation
```

Focused on reasoning about tradeoffs and desired qualities</script></section><section data-markdown><script type="text/template">## Software Architecture

> The software architecture of a program or computing system is the **structure or structures** of the system, which comprise **software elements**, the ***externally visible properties*** of those elements, and the relationships among them.
> -- [Kazman et al. 2012](https://www.oreilly.com/library/view/software-architecture-in/9780132942799/?ar)
</script></section><section data-markdown><script type="text/template">
## Why Architecture? ([Kazman et al. 2012](https://www.oreilly.com/library/view/software-architecture-in/9780132942799/?ar))

* Represents earliest design decisions.
* Aids in **communication** with stakeholders
    * Shows them ‚Äúhow‚Äù at a level they can understand, raising questions about whether it meets their needs
* Defines **constraints** on implementation
    * Design decisions form ‚Äúload-bearing walls‚Äù of application
* Dictates **organizational structure**
    * Teams work on different components
* Inhibits or enables **quality attributes**
    * Similar to design patterns
* Supports **predicting** cost, quality, and schedule
    * Typically by predicting information for each component
* Aids in software **evolution**
    * Reason about cost, design, and effect of changes
* Aids in **prototyping**
    * Can implement architectural skeleton early
</script></section><section data-markdown><script type="text/template">
## Case Study: Twitter

![twitter](twitter.png)

<aside class="notes"><p>Source and additional reading: Raffi. <a href="https://blog.twitter.com/engineering/en_us/a/2013/new-tweets-per-second-record-and-how.html">New Tweets per second record, and how!</a> Twitter Blog, 2013</p>
</aside></script></section><section data-markdown><script type="text/template">
## Twitter - Caching Architecture

![twitter](twitter-caching.png)
<!-- .element: class="stretch" -->

<aside class="notes"><ul>
<li>Running one of the world‚Äôs largest Ruby on Rails installations</li>
<li>200 engineers</li>
<li>Monolithic: managing raw database, memcache, rendering the site, and * presenting the public APIs in one codebase</li>
<li>Increasingly difficult to understand system; organizationally challenging to manage and parallelize engineering teams</li>
<li>Reached the limit of throughput on our storage systems (MySQL); read and write hot spots throughout our databases</li>
<li>Throwing machines at the problem; low throughput per machine (CPU + RAM limit, network not saturated)</li>
<li>Optimization corner: trading off code readability vs performance</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">
## Twitter's Redesign Goals

* Performance
    * Improve median latency; lower outliers 
    * Reduce number of machines 10x
+ Reliability
    * Isolate failures
+ Maintainability
    * "We wanted cleaner boundaries with ‚Äúrelated‚Äù logic being in one place": 
encapsulation and modularity at the systems level (rather than at the class, module, or package level)
* Modifiability
    * Quicker release of new features: "run small and empowered engineering teams that could make local decisions and ship user-facing changes, independent of other teams"

<!-- references -->

Raffi. [New Tweets per second record, and how!](https://blog.twitter.com/engineering/en_us/a/2013/new-tweets-per-second-record-and-how.html) Twitter Blog, 2013
</script></section><section data-markdown><script type="text/template">
## Twitter: Redesign Decisions

* Ruby on Rails -> JVM/Scala 
* Monolith -> Microservices
* RPC framework with monitoring, connection pooling, failover strategies, loadbalancing, ... built in
* New storage solution, temporal clustering, "roughly sortable ids"
* Data driven decision making

<!-- split -->

![Gizzard](gizzard.png)
<!-- .element: class="stretch" -->
</script></section><section data-markdown><script type="text/template">
## Twitter Case Study: Key Insights

* Architectural decisions affect entire systems, not only individual modules
* Abstract, different abstractions for different scenarios
* Reason about quality attributes early
* Make architectural decisions explicit
* Question: **Did the original architect make poor decisions?**

</script></section></section><section ><section data-markdown><script type="text/template">
# Architectural Modeling and Reasoning</script></section><section data-markdown><script type="text/template">![](pgh.png)
<aside class="notes"><p>Map of Pittsburgh. Abstraction for navigation with cars.</p>
</aside></script></section><section data-markdown><script type="text/template">![](pgh-cycling.png)
<aside class="notes"><p>Cycling map of Pittsburgh. Abstraction for navigation with bikes and walking.</p>
</aside></script></section><section data-markdown><script type="text/template">![](pgh-firezones.png)
<aside class="notes"><p>Fire zones of Pittsburgh. Various use cases, e.g., for city planners.</p>
</aside></script></section><section data-markdown><script type="text/template">## Analysis-Specific Abstractions

* All maps were abstractions of the same real-world construct
* All maps were created with different goals in mind
    - Different relevant abstractions
    - Different reasoning opportunities
* 
* Architectural models are specific system abstractions, for reasoning about specific qualities
* No uniform notation
</script></section><section data-markdown><script type="text/template">
## What can we reason about?

![](lan-boundary.png)
<!-- .element: class="stretch" -->
</script></section><section data-markdown><script type="text/template">
## What can we reason about?

![](gfs.png)

<!-- references -->
Ghemawat, Sanjay, Howard Gobioff, and Shun-Tak Leung. "[The Google file system.](https://ai.google/research/pubs/pub51.pdf)" ACM SIGOPS operating systems review. Vol. 37. No. 5. ACM, 2003.

<aside class="notes"><p>Scalability through redundancy and replication; reliability wrt to single points of failure; performance on edges; cost</p>
</aside></script></section><section data-markdown><script type="text/template">
## Modeling Recommendations

* Use notation suitable for analysis
* Document meaning of boxes and edges in legend
* Graphical or textual both okay; whiteboard sketches often sufficient
* Formal notations available














</script></section></section><section ><section data-markdown><script type="text/template">
# Case Study: Augmented Reality Translation


![Seoul Street Signs](seoul.jpg)
<!-- .element: class="stretch" -->


<aside class="notes"><p>Image: <a href="https://pixabay.com/photos/nightlife-republic-of-korea-jongno-2162772/">https://pixabay.com/photos/nightlife-republic-of-korea-jongno-2162772/</a></p>
</aside></script></section><section data-markdown><script type="text/template">## Case Study: Augmented Reality Translation
![Google Translate](googletranslate.png)</script></section><section data-markdown><script type="text/template">## Case Study: Augmented Reality Translation
![Google Glasses](googleglasses.jpg)
<aside class="notes"><p>Consider you want to implement an instant translation service similar toGoogle translate, but run it on embedded hardware in glasses as an augmented reality service.</p>
</aside></script></section><section data-markdown><script type="text/template">## Qualities of Interest?

<!-- discussion -->

</script></section></section><section  data-markdown><script type="text/template"># Architectural Decision: Selecting AI Techniques

What AI techniques to use and why? Tradeoffs?

![](googletranslate.png)

<aside class="notes"><p>Relate back to previous lecture about AI technique tradeoffs, including for example
Accuracy
Capabilities (e.g. classification, recommendation, clustering‚Ä¶)
Amount of training data needed
Inference latency
Learning latency; incremental learning?
Model size
Explainable? Robust?</p>
</aside></script></section><section ><section data-markdown><script type="text/template"># Architectural Decision: Where Should the Model Live?
</script></section><section data-markdown><script type="text/template">## Where Should the Model Live?

* Glasses
* Phone
* Cloud

What qualities are relevant for the decision?

<!-- split -->
![](googletranslate.png)

<aside class="notes"><p>Trigger initial discussion</p>
</aside></script></section><section data-markdown><script type="text/template">## Considerations

* How much data is needed as input for the model?
* How much output data is produced by the model?
* How fast/energy consuming is model execution?
* What latency is needed for the application?
* How big is the model? How often does it need to be updated?
* Cost of operating the model? (distribution + execution)
* Opportunities for telemetry?
* What happens if users are offline?
</script></section><section data-markdown><script type="text/template">## Exercise: Latency and Bandwidth Analysis of AR Translation

1. Identify key components of a solution and their interactions

2. Estimate latency and bandwidth requirements between components

3. Discuss tradeoffs among different deployment models

<!-- discussion -->

<aside class="notes"><p>Identify at least OCR and Translation service as two AI components in a larger system. Discuss which system components are worth modeling (e.g., rendering, database, support forum). Discuss how to get good estimates for latency and bandwidth.</p>
<p>Some data:
200ms latency is noticable as speech pause; 
20ms is perceivable as video delay, 10ms as haptic delay;
5ms referenced as cybersickness threshold for virtual reality
20ms latency might be acceptable</p>
<p>bluetooth latency around 40ms to 200ms</p>
<p>bluetooth bandwidth up to 3mbit, wifi 54mbit, video stream depending on quality 4 to 10mbit for low to medium quality</p>
<p>google glasses had 5 megapixel camera, 640x360 pixel screen, 1 or 2gb ram, 16gb storage</p>
</aside></script></section><section data-markdown><script type="text/template">## When would one use the following designs?

* Static intelligence in the product
* Client-side intelligence
* Server-centric intelligence
* Back-end cached intelligence
* Hybrid models


<aside class="notes"><p>From the reading:</p>
<ul>
<li>Static intelligence in the product<ul>
<li>difficult to update</li>
<li>good execution latency</li>
<li>cheap operation</li>
<li>offline operation</li>
<li>no telemetry to evaluate and improve</li>
</ul>
</li>
<li>Client-side intelligence<ul>
<li>updates costly/slow, out of sync problems</li>
<li>complexity in clients</li>
<li>offline operation, low execution latency</li>
</ul>
</li>
<li>Server-centric intelligence<ul>
<li>latency in model execution (remote calls)</li>
<li>easy to update and experiment</li>
<li>operation cost</li>
<li>no offline operation</li>
</ul>
</li>
<li>Back-end cached intelligence<ul>
<li>precomputed common results</li>
<li>fast execution, partial offline </li>
<li>saves bandwidth, complicated updates</li>
</ul>
</li>
<li>Hybrid models</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">## More Considerations

* Coupling of ML pipeline parts
* Coupling with other parts of the system
* Ability for different developers and analysts to collaborate
* Support online experiments
* Ability to monitor






</script></section></section><section ><section data-markdown><script type="text/template"># Architectural Decision: Telemetry Requirements</script></section><section data-markdown><script type="text/template">## Telemetry Design

How to evaluate mistakes in production?

![](googletranslate.png)

<aside class="notes"><p>Discuss strategies to determine accuracy in production. What kind of telemetry needs to be collected?</p>
</aside></script></section><section data-markdown><script type="text/template">## The Right and Right Amount of Telemetry

Purpose:
* Monitor operation
* Monitor mistakes (e.g., accuracy)
* Improve models over time (e.g., detect new features)

Challenges:
* too much data
* no/not enough data
* hard to measure, poor proxy measures
* rare events
* cost
* privacy</script></section><section data-markdown><script type="text/template">## Telemetry Tradeoffs

What data to collect? How much? When?

Estimate data volume and possible bottlenecks in system.

![](googletranslate.png)

<aside class="notes"><p>Discuss alternatives and their tradeoffs. Draw models as suitable.</p>
<p>Some data for context:
Full-screen png screenshot on Pixel 2 phone (1080x1920) is about 2mb (2 megapixel); Google glasses had a 5 megapixel camera and a 640x360 pixel screen, 16gb of storage, 2gb of RAM. Cellar cost are about $10/GB.</p>
</aside></script></section><section data-markdown><script type="text/template">## Related: Cost of Data and Feature Engineering

* How much data do we acquire for training and evaluating models?
* What data sources at what scale and latency (considering engineering cost, storage cost, processing cost, license cost, ...)
* Is it worth investing more time in feature engineering? What if additional data sources are needed?
* What is the cost for cleaning, preprocessing the data and the value of the additional accuracy?





</script></section></section><section ><section data-markdown><script type="text/template"># Architectural Decision: Independent Model Service

Microservice architecture: 

Model Inference and Model Learning as a RESTful Service?
</script></section><section data-markdown><script type="text/template">## Coupling and Changeability

What's the interface between the AI component and the rest of the system?

* Learning data and process
* Inference API
    - Where does feature extraction happen? 
    - Provide raw data (images, user profile, all past purchases) to service, grant access to shared database, or provide feature vector?
    - Cost of feature extraction? Who bears the cost?
    - Versioned interface?
* Coupling to other models? Direct coupling to data sources (e.g., files, databases)? Expected formats for raw data (e.g., image resolution)?
* Coupling to telemetry?
</script></section><section data-markdown><script type="text/template">## Model Service API

Consider encapsulating the model as a microservice. Sketch a (REST) API.

![](googletranslate.png)
</script></section><section data-markdown><script type="text/template">## Future-Proofing an API

* Anticipating and encapsulating change
    - What parts around the model service are likely to change?
    - Rigid vs flexible data formats?
* Versioning of APIs
    - Version numbers vs immutable services?
    - Expecting to run multiple versions in parallel? Implications for learning and evolution?
</script></section><section data-markdown><script type="text/template">## Robustness

* Redundancy for availability?
* Load balancer for scalability?
* Can mistakes be isolated?
    - Local error handling?
    - Telemetry to isolate errors to component?
* Logging and log analysis for what qualities?










</script></section></section><section ><section data-markdown><script type="text/template"># Architectural Decision: Updating Models

* Design for change!
* Models are rarely static outside the lab
* Data drift, feedback loops, new features, new requirements
* When and how to update models?
* How to version? How to avoid mistakes?
</script></section><section data-markdown><script type="text/template">## Risk of Stale Models

What could happen if models become stale?

![](googletranslate.png)

Risk: Discuss drift, adversarial interactions, feedback loops</script></section><section data-markdown><script type="text/template">## Update Requirements or Goals

Estimate the required update frequency and the related cost regarding training, data transfer, etc.

![](googletranslate.png)

<aside class="notes"><p>Discuss how frequently the involved models need to be updated. Are static models acceptable? Identify what information to collect and estimate 
the relevant values.</p>
</aside></script></section><section data-markdown><script type="text/template">## Outlook: Big Data Designs

Stream + Batch Processing

![Lambda Architecture](lambda.png)






</script></section></section><section ><section data-markdown><script type="text/template"># Architectural Styles / Tactics / Design Patterns for AI Enabled Systems

(no standardization, *yet*)
</script></section><section data-markdown><script type="text/template">## Architectures and Patterns

* The Big Ass Script Architecture
* Decoupled multi-tiered architecture (data vs data analysis vs reporting; separate business logic from ML)
* Microservice architecture (multiple learning and inference services)
* Gateway Routing Architecture
* 
* Pipelines
* Data lake, lambda architecture
* Reuse between training and serving pipelines
* Continuous deployment, ML versioning, pipeline testing

<!-- references -->
* Daniel Smith. "[Exploring Development Patterns in Data Science](https://www.theorylane.com/2017/10/20/some-development-patterns-in-data-science/)." TheoryLane Blog Post. 2017.
* Washizaki, Hironori, Hiromu Uchida, Foutse Khomh, and Yann-Ga√´l Gu√©h√©neuc. "[Machine Learning Architecture and Design Patterns](http://www.washi.cs.waseda.ac.jp/wp-content/uploads/2019/12/IEEE_Software_19__ML_Patterns.pdf)." Draft, 2019
</script></section><section data-markdown><script type="text/template">## Anti-Patterns

* Big Ass Script Architecture
* Dead Experimental Code Paths
* Glue code
* Multiple Language Smell
* Pipeline Jungles
* Plain-Old Datatype Smell
* Undeclared Consumers



<!-- references -->
* Washizaki, Hironori, Hiromu Uchida, Foutse Khomh, and Yann-Ga√´l Gu√©h√©neuc. "[Machine Learning Architecture and Design Patterns](http://www.washi.cs.waseda.ac.jp/wp-content/uploads/2019/12/IEEE_Software_19__ML_Patterns.pdf)." Draft, 2019
* Sculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan Dennison. "[Hidden technical debt in machine learning systems](http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)." In Advances in neural information processing systems, pp. 2503-2511. 2015.






</script></section></section><section ><section data-markdown><script type="text/template">
# AI as a Service

Third-Party AI Components in the Cloud

AI Components as Microservices
</script></section><section data-markdown><script type="text/template">
## Readymade AI Components in the Cloud

* Data Infrastructure
    - Large scale data storage, databases, stream (MongoDB, Bigtable, Kafka)
* Data Processing
    - Massively parallel stream and batch processing (Sparks, Hadoop, ...)
    - Elastic containers, virtual machines (docker, AWS lambda, ...)
* AI Tools
    - Notebooks, IDEs, Visualization
    - Learning Libraries, Frameworks (tensorflow, torch, keras, ...)
* Models
    - Image, face, and speech recognition, translation
    - Chatbots, spell checking, text analytics
    - Recommendations, knowledge bases
</script></section><section data-markdown><script type="text/template">
![Azure AI Platform](azure.png)
</script></section><section data-markdown><script type="text/template">
## Build vs Buy

Hardware, software, models?

<!-- discussion -->

<aside class="notes"><p>Discuss privacy implications</p>
</aside></script></section></section><section  data-markdown><script type="text/template">
# Reflection

Qualities of interest? Important design tradeoffs? Decisions?

![](googletranslate.png)










</script></section><section  data-markdown><script type="text/template">
# Summary

* Software architecture is an established discipline to reason about design alternatives
* Understand relevant quality goals 
* Problem-specific modeling and analysis: Gather estimates, consider design alternatives, make tradeoffs explicit
* Examples of important design decision:
    - modeling technique to use
    - where to deploy the model
    - how and how much telemetry to collect
    - whether and how to modularize the model service
    - when and how to update models
    - build vs buy, cloud resources


</script></section><section ><section data-markdown><script type="text/template">
# Case Study 2: Uber Surge Prediction
![Surge Screen in Uber App](ubersurge.png)
<aside class="notes"><p>Consider you work at Uber and want to predict where rider demand is going to be high.</p>
</aside></script></section><section data-markdown><script type="text/template">## Qualities of Interest?

<!-- discussion -->
</script></section><section data-markdown><script type="text/template">

<!-- colstart -->
![](googletranslate.png)
<!-- col -->
![](ubersurge.png)
<!-- colend -->
</script></section><section data-markdown><script type="text/template">## Where Should the Model Live?

* Car
* Phone
* Cloud

What qualities are relevant for the decision? 

<!-- split -->
![](ubersurge.png)

<aside class="notes"><p>Trigger initial discussion</p>
</aside></script></section><section data-markdown><script type="text/template">## Telemetry Design

How to evaluate mistakes in production?

![](ubersurge.png)


<!-- 

TODO: Other discussions

When and *where* to learn:

Learn in cloud and release model regularly, or learn on device?
e.g. Tesla only learns in cloud and releases models after some QA
Apple learns face recognition in photos on devices from user labels, for privacy
Tinder may learn on device or in cloud -- discuss whether it matters


 --></script></section></section></div>
    </div>

    <script src="./../js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './../plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './../plugin/zoom-js/zoom.js', async: true },
        { src: './../plugin/notes/notes.js', async: true },
        { src: './../plugin/math/math.js' },
        { src: './../rplugin/embed-tweet/embed-tweet.js' },
        { src: './../rplugin/menu/menu.js', async: true },
        { src: './../rplugin/spreadsheet/spreadsheet.js' },
        { src: './../rplugin/chalkboard/chalkboard.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps,
	chalkboard: { // font-awesome.min.css must be available
		toggleChalkboardButton: { left: "80px" },
		toggleNotesButton: { left: "130px" },
	},
	keyboard: {
	    67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle notes canvas when 'c' is pressed
	    66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
	    46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
	     8: function() { RevealChalkboard.reset() },	// reset chalkboard data on current slide when 'BACKSPACE' is pressed
	    68: function() { RevealChalkboard.download() },	// downlad recorded chalkboard drawing when 'd' is pressed
	    88: function() { RevealChalkboard.colorNext() },	// cycle colors forward when 'x' is pressed
	    89: function() { RevealChalkboard.colorPrev() },	// cycle colors backward when 'y' is pressed
	}
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"theme":"white","slideNumber":true,"hash":true,"center":false}, queryOptions);
    </script>

    <script src="./../_assets/_assets/mermaid.min.js"></script>
    <script src="./../_assets/_assets/loadmymarkdown.js"></script>

    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
