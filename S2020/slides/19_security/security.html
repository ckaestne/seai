<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>17-445: Security, Adversarial Learning, and Privacy</title>
    <link rel="stylesheet" href="./../css/reveal.css" />
    <link rel="stylesheet" href="./../css/theme/white.css" id="theme" />
    <link rel="stylesheet" href="./../css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./../css/print/paper.css" type="text/css" media="print" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
     <script src="./../rplugin/spreadsheet/ruleJS.all.full.min.js"></script>
   <link rel="stylesheet" href="./../rplugin/spreadsheet/spreadsheet.css">
    <link rel="stylesheet" href="./../_assets/_assets/cmu.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><div id="footer">17-445 Software Engineering for AI-Enabled Systems, Christian Kaestner</div><section  data-markdown><script type="text/template">

# Security, Adversarial Learning, and Privacy

Christian Kaestner

with slides from Eunsuk Kang

<!-- references -->

Required reading: 
üïÆ Hulten, Geoff. "Building Intelligent Systems: A Guide to Machine Learning Engineering." (2018), Chapter 25 (Adversaries and Abuse)
üïÆ Agrawal, A., Gans, J., & Goldfarb, A. (2018). [*Prediction machines: the simple economics of artificial intelligence*](https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019698987304436). Harvard Business Press. Chapter 19 (Managing AI Risk)

Recommended reading: 
üóé Goodfellow, I., McDaniel, P., & Papernot, N. (2018). [Making machine learning robust against adversarial inputs](https://par.nsf.gov/servlets/purl/10111674). *Communications of the ACM*, *61*(7), 56-66. 
üóé Huang, L., Joseph, A. D., Nelson, B., Rubinstein, B. I., & Tygar, J. D. (2011, October). [Adversarial machine learning](http://www.blaine-nelson.com/research/pubs/Huang-Joseph-AISec-2011.pdf). In *Proceedings of the 4th ACM workshop on Security and artificial intelligence* (pp. 43-58). 
</script></section><section  data-markdown><script type="text/template"># Learning Goals

* Explain key concerns in security (in general and with regard to ML models)
* Analyze a system with regard to attacker goals, attack surface, attacker capabilities 
* Describe common attacks against ML models, including poisoning attacks, evasion attacks, leaking IP and private information
* Measure robustness of a prediction and a model
* Understand design opportunities to address security threats at the system level
* Identify security requirements with threat modeling
* Apply key design principles for secure system design
* Discuss the role of AI in securing software systems
</script></section><section  data-markdown><script type="text/template">## Security at the Model Level

* Various attack discussions, e.g. poisioning attacks
* Model robustness
* Attack detection
* ...

<!-- split -->
## Security at the System Level

* Requirements analysis
* System-level threat modeling
* Defense strategies beyond the model
* Security risks beyond the model
* ...
</script></section><section ><section data-markdown><script type="text/template"># Security
</script></section><section data-markdown><script type="text/template">## Elements of Security

* Security requirements (policies)
  * What does it mean for my system to be secure?
* Threat model
    * What are the attacker's goal, capability, and incentive?
* Attack surface
	* Which parts of the system are exposed to the attacker?
* Protection mechanisms
	* How do we prevent the attacker from compromising a security requirement?
</script></section><section data-markdown><script type="text/template">## Security Requirements

![](cia-triad.png)

* "CIA triad" of information security
* __Confidentiality__: Sensitive data must be accessed by authorized users only
* __Integrity__: Sensitive data must be modifiable by authorized users only
* __Availability__: Critical services must be available when needed by clients
</script></section><section data-markdown><script type="text/template">![Garmin Hack](garmin.jpg)

</script></section><section data-markdown><script type="text/template"># Other Security Properties 

* Authentication (no spoofing): Users are who they say they are
* Integrety (no tampering): Data is changed only through authorized processes
* Non-repudiation: Every change can be traced to who was responsible for it
* Confidentiality (no inform. disclosure): Information only accessible to authorized users
* Availability (no denial of service): Critical services must be available when needed by clients
* Authorization (no escalation of privilege): Only users with the right permissions can access a resource/perform an action
</script></section><section data-markdown><script type="text/template">## Example: College Admission System

![](admission-hack.png)
</script></section><section data-markdown><script type="text/template">## Confidentiality, integrity, or availability?

* Applications to the program can only be viewed by staff and faculty
in the department.
* The application site should be able to handle requests on the
day of the application deadline.
* Application decisions are recorded only by the faculty and staff.
* The application site should backup all applications in case of a
server failure.
* The acceptance notices can only be sent out by the program director.
</script></section><section data-markdown><script type="text/template">## CIA of an ML Model

*What are security concerns of a **ML model** for ranking applications?*


<!-- discussion -->

* __Confidentiality__: Sensitive data must be accessed by authorized users only
* __Integrity__: Sensitive data must be modifiable by authorized users only
* __Availability__: Critical services must be available when needed by clients


<aside class="notes"><p>Many examples:
Confidentiality attacks: try to infer sensitive labels for data (e.g. training instances)
Integrity: cause a model to misclassify a data point, e.g. spam as nonspam
Availability attack: Misclassify many data points to make a model essentially useless</p>
</aside></script></section></section><section ><section data-markdown><script type="text/template"># Understanding Attacker Goals
</script></section><section data-markdown><script type="text/template">## Why Threat Model?

![](gate.png)
</script></section><section data-markdown><script type="text/template">## What is Threat Modeling?

* Threat model: A profile of an attacker
  * __Goal__: What is the attacker trying to achieve?
  * __Capability__:
	* Knowledge: What does the attacker know?
	* Actions: What can the attacker do?
	* Resources: How much effort can it spend? 
  * __Incentive__: Why does the attacker want to do this?


</script></section><section data-markdown><script type="text/template">## Attacker Goals and Incentives

* What is the attacker trying to achieve? Undermine one or more security requirements
* Why does the attacker want to do this?

*Example goals and incentives in Garmin/college admission scenario?*
	
<!-- discussion -->

<aside class="notes"><ul>
<li>Access other applicants info without being authorized<ul>
<li>Modify application status to ‚Äúaccepted‚Äù<ul>
<li>Submit applications that get accepted</li>
<li>Cause expense by making the model useless and forcing manual evaluations or poor outcomes</li>
</ul>
</li>
<li>Cause website shutdown to sabotage other applicants</li>
</ul>
</li>
</ul>
</aside></script></section></section><section ><section data-markdown><script type="text/template"># Attacks on ML Models


</script></section><section data-markdown><script type="text/template">## Scenario: Rankings and Reviews on Web Shop

![Amazon product results](amazon.png)
</script></section><section data-markdown><script type="text/template">```mermaid
graph LR

s(Search term) --> Model 
Model --> Results
Reviews --> Model
p(Product description) --> Model
sa(Past sales) --> Model
```


</script></section><section data-markdown><script type="text/template">## Scenario: Spam Filter

![Spam filter](spam.png)

</script></section><section data-markdown><script type="text/template">## Capabilities

**How can an attacker interact with / influence the model?**

<!-- discussion -->
</script></section><section data-markdown><script type="text/template">## Attack vectors

* Influence the training data ("causative attack", "poisioning attack")
* Influence the input data ("exploratory attack", "evasion attack")
* Influence the telemetry data

**Examples in spam filter scenario?**


</script></section><section data-markdown><script type="text/template">## Poisoning Attack: Availability

![](virus.png)

* Availability: Inject mislabeled training data to damage model
quality
    * 3% poisoning => 11% decrease in accuracy (Steinhardt, 2017)
* Attacker must have some access to the training set
    * models trained on public data set (e.g., ImageNet)
    * retrained automatically on telemetry

<aside class="notes"><ul>
<li>Example: Anti-virus (AV) scanner<ul>
<li>Online platform for submission of potentially malicious code<ul>
<li>Some AV company (allegedly) poisoned competitor&#39;s model</li>
</ul>
</li>
</ul>
</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">## Poisoning Attack: Integrity

![](spam.jpg)

* Insert training data with seemingly correct labels
* More targeted than availability attacks
  * Cause misclassification from one specific class to another

<!-- references -->
_Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural
Networks_, Shafahi et al. (2018)
</script></section><section data-markdown><script type="text/template">## Many Different Kinds of Attacks on Training Data

* Correlated outlier attack: add spurious features to malicious instances to misclassify benign instances 
* Red herring attack: add spurious features to early malicious instances, then send malicious payload without those features


</script></section><section data-markdown><script type="text/template">## Poisoning Attack in Web Shop?

![Amazon product results](amazon.png)


</script></section><section data-markdown><script type="text/template">## Defense against Poisoning Attacks

![](data-sanitization.png)


<!-- references -->

_Stronger Data Poisoning Attacks Break Data Sanitization Defenses_,
Koh, Steinhardt, and Liang (2018).


</script></section><section data-markdown><script type="text/template">## Defense against Poisoning Attacks


* Anomaly detection & data sanitization
  * Identify and remove outliers in training set
  * Identify and understand drift from telemetry
  * See [data quality lecture](https://ckaestne.github.io/seai/S2020/slides/11_dataquality/dataquality.html#/)
* Quality control over your training data
  * Who can modify or add to my training set? Do I trust the data
  source?
  * Use security mechanisms (e.g., authentication) and logging to
    track data provenance
* Slow down retraining, monitor model quality
* Debug models + explainability (e.g., influential instances)
* Use models that are robust against noisy training data



</script></section><section data-markdown><script type="text/template">## Attacks on Input Data (Evasion Attacks, Adversarial Examples)

![](evasion-attack.png)

* Add noise to an existing sample & cause misclassification
  - achieve specific outcome (evasion attack)
  - circumvent ML-based authentication like FaceID (impersonation attack)
* Attack at inference time

<!-- references -->

_Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art
Face Recognition_, Sharif et al. (2016).

</script></section><section data-markdown><script type="text/template">## Task Decision Boundary vs Model Boundary

[![Decision boundary vs model boundary](decisionboundary.png)](decisionboundary.png)
<!-- .element: class="stretch" -->

From Goodfellow et al (2018). [Making machine learning robust against adversarial inputs](https://par.nsf.gov/servlets/purl/10111674). *Communications of the ACM*, *61*(7), 56-66. </script></section><section data-markdown><script type="text/template">## Generating Adversarial Examples

* see [counterfactual explanations](https://ckaestne.github.io/seai/S2020/slides/16_explainability/explainability.html#/7/1)
* Find similar input with different prediction
  - targeted (specific prediction) vs untargeted (any wrong prediction)
* Many similarity measures (e.g., change one feature vs small changes to many features) 
  - $x^* = x + arg min \\{ |z| : f(x+z)=t \\}$
* Attacks more affective which access to model internals, but also black-box attacks (with many queries to the model) feasible
  - With model internals: follow the model's gradient
  - Without model internals: learn [surrogate model](https://ckaestne.github.io/seai/S2020/slides/16_explainability/explainability.html#/6/2)
  - With access to confidence scores: heuristic search (eg. hill climbing)

</script></section><section data-markdown><script type="text/template">## Example of Evasion Attacks

Spam scenario? Web store scenario? Credit scoring scenario?

![Amazon product results](amazon.png)
</script></section><section data-markdown><script type="text/template">## Recall: Gaming Models with Weak Features

*Does providing an explanation allow customers to 'hack' the system?*

* Loan applications?
* Apple FaceID?
* Recidivism?
* Auto grading?
* Cancer diagnosis?
* Spam detection?

**Gaming not possible if model boundary = task decision boundary**

<!-- split -->

<!-- discussion -->


</script></section><section data-markdown><script type="text/template">## Discussion: Can We Secure a System with a Known Model?

* Can we protect the model?
* How to prevent surrogate models?
* Security by obscurity?
*
* Alternative model hardening or system design strategies?
* 
<!-- discussion -->








</script></section></section><section ><section data-markdown><script type="text/template"># Excursion: Robustness

property with massive amount of research, in context of security and safety
</script></section><section data-markdown><script type="text/template">## Defining Robustness:

* A prediction for $x$ is robust if the outcome is stable under minor perturbations of the input
  - $\forall x'. d(x, x')<\epsilon \Rightarrow f(x) = f(x')$
  - distance function $d$ and permissible distance $\epsilon$ depends on problem
* A model is robust if most predictions are robust
</script></section><section data-markdown><script type="text/template">## Robustness and Distance for Images

+ slight rotation, stretching, or other transformations
+ change many pixels minimally (below human perception)
+ change only few pixels
+ change most pixels mostly uniformly, eg brightness

![Handwritten number transformation](handwriting-transformation.png)

<!-- references -->
Image: Singh, Gagandeep, Timon Gehr, Markus P√ºschel, and Martin Vechev. "[An abstract domain for certifying neural networks](https://dl.acm.org/doi/pdf/10.1145/3290354)." Proceedings of the ACM on Programming Languages 3, no. POPL (2019): 1-30. 
</script></section><section data-markdown><script type="text/template">## Robustness and Distance

* For text:
  - insert words
  - replace words with synonyms
  - reorder text
* For tabular data:
  - change values
  - depending on feature extraction, small changes may have large effects
* ...
  
*note, not all changes may be feasible or realistic; some changes are obvious to humans*

*realistically, a defender will not anticipate all attacks and corresponding distances*
</script></section><section data-markdown><script type="text/template">## No Model is Fully Robust

* Every useful model has at least one decision boundary (ideally at the real task decision boundary)
* Predictions near that boundary are not (and should not) be robust

![Decision boundary](decisionboundary2.svg)
</script></section><section data-markdown><script type="text/template">## Robustness of Interpretable Models


```
IF age between 18‚Äì20 and sex is male THEN predict arrest
ELSE 
IF age between 21‚Äì23 and 2‚Äì3 prior offenses THEN predict arrest
ELSE 
IF more than three priors THEN predict arrest
ELSE predict no arrest
```

<!-- references -->

Rudin, Cynthia. "[Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://arxiv.org/abs/1811.10154)." Nature Machine Intelligence 1, no. 5 (2019): 206-215.  


</script></section><section data-markdown><script type="text/template">## Decision Boundaries in Practice

* With many models (especially deep neural networks), we do not understand the model's decision boundaries
* We are not confident that model decision boundaries align with task decision boundaries
  - The model's perception does not align well with human perception
* Models may pick up on parts of the input in surprising ways


![](evasion-attack.png)
</script></section><section data-markdown><script type="text/template">## Assuring Robustness

* Much research, many tools and approaches (especially for DNN)
* Formal verification
  - Constraint solving or abstract interpretation over computations in neuron activations
  - Conservative abstraction, may label robust inputs as not robust
  - Currently not very scalable
  - Example: üóé Singh, Gagandeep, Timon Gehr, Markus P√ºschel, and Martin Vechev. "[An abstract domain for certifying neural networks](https://dl.acm.org/doi/pdf/10.1145/3290354)." Proceedings of the ACM on Programming Languages 3, no. POPL (2019): 1-30. 
* Sampling
  - Sample within distance, compare prediction to majority prediction
  - Probabilistic guarantees possible (with many queries, e.g., 100k)
  - Example: üóé Cohen, Jeremy M., Elan Rosenfeld, and J. Zico Kolter. "[Certified adversarial robustness via randomized smoothing](https://arxiv.org/abs/1902.02918)." In Proc. International Conference on Machine Learning, p. 1310--1320, 2019.
</script></section><section data-markdown><script type="text/template">## Practical Use of Robustness?


<!-- discussion -->

*Current abilities: Detect for a given input whether neighboring inputs predict same result*
</script></section><section data-markdown><script type="text/template">## Practical Use of Robustness

* Defense and safety mechanism at inference time
  - Check robustness of each prediction at runtime
  - Handle inputs with non-robust predictions differently (e.g. discard, low confidence)
  - Significantly raises cost of prediction (e.g. 100k model inferences or constraint solving at runtime)
* Testing and debugging
  - Identify training data near model's decision boundary (i.e., model robust around all training data?)
  - Check robustness on test data
  - Evaluate distance for adversarial attacks on test data

*(most papers on the topic focus on techniques and evaluate on standard benchmarks like handwitten numbers, but do not discuss practical scenarios)*
</script></section><section data-markdown><script type="text/template">## Increasing Model Robustness

* Augment training data with transformed versions of training data (same label) or with identified adversaries
* Defensive distillation: Second model trained on "soft" labels of first 
* Input transformations: Learning and removing adversarial transformations
* Inserting noise into model to make adversarial search less effective, mask gradients
* Dimension reduction: Reduce opportunity to learn spurious decision boundaries
* Ensemble learning: Combine models with different biases
* 
* Lots of research claiming effectiveness and vulnerabilities of various strategies

<!-- references -->
More details and papers: Rey Reza Wiyatno. [Securing machine learning models against adversarial attacks](https://www.elementai.com/news/2019/securing-machine-learning-models-against-adversarial-attacks). Element AI 2019
</script></section><section data-markdown><script type="text/template">## Detecting Adversaries

* Adversarial Classification: Train a model to distinguish benign and adversarial inputs
* Distribution Matching: Detect inputs that are out of distribution
* Uncertainty Thresholds: Measuring uncertainty estimates in the model for an input

<!-- references -->
More details and papers: Rey Reza Wiyatno. [Securing machine learning models against adversarial attacks](https://www.elementai.com/news/2019/securing-machine-learning-models-against-adversarial-attacks). Element AI 2019
</script></section><section data-markdown><script type="text/template">## Robustness in Web Store Scenario?

![Amazon product results](amazon.png)






</script></section></section><section ><section data-markdown><script type="text/template"># IP and Privacy
</script></section><section data-markdown><script type="text/template">
[![Article: Google Catches Bing Copying; Microsoft Says 'So What?'](wired_google.png)](https://www.wired.com/2011/02/bing-copies-google/)
</script></section><section data-markdown><script type="text/template">## Intellectual Property Protection

* Depending on deployment scenario
* May have access to model internals (e.g. in app binary)
* May be able to repeatedly query model's API
  - build surrogate model (*inversion attack*)
  - cost per query? rate limit? abuse detection?
* Surrogate models ease other forms of attacks
</script></section><section data-markdown><script type="text/template">[![Article: NetFlix Cancels Recommendation Contest After Privacy Lawsuit](wired_netflix.png)](https://www.wired.com/2010/03/netflix-cancels-contest/)

<aside class="notes"><p>&quot;an in-the-closet lesbian mother sued Netflix for privacy invasion, alleging the movie-rental company made it possible for her to be outed when it disclosed insufficiently anonymous information about nearly half-a-million customers as part of its $1 million contest.&quot;</p>
</aside></script></section><section data-markdown><script type="text/template">![Image recovered from a DNN](image_recovery.png)


<!-- references -->
Fredrikson, Matt, Somesh Jha, and Thomas Ristenpart. "[Model inversion attacks that exploit confidence information and basic countermeasures](http://www.cs.cmu.edu/~mfredrik/papers/fjr2015ccs.pdf)." In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, pp. 1322-1333. 2015.
</script></section><section data-markdown><script type="text/template">## Privacy

* Various privacy issues about acquiring and sharing training data, e.g.,
  - DeepMind receiving NHS data on 1.6 million patients without their consent
  - Chest X-rays not shared for training because they may identify people
  - Storage of voice recordings of voice assistants
* Model inversion attacks: Models contain information from training data, may recover information from training data
  - Extract DNA from medical model
  - Extract training images from face recognition model



<!-- references -->
Kyle Wiggers. [AI has a privacy problem, but these techniques could fix it](https://venturebeat.com/2019/12/21/ai-has-a-privacy-problem-but-these-techniques-could-fix-it/). Venturebeat, 2019
</script></section><section data-markdown><script type="text/template"># Generative Adversarial Networks

```mermaid
graph LR
 r[Real images] --> rs[Sample] 
 rs --> d[Discriminator]
 Generator --> gs[Sample]
 gs --> d
 d --> l[Disc. loss]
 d --> ll[Gen. loss]
 l -.->|backprop.| d
 style Generator fill:#bbf
 style d fill:#bbf
```
</script></section><section data-markdown><script type="text/template"># Prototypical inputs with GANs

[![Generated image of a woman](gan_women.jpg)](https://commons.wikimedia.org/wiki/File:Woman_2.jpg)
<!-- .element: class="stretch" -->

<aside class="notes"><ul>
<li>Generative adversarial networks: 2 models, one producing samples and one discriminating real from generated samples<ul>
<li>Learn data distribution of training data</li>
<li>Produce prototypical images, e.g. private jets</li>
<li>Deep fakes</li>
</ul>
</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">## Privacy Protection Strategies

* Federated learning (local models, no access to all data)
* Differential privacy (injecting noise to avoid detection of individuals)
* Homomorphic encryption (computing on encrypted data)
* 
* Much research
* Some adoption in practice (Android keyboard, Apple emoji)
* Usually accuracy or performance tradeoffs

<!-- references -->
Kyle Wiggers. [AI has a privacy problem, but these techniques could fix it](https://venturebeat.com/2019/12/21/ai-has-a-privacy-problem-but-these-techniques-could-fix-it/). Venturebeat, 2019





</script></section></section><section ><section data-markdown><script type="text/template"># Security at the System Level

*security is more than model robustness*

*defenses go beyond hardening models*
</script></section><section data-markdown><script type="text/template">![Temi Transcription Service](temi.png)

<aside class="notes"><p>At a price of $.25 per min it iss possibly not economical to train a surrogate model or inject bad telemetry</p>
</aside></script></section><section data-markdown><script type="text/template">![Amazon verified reviews](verifiedreviews.png)

<aside class="notes"><p>Raise the price of wrong inputs</p>
</aside></script></section><section data-markdown><script type="text/template">![Tweet with fact checking label](covidtwitter.webp)

<aside class="notes"><p>source <a href="https://www.buzzfeednews.com/article/pranavdixit/twitter-5g-coronavirus-conspiracy-theory-warning-label">https://www.buzzfeednews.com/article/pranavdixit/twitter-5g-coronavirus-conspiracy-theory-warning-label</a></p>
<p>Shadow banning also fits here</p>
</aside></script></section><section data-markdown><script type="text/template">![Youtube Strikes Rules](youtube_strikes.png)

<aside class="notes"><p>Block user of suspected attack to raise their cost, burn their resources</p>
</aside></script></section><section data-markdown><script type="text/template">![Youtube Spam](youtube_spam.png)

<aside class="notes"><p>Reporting function helps to crowdsource detection of malicious content and potentially train a future classifier (which again can be attacked)</p>
</aside></script></section><section data-markdown><script type="text/template">![Stack overflow post](stackoverflow.png)

<aside class="notes"><p>See reputation system</p>
</aside></script></section><section data-markdown><script type="text/template">![Too many attempts warning on Android](android_login.png)
<!-- .element: class="stretch" -->

<aside class="notes"><p>Block system after login attempts with FaceID or fingerprint</p>
</aside></script></section><section data-markdown><script type="text/template">## System Design Questions

* What is one simple change to make the system less interesting to abusers?
* Increase the cost of abuse, limit scale?
* Decrease the value of abuse?
* Trust established users over new users?
* Reliance on ML to combat abuse?
* Incidence response plan?

**Examples for web shop/college admissions AI?**

</script></section></section><section ><section data-markdown><script type="text/template"># Threat Modeling
</script></section><section data-markdown><script type="text/template">## Threat Modeling


* Attacker Profile
  * __Goal__: What is the attacker trying to achieve?
  * __Capability__:
  * Knowledge: What does the attacker know?
  * Actions: What can the attacker do?
  * Resources: How much effort can it spend? 
  * __Incentive__: Why does the attacker want to do this?
+ Understand how the attacker can interact with the system
+ Understand security strategies and their scope
+ **Identify security requirements**


	</script></section><section data-markdown><script type="text/template">## Attacker Capability

![](admission-threat-model.jpg)

* Capabilities depends on system boundary & its exposed interfaces
* Use an architecture diagram to identify attack surface & actions
* Example: Garmin/College admission
    * Physical: Break into building & access server
    * Cyber: Send malicious HTTP requests for SQL injection,
  DoS attack
    * Social: Send phishing e-mail, bribe an insider for access
  </script></section><section data-markdown><script type="text/template">## Architecture Diagram for Threat Modeling

![](admission-threat-model.jpg)

* Dynamic and physical architecture diagram
* Describes system components and users and their interactions
* Describe thrust boundaries

</script></section><section data-markdown><script type="text/template">## STRIDE Threat Modeling

![](stride.png)

* Systematic inspection to identifying threats & attacker actions
  * For each component/connection, enumerate & identify potential threats using checklist
  * e.g., Admission Server & DoS: Applicant may flood it with requests
  * Derive security requirements
* Tool available (Microsoft Threat Modeling Tool)
* Popularized by Microsoft, broadly used in practice

</script></section><section data-markdown><script type="text/template">## Open Web Application Security Project

![](owasp.png)

* OWASP: Community-driven source of knowledge & tools for web security

</script></section><section data-markdown><script type="text/template">## Threat Modeling Limitations

* Manual approach, false positives and false negatives
* May end up with a long list of threats, not all of them relevant
* Need to still correctly implement security requirements
* False sense of security: STRIDE does not imply completeness!
</script></section><section data-markdown><script type="text/template">## Threat Modeling Adjustments for AI?

<!-- discussion -->

</script></section><section data-markdown><script type="text/template">## Threat Modeling Adjustments for AI?

* Explicitly consider origins, access, and influence of all relevant data (training, prediction input, prediction result, model, telemetry)
* Consider AI-specific attacks
  - Poisoning attacks
  - Evasion attacks
  - Surrogate models
  - Privacy leaks
  - ...

</script></section><section data-markdown><script type="text/template">## State of ML Security

![](arms-race.jpg)

* On-going arms race (mostly among researchers)
    * Defenses proposed & quickly broken by noble attacks
* *Assume ML component is likely vulnerable*
    * Design your system to minimize impact of an attack
* Remember: There may be easier ways to compromise system
    * e.g., poor security misconfiguration (default password), lack of
    encryption, code vulnerabilities, etc., 
</script></section></section><section ><section data-markdown><script type="text/template"># Designing for Security
</script></section><section data-markdown><script type="text/template">## Secure Design Principles 

* Principle of Least Privilege
  * A component should be given the minimal privileges needed to fulfill its functionality
  * Goal: Minimize the impact of a compromised component
* Isolation
  * Components should be able to interact with each other no more than necessary
  * Goal: Reduce the size of trusted computing base (TCB) 
  * TCB: Components responsible for establishing a security requirement(s)
  * If any of TCB compromised => security violation
  * Conversely, a flaw in non-TCB component => security still preserved!
  * In poor system designs, TCB = entire system
</script></section><section data-markdown><script type="text/template">## Monolithic Design

![](monolithic1.png)


Flaw in any part of the system =>  Security impact on the entire system!

</script></section><section data-markdown><script type="text/template">## Compartmentalized Design

![](component-design2.png)

Flaw in one component =>  Limited impact on the rest of the system!
</script></section><section data-markdown><script type="text/template">## Non-ML Example: Mail Client

* Requirements
  * Receive & send email over external network
  * Place incoming email into local user inbox files
* Sendmail
  * Monolithic design; entire program runs as UNIX root
  * Historical source of many vulnerabilities
* Qmail: ‚ÄúSecurity-aware‚Äù mail agent
  + Compartmentalized design
  + Isolation based on OS process isolation
  + Separate modules run as separate ‚Äúusers‚Äù (UID)
  + Mutually distrusting processes
  + Least privilege 
  + Minimal privileges for each UID; access to specific resources (files, network sockets, ‚Ä¶)
  + Only one ‚Äúroot‚Äù user (with all privileges)
</script></section><section data-markdown><script type="text/template">## Qmail Architecture

![](qmail1.png)
</script></section><section data-markdown><script type="text/template">## Qmail Architecture

![](qmail2.png)
</script></section><section data-markdown><script type="text/template">## Qmail Architecture

![](qmail3.png)





</script></section></section><section ><section data-markdown><script type="text/template"># AI for Security
</script></section><section data-markdown><script type="text/template">[![Article: 30 COMPANIES MERGING AI AND CYBERSECURITY TO KEEP US SAFE AND SOUND](30aisec.png)](https://builtin.com/artificial-intelligence/artificial-intelligence-cybersecurity)
<!-- .element: class="stretch" -->
</script></section><section data-markdown><script type="text/template">## Many Defense Systems use Machine Learning

* Classifiers to learn malicious content
  - Spam filters, virus detection
* Anomaly detection
  - Identify unusual/suspicious activity, eg. credit card fraud, intrusion detection
  - Often unsupervised learning, e.g. clustering
* Game theory
  - Model attacker costs and reactions, design countermeasures
* Automate incidence response and mitigation activites
  - Integrated with DevOps
* Network analysis
  - Identify bad actors and their communication in public/intelligence data
* Many more, huge commercial interest

<!-- references -->

Recommended reading: Chandola, Varun, Arindam Banerjee, and Vipin Kumar. "[Anomaly detection: A survey](http://cucis.ece.northwestern.edu/projects/DMS/publications/AnomalyDetection.pdf)." ACM computing surveys (CSUR) 41, no. 3 (2009): 1-58.  
</script></section><section data-markdown><script type="text/template">## AI Security Solutions are AI-Enabled Systems Too

* AI/ML component one part of a larger system
* Consider entire system, from training to telemetry, to user interface, to pipeline automation, to monitoring
* AI-based security solutions can be attacked themselves
</script></section><section data-markdown><script type="text/template">![Equifax logo](equifax.png)

<aside class="notes"><p>One contributing factor to the Equifax attack was an expired certificate for an intrusion detection system</p>
</aside></script></section></section><section  data-markdown><script type="text/template"># Summary

* Security requirements: Confidentiality, integrity, availability
* ML-specific attacks on training data, telemetry, or the model
  - Poisoning attack on training data to influence predictions
  - Evasion attacks to shape input data to achieve intended predictions (adversarial learning)
  - Leaks of model IP (surrogates) and training data
* Robustness as a measure of prediction stability w.r.t to input perturbations; verification possible
* Security design at the system level
  - Influence costs and gains
  - Security mechanisms beyond the model
* Threat modeling to identify security requirements
* AI can be used for defense (e.g. anomaly detection)
* __Key takeaway__: Adopt a security mindset! Assume all components may be vulnerable in one way or another. Design your system to explicitly reduce the impact of potential attacks

<!-- smallish -->





</script></section></div>
    </div>

    <script src="./../js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './../plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './../plugin/zoom-js/zoom.js', async: true },
        { src: './../plugin/notes/notes.js', async: true },
        { src: './../plugin/math/math.js' },
        { src: './../rplugin/embed-tweet/embed-tweet.js' },
        { src: './../rplugin/menu/menu.js', async: true },
        { src: './../rplugin/spreadsheet/spreadsheet.js' },
        { src: './../rplugin/chalkboard/chalkboard.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps,
	chalkboard: { // font-awesome.min.css must be available
		toggleChalkboardButton: { left: "80px" },
		toggleNotesButton: { left: "130px" },
	},
	keyboard: {
	    67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle notes canvas when 'c' is pressed
	    66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
	    46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
	     8: function() { RevealChalkboard.reset() },	// reset chalkboard data on current slide when 'BACKSPACE' is pressed
	    68: function() { RevealChalkboard.download() },	// downlad recorded chalkboard drawing when 'd' is pressed
	    88: function() { RevealChalkboard.colorNext() },	// cycle colors forward when 'x' is pressed
	    89: function() { RevealChalkboard.colorPrev() },	// cycle colors backward when 'y' is pressed
	}
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"theme":"white","slideNumber":true,"hash":true,"center":false}, queryOptions);
    </script>

    <script src="./../_assets/_assets/mermaid.min.js"></script>
    <script src="./../_assets/_assets/loadmymarkdown.js"></script>

    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
