<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>17-445: Risk and Planning for Mistakes</title>
    <link rel="stylesheet" href="./../css/reveal.css" />
    <link rel="stylesheet" href="./../css/theme/white.css" id="theme" />
    <link rel="stylesheet" href="./../css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./../css/print/paper.css" type="text/css" media="print" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
     <script src="./../rplugin/spreadsheet/ruleJS.all.full.min.js"></script>
   <link rel="stylesheet" href="./../rplugin/spreadsheet/spreadsheet.css">
    <link rel="stylesheet" href="./../_assets/_assets/cmu.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><div id="footer">17-445 Software Engineering for AI-Enabled Systems, Christian Kaestner</div><section  data-markdown><script type="text/template">  

# Risk and Planning for Mistakes

Christian Kaestner

<!-- references -->
With slides adopted from Eunsuk Kang

Required reading: üïÆ Hulten, Geoff. "Building Intelligent Systems: A Guide to Machine Learning Engineering." (2018), Chapters 6‚Äì8 (Why creating IE is hard, balancing IE, modes of intelligent interactions) and 24 (Dealing with Mistakes)
</script></section><section  data-markdown><script type="text/template">## Learning goals:

* Analyze how mistake in an AI component can influence the behavior of a system
* Analyze system requirements at the boundary between the machine and world
* Evaluate risk of a mistake from the AI component using fault trees
* Design and justify a mitigation strategy for a concrete system
</script></section><section ><section data-markdown><script type="text/template"># Wrong Predictions
</script></section><section data-markdown><script type="text/template">
![Cancer detection](mri.jpg)
<!-- .element: class="stretch" -->
</script></section><section data-markdown><script type="text/template">![Uber crash](ubercrash.png)

</script></section><section data-markdown><script type="text/template">> Cops raid music fan‚Äôs flat after Alexa Amazon Echo device ‚Äòholds a party on its own‚Äô while he was out
Oliver Haberstroh's door was broken down by irate cops after neighbours complained about deafening music blasting from Hamburg flat

https://www.thesun.co.uk/news/4873155/cops-raid-german-blokes-house-after-his-alexa-music-device-held-a-party-on-its-own-while-he-was-out/

> News broadcast triggers Amazon Alexa devices to purchase dollhouses.

https://www.snopes.com/fact-check/alexa-orders-dollhouse-and-cookies/
</script></section><section data-markdown><script type="text/template">
![Tay Chat Bot deying Holocaust](tay.png)
</script></section><section data-markdown><script type="text/template">![Amazon Hiring Tool Scraped due to Bias](amazonhiring.png)
</script></section><section data-markdown><script type="text/template">## Your Examples?

<!-- discussion -->





</script></section></section><section ><section data-markdown><script type="text/template"># Sources of Wrong Predictions
</script></section><section data-markdown><script type="text/template">## Sources of Wrong Predictions?

<!-- discussion -->

</script></section><section data-markdown><script type="text/template">## Correlation vs Causation

![causation1](causation1.png)

![causation2](causation2.png)
</script></section><section data-markdown><script type="text/template">## Confounding Variables

<!-- colstart -->
```mermaid
graph TD;
IV[Independent Var.] -.->|spurious correlation| DV[Dependent Var.]
CV[Confounding Var.] -->|causal| DV
CV -->|causal| IV
```
<!-- col -->
```mermaid
graph TD;
IV[Coffee] -.->|spurious correlation| DV[Cancer]
CV[Smoking] -->|causal| DV
CV -->|causal| IV
```
<!-- colend -->
</script></section><section data-markdown><script type="text/template">## Hidden Confounds

![CT Scan Image](ctscan.jpg)

<aside class="notes"><p>ML algorithms may pick up on things that do not relate to the task but correlate with the outcome or hidden human inputs. For example, in cancer prediction, ML models have picked up on the kind of scanner used, learning that mobile scanners were used for particularly sick patients who could not be moved to the large installed scanners in a different part of the hospital.</p>
</aside></script></section><section data-markdown><script type="text/template">## Reverse Causality

![Chess](chess.jpg)

<aside class="notes"><p>(from Prediction Machines, Chapter 6) Early 1980s chess  program learned from Grandmaster games, learned that sacrificing queen would be a winning move, because it was occuring frequently in winning games. Program then started to sacrifice queen early.</p>
</aside></script></section><section data-markdown><script type="text/template">## Reverse Causality

![Hotel reception](hotel.jpg)

<aside class="notes"><p>(from Prediction Machines, Chapter 6) Low hotel prices in low sales season. Model might predict that high prices lead to higher demand.</p>
</aside></script></section><section data-markdown><script type="text/template">## Missing Counterfactuals

![Stock trading](stocktrading.jpg)

<aside class="notes"><p>Training data often does not indicate what would have happened with different situations, thus identifying causation is hard</p>
</aside></script></section><section data-markdown><script type="text/template">## Other Issues

* Insufficient training data
* Noisy training data
* Biased training data
* Overfitting
* Poor model fit, poor model selection, poor hyperparameters
* Missing context, missing important features
* Noisy inputs
* "Out of distribution" inputs


</script></section><section data-markdown><script type="text/template">## Another Perspective: What do we know?

<!-- smallish -->

* Known knowns: 
    - Rich data available, models can make confident predictions near training data
* Known unknowns (known risks):
    - We know that model's predictions will be poor; we have too little relevant training data, problem too hard
    - Model may recognize that its predictions are poor (e.g., out of distribution)
    - Humans are often better, because they can model the problem and make analogies
* Unknown unknowns:
    - "Black swan events", unanticipated changes could not have been predicted
    - Neither machines nor humans can predict these
* Unknown knowns:
    - Model is confident about wrong answers, based on picking up on wrong relationships (reverse causality, omitted variables) or attacks on the model

**Examples?**

<!-- references -->
üïÆ Ajay Agrawal, Joshua Gans, Avi Goldfarb. ‚Äú[Prediction Machines: The Simple Economics of Artificial Intelligence](https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019698987304436)‚Äù 2018, Chapter 6



<aside class="notes"><p>Examples:</p>
<ul>
<li>Known knowns: many current AI applications, like recommendations, navigation, translation</li>
<li>Known unknowns: predicting elections, predicting value of merger </li>
<li>Unknown unknown: new technology (mp3 file sharing), external disruptions (pandemic)</li>
<li>Unknown knowns: chess example (sacrificing queen detected as promising move), book making you better at a task?</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">![Known unknowns classification](unknownknowns.svg)

</script></section></section><section ><section data-markdown><script type="text/template"># Accepting that Mistakes will Happen
</script></section><section data-markdown><script type="text/template">## ML Models make Crazy Mistakes

* Humans often make predicable mistakes
    * most mistakes near to correct answer, distribution of mistakes
* ML models may be wildly wrong when they are wrong
    - especially black box models may use (spurious) correlations humans would never think about
    - may be very confident about wrong answer
    - "fixing" one mistake may cause others

![Adversarial example](adversarial.png)
</script></section><section data-markdown><script type="text/template">## Accepting Mistakes

* Never assume all predictions will be correct or close
* Always expect random, unpredictable mistakes to some degree, including results that are wildly wrong
* Best efforts at more data, debugging, "testing" likely will not eliminate the problem

Hence: **Anticipate existence of mistakes, focus on worst case analysis and mitigation outside the model -- system perspective needed**

Alternative paths: symbolic reasoning, interpretable models, and restricting predictions to "near" training data
</script></section><section data-markdown><script type="text/template">## Recall: Experience/UI Design

Balance forcefulness (automate, prompt, organize, annotate), frequency of interactions

![Powerpoint Slide Designer](powerpoint.png)
<!-- .element: class="stretch" -->
</script></section><section data-markdown><script type="text/template">## Recall: System-level Safeguards

<!-- colstart -->
![Toaster](toaster.jpg)
<!-- col -->
![A thermal fuse protecting the windings of a small motor](thermalfuse.png)
<!-- colend -->

<!-- references -->
(Image CC BY-SA 4.0, C J Cowie)





</script></section></section><section ><section data-markdown><script type="text/template"># Common Strategies to Handle Mistakes
</script></section><section data-markdown><script type="text/template">## Guardrails

*Software or hardware overrides outside the AI component*

![Toaster](toaster.jpg)

</script></section><section data-markdown><script type="text/template">## Redundancy and Voting

*Train multiple models, combine with heuristics, vote on results*

- Ensemble learning, reduces overfitting
- May learn the same mistakes, especially if data is biased
- Hardcode known rules (heuristics) for some inputs -- for important inputs

**Examples?**
</script></section><section data-markdown><script type="text/template">## Human in the Loop

*Less forceful interaction, making suggestions, asking for confirmation*

* AI and humans are good at predictions in different settings
    - e.g., AI better at statistics at scale and many factors; humans understand context and data generation process and often better with thin data (see *known unknowns*)
* AI for prediction, human for judgment?
* But
    * Notification fatigue, complacency, just following predictions; see *Tesla autopilot*
    * Compliance/liability protection only?
* Deciding when and how to interact
* Lots of UI design and HCI problems


**Examples?**

<aside class="notes"><p>Cancer prediction, sentencing + recidivism, Tesla autopilot, military &quot;kill&quot; decisions, powerpoint design suggestions</p>
</aside></script></section><section data-markdown><script type="text/template">## Undoable Actions

*Design system to reduce consequence of wrong predictions, allowing humans to override/undo*

**Examples?**

<aside class="notes"><p>Smart home devices, credit card applications, Powerpoint design suggestions</p>
</aside></script></section><section data-markdown><script type="text/template">## Review Interpretable Models

*Use interpretable machine learning and have humans review the rules*

```txt
IF age between 18‚Äì20 and sex is male THEN predict arrest
ELSE IF age between 21‚Äì23 and 2‚Äì3 prior offenses THEN predict arrest
ELSE IF more than three priors THEN predict arrest
ELSE predict no arrest
```

-> Approve the model as specification



















</script></section></section><section ><section data-markdown><script type="text/template"># Risk Analysis

(huge field, many established techniques; here overview only)
</script></section><section data-markdown><script type="text/template">## What's the worst that could happen?

![Robot uprising](robot-uprising.jpg)

*Likely?*

Toby Ord predicts existential risk from GAI at 10% within 100 years

<!-- references -->

Toby Ord, "The Precipice: Existential Risk and the Future of Humanity", 2020

<aside class="notes"><p>Discussion on existential risk. Toby Ord, Oxford philosopher predicts</p>
</aside></script></section><section data-markdown><script type="text/template">[![Paperclips game](paperclips.png)](https://www.decisionproblem.com/paperclips/index2.html)
</script></section><section data-markdown><script type="text/template">## What's the worst that could happen?

![Lane Assist in Tesla](lane.jpg)
<!-- .element: class="stretch" -->

</script></section><section data-markdown><script type="text/template">
![Cancer detection](mri.jpg)
<!-- .element: class="stretch" -->
</script></section><section data-markdown><script type="text/template">![Uber crash](ubercrash.png)
</script></section><section data-markdown><script type="text/template">
![Tay Chat Bot deying Holocaust](tay.png)
</script></section><section data-markdown><script type="text/template">![Amazon Hiring Tool Scraped due to Bias](amazonhiring.png)


</script></section><section data-markdown><script type="text/template">## What is Risk Analysis?

*  What can possibly go wrong in my system, and what are potential 
impacts on system requirements?
* Risk = Likelihood * Impact
* Many established methods:
  * Failure mode & effects analysis (FMEA)
  * Hazard analysis
  * Why-because analysis
  * Fault tree analysis (FTA)
  * Hazard and Operability Study (HAZOP)
  * ...
</script></section><section data-markdown><script type="text/template">
## Risks?

* Lane assist system
* Credit rating
* Amazon product recommendation
* Audio transcription service
* Cancer detection
* Predictive policing

**Discuss potential risks, including impact and likelyhood**

<!-- discussion -->



  </script></section><section data-markdown><script type="text/template">## Fault Tree Analysis (FTA)

* Fault tree: A top-down diagram that displays the relationships
between a system failure (i.e., requirement violation) and its potential causes.  
  * Identify sequences of events that result in a failure
  * Prioritize the contributors leading to the failure
  * Inform decisions about how to (re-)design the system
  * Investigate an accident & identify the root cause 
* Often used for safety & reliability, but can also be used for
other types of requirement (e.g., poor performance, security attacks...)

</script></section><section data-markdown><script type="text/template">## Fault Tree Example

![fta-example](fta-example.png)

* Every tree begins with a TOP event (typically a violation of a requirement)
* Every branch of the tree must terminate with a basic event

<!-- references -->
Figure from _Fault Tree Analysis and Reliability Block Diagram_
(2016), Jaroslav Menƒç√≠k. 

</script></section><section data-markdown><script type="text/template">## Fault Trees: Basic Building Blocks

![fta-blocks](fta-blocks.png)

* Event: An occurrence of a fault or an undesirable action
  * (Intermediate) Event: Explained in terms of other events
  * Basic Event: No further development or breakdown; leafs of the tree
* Gate: Logical relationship between an event & its immedicate subevents
   * AND: All of the sub-events must take place
  * OR: Any one of the sub-events may result in the parent event

<!-- references -->
Figure from _Fault Tree Analysis and Reliability Block Diagram_
(2016), Jaroslav Menƒç√≠k. 
</script></section><section data-markdown><script type="text/template">## Analysis

* What can we do with fault trees?
  * Qualitative analysis: Determine potential root causes of a
    failiure through _minimal cut set analysis_
  * Quantitative analysis: Compute the probability of a failure,
    based on estimated probabilities of basic events

*(cut set = set of basic events whose simultaneous occurrence is
  sufficient to guarantee that the TOP event occurs)*

<!-- split -->
![fta-example](fta-example.png)

<aside class="notes"><ul>
<li><em>Minimal</em> cut set: A cut set from which a smaller cut set can be
obtained by removing a basic event.</li>
<li>Switch failed alone is sufficient (minimal cut set), so is fused burned, whereas lamp1 + lamp2 burned is a cut set, but not minimal.</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">## Fault Tree Analysis & AI

* Anticipate mistakes and understand consequences
* How do mistakes made by AI contribute to system failures/catastrophe? 
* Increasingly used in automotive, aeronautics, industrial control systems, etc.




</script></section><section data-markdown><script type="text/template">## FTA Process

1. Specify the system structure
   * Environment entities & machine components
   * Assumptions (ENV) & specifications (SPEC)
2. Identify the top event as a violation of REQ
3. Construct the fault tree
  * Intermediate events can be derived from violation of SPEC/ENV
4. Analyze the tree
  * Identify all possible minimal cut sets
5. Consider design modifications to eliminate certain cut sets
6. Repeat
</script></section><section data-markdown><script type="text/template">## Exercise: Draw Fault Tree for Smart Toaster

```mermaid
graph TD
    left[TOP: Smart toaster burning]
```

</script></section><section data-markdown><script type="text/template">## Fault-Tree Analysis Discussion

* Town-down, *backward* search for the root cause of issues
    - from final outcomes to initiating events
* Issues (TOP events) need to be known upfront
* Quantitative analysis possible
* Useful for understanding faults post-hoc
* Where do outcomes come from?
</script></section><section data-markdown><script type="text/template">## Failure Mode and Effects Analysis (FMEA)

![](fmea-radiation.png)

* A __forward search__ technique to identify potential hazards
* Widely used in aeronautics, automotive, healthcare, food services,
  semiconductor processing, and (to some extent) software
</script></section><section data-markdown><script type="text/template">## FMEA Process

* Identify system components
* Enumerate potential failure modes
    - *for ML component: Always suspect prediction may be wrong*
* For each failure mode, identify:
  * Potential hazardous effect on the system
  * Method for detecting the failure
  * Potential mitigation strategy


</script></section><section data-markdown><script type="text/template">## FMEA Example: Lane Assist

```mermaid
graph LR
Camera --> LP[LanePrediction]
SteeringStatus --> SteeringPlanning
LP --> SteeringPlanning
SteeringPlanning --> Guardian
Guardian --> SteeringActuators
Guardian --> Beeper
GyroSensor --> SteeringPlanning
GyroSensor --> Guardian
```

Failure modes? Failure effects? Detection? Mitigation?

<aside class="notes"><p>More general Autonomous Vehicle example</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Failure Mode</th>
<th>Failure Effects</th>
<th>Detection</th>
<th>Mitigation</th>
</tr>
</thead>
<tbody><tr>
<td>Perception</td>
<td>Failure to detect an object</td>
<td>Risk of collision</td>
<td>Human operator (if present)</td>
<td>Deploy secondary classifier</td>
</tr>
<tr>
<td>Perception</td>
<td>Detected but misclassified</td>
<td>&quot;</td>
<td>&quot;</td>
<td>&quot;</td>
</tr>
<tr>
<td>Lidar Sensor</td>
<td>Mechanical failure</td>
<td>Inability to detect objects</td>
<td>Monitor</td>
<td>Switch to manual control mode</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody></table>
</aside></script></section><section data-markdown><script type="text/template">## "Wrong Prediction" as Failure Mode?

* "Wrong prediction" is a very cause grained failure mode
* May not be possible to decompose further
* However, may evaluate causes of wrong prediction for better understanding, as far as possible --> FTA?

</script></section><section data-markdown><script type="text/template">## Exercise: FMEA Analysis for Smart Toaster

(video sensor, temperature sensor, heat sensor, user setting, ML model, heuristic shutdown, thermal fuse)

Failure modes? Failure effects? Detection? Mitigation?

<!-- discussion -->

</script></section><section data-markdown><script type="text/template">## FMEA Summary

* Forward analysis: From components to possible failures
* Focus on single component failures, no interactions
* Identifying failure modes may require domain understanding
</script></section><section data-markdown><script type="text/template">## Hazard and Interoperability Study (HAZOP)
   
*identify hazards and component fault scenarios through guided inspection of requirements*

![HAZOP example](hazop-perception.jpg)















</script></section></section><section ><section data-markdown><script type="text/template"># Decomposing Requirements to Understand Problems
</script></section><section data-markdown><script type="text/template">## The Role of Requirements Engineering

* Requirements engineering essential to understand risks and mistake mitigation
* Understand 
    * user interactions
    * safety requirements
    * security and privacy requirements
    * fairness requirements
    * possible feedback loops
</script></section><section data-markdown><script type="text/template">## Machine vs World

![machine-world](machine-world.png)

* No software lives in vacuum; every system is deployed as part of the world
* A requirement describes a desired state of the world (i.e., environment)
* Machine (software) is _created_ to manipulate the environment into
  this state 
</script></section><section data-markdown><script type="text/template">## Shared Phenomena

![phenomena](phenomena.jpg)

* Shared phenomena: Interface between the world & machine (actions,
  events, dataflow, etc.,)
* Requirements (REQ) are expressed only in terms of world phenomena 
* Assumptions (ENV) are expressed in terms of world & shared phenomena
* Specifications (SPEC) are expressed in terms of machine & shared phenomena

</script></section><section data-markdown><script type="text/template">## Discussion: Machine vs World

![machine-world](machine-world.png)

**Discuss examples for self-driving car, Amazon product recommendation, smart toaster**
</script></section><section data-markdown><script type="text/template">## Example: Lane Assist

![Lane Assist in Tesla](lane.jpg)
<!-- .element: class="stretch" -->

* Requirement: Car should beep when exiting lane / adjust steering to stay in lane
* Environment assumptions: ??
* Specifications: ??

<aside class="notes"><p>ENV: Engine is working as intended; sensors are providing accurate information about the leading car (current speed, distance...)
SPEC: Depending on the sensor readings, the controller must issue an actuator command to beep/steer the vehicle as needed.</p>
</aside></script></section><section data-markdown><script type="text/template">## Recall: Lack of Specifications for AI components

* In addition to world vs machine challenges
* We do not have clear specifications for AI components
    - goals, average accuracy
    - at best probabilistic specifications in some symbolic AI techniques
* Viewpoint: Machine learning techniques mine specifications from data, but not usually understandable 

</script></section><section data-markdown><script type="text/template">## What could go wrong?

![machine-world](machine-world.png)

* Missing/incorrect environmental assumptions (ENV)
* Wrong specification (SPEC)
* Inconsistency in assumptions & spec (ENV ‚àß SPEC = False)
* Inconsistency in requirements (REQ = False)
</script></section><section data-markdown><script type="text/template">## Non-AI Example: Lufthansa 2904 Runway Crash

![Plane Crash](lufthansacrash.jpg)
<!-- .element: class="stretch" -->


* Reverse thrust (RT): Decelerates plane during landing
* What was required (REQ): RT enabled if and only if plane on the ground
* What was implemented (SPEC): RT enabled if and only if wheel turning
* But: Runway wet + wind, wheels did not turn, pilot overridden by software

<aside class="notes"><p>For more details see <a href="https://en.wikipedia.org/wiki/Lufthansa_Flight_2904">https://en.wikipedia.org/wiki/Lufthansa_Flight_2904</a>; Image credit <a href="https://en.wikipedia.org/wiki/Lufthansa_Flight_2904#/media/File:Lufthansa_Flight_2904_crash_site_Siecinski.jpg">Mariusz Siecinski</a></p>
</aside></script></section><section data-markdown><script type="text/template">## Feedback Loops and Adversaries

![machine-world](machine-world.png)
<!-- .element: class="stretch" -->

* Feedback loops: Behavior of the machine affects the world, which affects inputs to the machine
* Data drift: Behavior of the world changes over time, assumptions no longer valid
* Adversaries: Bad actors deliberately may manipulate inputs, violate environment assumptions

**Examples?**

</script></section><section data-markdown><script type="text/template">## Implications on Software Development

* Software/AI alone cannot establish system requirements -- they are just one part of the system
* Environmental assumptions are just as critical
  * But typically you can't modify these
  * Must design SPEC while treating ENV as given
* If you ignore/misunderstand these, your system may fail to satisfy
  its requirements
</script></section><section data-markdown><script type="text/template">## Deriving SPEC from REQ

1. Identify environmental entities and machine components
2. State a desired requirement (REQ) over the environment
3. Identify the interface between the environment & machines
4. Identify the environmental assumptions (ENV)
5. Develop software specifications (SPEC) that are sufficient to establish REQ
6. Check whether ENV ‚àß SPEC ‚äß REQ
7. If NO, strengthen SPEC & repeat Step 6





















</script></section></section><section  data-markdown><script type="text/template"># Summary

* Accept that ML components will confidently make mistakes
* Many reasons for wrong predictions (poor data, reverse causation, ...)
* Plan for mistakes
    - System-level safeguards
    - Human computer interaction, interface design
* Understand world-machine interactions
* Use Risk/Hazard analysis to identify and mitigate potential problems </script></section></div>
    </div>

    <script src="./../js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './../plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './../plugin/zoom-js/zoom.js', async: true },
        { src: './../plugin/notes/notes.js', async: true },
        { src: './../plugin/math/math.js' },
        { src: './../rplugin/embed-tweet/embed-tweet.js' },
        { src: './../rplugin/menu/menu.js', async: true },
        { src: './../rplugin/spreadsheet/spreadsheet.js' },
        { src: './../rplugin/chalkboard/chalkboard.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps,
	chalkboard: { // font-awesome.min.css must be available
		toggleChalkboardButton: { left: "80px" },
		toggleNotesButton: { left: "130px" },
	},
	keyboard: {
	    67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle notes canvas when 'c' is pressed
	    66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
	    46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
	     8: function() { RevealChalkboard.reset() },	// reset chalkboard data on current slide when 'BACKSPACE' is pressed
	    68: function() { RevealChalkboard.download() },	// downlad recorded chalkboard drawing when 'd' is pressed
	    88: function() { RevealChalkboard.colorNext() },	// cycle colors forward when 'x' is pressed
	    89: function() { RevealChalkboard.colorPrev() },	// cycle colors backward when 'y' is pressed
	}
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"theme":"white","slideNumber":true,"hash":true,"center":false}, queryOptions);
    </script>

    <script src="./../_assets/_assets/mermaid.min.js"></script>
    <script src="./../_assets/_assets/loadmymarkdown.js"></script>

    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
