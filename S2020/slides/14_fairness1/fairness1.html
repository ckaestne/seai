<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>17-445: Ethics & Fairness in AI-Enabled Systems</title>
    <link rel="stylesheet" href="./../css/reveal.css" />
    <link rel="stylesheet" href="./../css/theme/white.css" id="theme" />
    <link rel="stylesheet" href="./../css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./../css/print/paper.css" type="text/css" media="print" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
     <script src="./../rplugin/spreadsheet/ruleJS.all.full.min.js"></script>
   <link rel="stylesheet" href="./../rplugin/spreadsheet/spreadsheet.css">
    <link rel="stylesheet" href="./../_assets/_assets/cmu.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><div id="footer">17-445 Software Engineering for AI-Enabled Systems, Christian Kaestner</div><section  data-markdown><script type="text/template">  

# Ethics & Fairness in AI-Enabled Systems

Christian Kaestner

(with slides from Eunsuk Kang)

<!-- references -->

Required reading: ðŸ—Ž R. Caplan, J. Donovan, L. Hanson, J.
Matthews. "[Algorithmic Accountability: A Primer](https://datasociety.net/wp-content/uploads/2019/09/DandS_Algorithmic_Accountability.pdf)", Data & Society
(2018).
</script></section><section  data-markdown><script type="text/template"># Learning Goals

* Review the importance of ethical considerations in designing AI-enabled systems
* Recall basic strategies to reason about ethical challenges
* Diagnose potential ethical issues in a given system
* Understand the types of harm that can be caused by ML
* Understand the sources of bias in ML
* Analyze a system for harmful feedback loops

</script></section><section  data-markdown><script type="text/template"># Overview

Many interrelated issues:
* Ethics
* Fairness
* Justice
* Discrimination
* Safety
* Privacy
* Security
* Transparency
* Accountability

*Each is a deep and nuanced research topic. We focus on survey of some key issues.*
</script></section><section ><section data-markdown><script type="text/template"># Ethical vs Legal

</script></section><section data-markdown><script type="text/template">
![Martin Shkreli](Martin_Shkreli_2016.jpg)

<!-- split -->

*In September 2015, Shkreli received widespread criticism when Turing obtained the manufacturing license for the antiparasitic drug Daraprim and raised its price by a factor of 56 (from USD 13.5 to 750 per pill), leading him to be referred to by the media as "the most hated man in America" and "Pharma Bro".* -- [Wikipedia](https://en.wikipedia.org/wiki/Martin_Shkreli)

"*I could have raised it higher and made more profits for our shareholders. Which is my primary duty.*" -- Martin Shkreli


<aside class="notes"><p>Image source: <a href="https://en.wikipedia.org/wiki/Martin_Shkreli#/media/File:Martin_Shkreli_2016.jpg">https://en.wikipedia.org/wiki/Martin_Shkreli#/media/File:Martin_Shkreli_2016.jpg</a></p>
</aside></script></section><section data-markdown><script type="text/template">## Terminology

* Legal = in accordance to societal laws
  - systematic body of rules governing society; set through government
  - punishment for violation
* Ethical = following moral principles of tradition, group, or individual
  - branch of philosophy, science of a standard human conduct
  - professional ethics = rules codified by professional organization
  - no legal binding, no enforcement beyond "shame"
  - high ethical standards may yield long term benefits through image and staff loyalty
</script></section><section data-markdown><script type="text/template">## With a few lines of code...


[![Headline: Some airlines may be using algorithms to split up families during flights](airlines_split.png)](https://www.vox.com/the-goods/2018/11/27/18115164/airline-flying-seat-assignment-ryanair)
</script></section><section data-markdown><script type="text/template">## The Implications of our Choices

![Les Paul Doodle](doodle.png)

> â€œUpdate Jun 17: Wowâ€”in just 48 hours in the U.S., you recorded 5.1 years worth of 
> musicâ€”40 million songsâ€”using our doodle guitar. 
> And those songs were played back 870,000 times!â€œ
</script></section><section data-markdown><script type="text/template"><iframe width="800" height="600" src="https://www.youtube.com/embed/pi_SCJ7COw0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</script></section></section><section ><section data-markdown><script type="text/template"># Concerns about an AI Future
</script></section><section data-markdown><script type="text/template">## Safety

![Robot uprising](robot-uprising.jpg)

</script></section><section data-markdown><script type="text/template">## Safety

<div class="tweet" data-src="https://twitter.com/EmilyEAckerman/status/1065700195776847872"></div>

</script></section><section data-markdown><script type="text/template">## Safety

<div class="tweet" data-src="https://twitter.com/EmilyEAckerman/status/1186363305851576321"></div>




</script></section><section data-markdown><script type="text/template">## Addiction

![Infinite Scroll](infinitescroll.png)
<!-- .element: class="stretch" -->

<aside class="notes"><p>Infinite scroll in applications removes the natural breaking point at pagination where one might reflect and stop use.</p>
</aside></script></section><section data-markdown><script type="text/template">## Addiction

[![Blog: Robinhood Has Gamified Online Trading Into an Addiction](robinhood.png)](https://marker.medium.com/robinhood-has-gamified-online-trading-into-an-addiction-cc1d7d989b0c)

</script></section><section data-markdown><script type="text/template">
[![Article: The Morality of A/B Testing](abtesting.png)](https://techcrunch.com/2014/06/29/ethics-in-a-data-driven-world/)

</script></section><section data-markdown><script type="text/template">## Mental Health

[![Social Media vs Mental Health](mentalhealth.png)](https://www.healthline.com/health-news/social-media-use-increases-depression-and-loneliness)
</script></section><section data-markdown><script type="text/template">[![Center for Humane Technology](humanetech.png)](https://humanetech.com/)
<!-- .element: class="stretch" -->

</script></section><section data-markdown><script type="text/template">## Society: Unemployment Engineering / Deskilling

![Automated food ordering system](automation.jpg)

<aside class="notes"><p>The dangers and risks of automating jobs.</p>
<p>Discuss issues around automated truck driving and the role of jobs.</p>
<p>See for example: Andrew Yang. The War on Normal People. 2019</p>
</aside></script></section><section data-markdown><script type="text/template">## Society: Polarization

[![Article: Facebook Executives Shut Down Efforts to Make the Site Less Divisive](facebookdivisive.png)](https://www.wsj.com/articles/facebook-knows-it-encourages-division-top-executives-nixed-solutions-11590507499)
<!-- .element: class="stretch" -->


<aside class="notes"><p>Recommendations for further readings: <a href="https://www.nytimes.com/column/kara-swisher">https://www.nytimes.com/column/kara-swisher</a>, <a href="https://podcasts.apple.com/us/podcast/recode-decode/id1011668648">https://podcasts.apple.com/us/podcast/recode-decode/id1011668648</a></p>
<p>Also isolation, Cambridge Analytica, collaboration with ICE, ...</p>
</aside></script></section><section data-markdown><script type="text/template">## Weapons, Surveillance, Suppression

<!-- colstart -->
![Boston Dynamics BigDog](bigdog.png)
<!-- col -->
[![Article: How U.S. surveillance technology is propping up authoritarian regimes](surveillance.png)](https://www.washingtonpost.com/outlook/2019/01/17/how-us-surveillance-technology-is-propping-up-authoritarian-regimes/)
<!-- colend -->
</script></section><section data-markdown><script type="text/template">## Discrimination

<div class="tweet" data-src="https://twitter.com/dhh/status/1192540900393705474"></div>

</script></section><section data-markdown><script type="text/template">## Discrimination

![Google Photo Mislabels](gphotos.png)
<!-- .element: class="stretch" -->

</script></section><section data-markdown><script type="text/template">## Discrimination

* Unequal treatment in hiring, college admissions, credit rating, insurance, policing, sentencing, advertisement, ...
* Unequal outcomes in healthcare, accident prevention, ...
* Reinforcing patterns in predictive policing with feedback loops
*
* Technological redlining
</script></section><section data-markdown><script type="text/template">## Any own experiences?

<!-- discussion -->

</script></section><section data-markdown><script type="text/template">## Summary -- so far

* Safety issues
* Addiction and mental health
* Societal consequences: unemployment, polarization, monopolies
* Weapons, surveillance, suppression
* Discrimination, social equity
*
* Many issues are ethically problematic, but some are legal. Consequences?
* Intentional? Negligence? Unforeseeable?









</script></section></section><section ><section data-markdown><script type="text/template"># Fairness
</script></section><section data-markdown><script type="text/template">## Legally protected classes (US)

* Race (Civil Rights Act of 1964)
* Color (Civil Rights Act of 1964)
* Sex (Equal Pay Act of 1963; Civil Rights Act of 1964)
* Religion (Civil Rights Act of 1964)
* National origin (Civil Rights Act of 1964)
* Citizenship (Immigration Reform and Control Act)
* Age (Age Discrimination in Employment Act of 1967)
* Pregnancy (Pregnancy Discrimination Act)
* Familial status (Civil Rights Act of 1968)
* Disability status (Rehabilitation Act of 1973; Americans with Disabilities Act of 1990)
* Veteran status (Vietnam Era Veterans' Readjustment Assistance Act of 1974; Uniformed Services Employment and Reemployment Rights Act)
* Genetic information (Genetic Information Nondiscrimination Act)

<!-- references -->
Barocas, Solon and Moritz Hardt. "[Fairness in machine learning](https://mrtz.org/nips17/#/)." NIPS Tutorial 1 (2017).
</script></section><section data-markdown><script type="text/template">## Regulated domains (US)

* Credit (Equal Credit Opportunity Act)
* Education (Civil Rights Act of 1964; Education Amendments of 1972)
* Employment (Civil Rights Act of 1964)
* Housing (Fair Housing Act)
* â€˜Public Accommodationâ€™ (Civil Rights Act of 1964)

Extends to marketing and advertising; not limited to final decision

<!-- references -->
Barocas, Solon and Moritz Hardt. "[Fairness in machine learning](https://mrtz.org/nips17/#/)." NIPS Tutorial 1 (2017).
</script></section><section data-markdown><script type="text/template">![Contrasting equality, equity, and justice](eej.jpg)
</script></section><section data-markdown><script type="text/template">## Harms of Allocation

* Withhold opportunities or resources
* Poor quality of service, degraded user experience for certain groups

![](gender-detection.png)

**Other examples?**

<!-- references -->

_[Gender Shades: Intersectional Accuracy Disparities in
Commercial Gender Classification](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)_, Buolamwini & Gebru, ACM FAT* (2018).
</script></section><section data-markdown><script type="text/template">## Harms of Representation

* Reinforce stereotypes, subordination along the lines of identity

![](online-ad.png)

**Other examples?**

<!-- references -->

Latanya Sweeney. [Discrimination in Online Ad Delivery](https://dl.acm.org/doi/pdf/10.1145/2460276.2460278), SSRN (2013).
</script></section><section data-markdown><script type="text/template">## Identifying harms

![](harms-table.png)

* Multiple types of harms can be caused by a product!
* Think about your system objectives & identify potential harms.

<!-- references -->

Swati Gupta, Henriette Cramer, Kenneth Holstein, Jennifer Wortman Vaughan, Hal DaumÃ© III, Miroslav DudÃ­k, Hanna Wallach, Sravana Reddy, Jean GarciaGathright. [Challenges of incorporating algorithmic fairness into practice](https://www.youtube.com/watch?v=UicKZv93SOY), FAT* Tutorial, 2019. ([slides](https://bit.ly/2UaOmTG))
</script></section><section data-markdown><script type="text/template">## The Role of Requirements Engineering

* Identify system goals
* Identify legal constraints
* Identify stakeholders and fairness concerns
* Analyze risks with regard to discrimination and fairness
* Analyze possible feedback loops (world vs machine)
* Negotiate tradeoffs with stakeholders
* Set requirements/constraints for data and model
* Plan mitigations in the system (beyond the model)
* Design incident response plan
* Set expectations for offline and online assurance and monitoring
</script></section><section data-markdown><script type="text/template">## Why care about fairness? 

* Obey the law
* Better product, serving wider audiences
* Competition
* Responsibility
* PR

*Examples?*

*Which argument appeals to which stakeholders?*

<!-- references -->

Swati Gupta, Henriette Cramer, Kenneth Holstein, Jennifer Wortman Vaughan, Hal DaumÃ© III, Miroslav DudÃ­k, Hanna Wallach, Sravana Reddy, Jean GarciaGathright. [Challenges of incorporating algorithmic fairness into practice](https://www.youtube.com/watch?v=UicKZv93SOY), FAT* Tutorial, 2019. ([slides](https://bit.ly/2UaOmTG))
</script></section><section data-markdown><script type="text/template">## Case Study: College Admission

![](college-admission.jpg)

* Objective: Decide "Is this student likely to succeed"?
* Possible harms: Allocation of resources? Quality of service?
  Stereotyping? Denigration? Over-/Under-representation?
</script></section><section data-markdown><script type="text/template">## Not all discrimination is harmful

![](gender-bias.png)

* Loan lending: Gender discrimination is illegal.
* Medical diagnosis: Gender-specific diagnosis may be desirable.
* Discrimination is a __domain-specific__ concept!

**Other examples?**

</script></section><section data-markdown><script type="text/template">## On Terminology

* Bias and discrimination are technical terms in machine learning
  - [selection bias](https://en.wikipedia.org/wiki/Selection_bias), [reporting bias](https://en.wikipedia.org/wiki/Reporting_bias), [bias of an estimator](https://en.wikipedia.org/wiki/Bias_of_an_estimator), [inductive/learning bias](https://en.wikipedia.org/wiki/Inductive_bias)
  - discrimination   refers to distinguishing outcomes (classification)
* The problem is *unjustified* differentiation, ethical issues
  - practical irrelevance
  - moral irrelevance







</script></section></section><section ><section data-markdown><script type="text/template"># Sources of Bias
</script></section><section data-markdown><script type="text/template">##  Where does the bias come from?

![](google-translate-bias.png)

<!-- references -->

Caliskan et al., _[Semantics derived automatically from language corpora contain
human-like biases](http://cs.bath.ac.uk/~jjb/ftp/CaliskanEtAl-authors-full.pdf)_, Science (2017).


</script></section><section data-markdown><script type="text/template">## Sources of Bias

* Tainted examples / historical bias
* Skewed sample
* Limited features
* Sample size disparity
* Proxies

<!-- references -->

Barocas, Solon, and Andrew D. Selbst. "[Big data's disparate impact](http://www.cs.yale.edu/homes/jf/BarocasDisparateImpact.pdf)." Calif. L. Rev. 104 (2016): 671.

Mehrabi, Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. "](https://arxiv.org/pdf/1908.09635.pdf." arXiv preprint arXiv:1908.09635 (2019).
</script></section><section data-markdown><script type="text/template">## Historical Bias

*Data reflects past biases, not intended outcomes*

![Image search for "CEO"](ceo.png)

<aside class="notes"><p>&quot;An example of this type of bias can be found in a 2018 image search
result where searching for women CEOs ultimately resulted in fewer female CEO images due
to the fact that only 5% of Fortune 500 CEOs were womanâ€”which would cause the search
results to be biased towards male CEOs. These search results were of course reflecting
the reality, but whether or not the search algorithms should reflect this reality is an issue worth
considering.&quot;</p>
</aside></script></section><section data-markdown><script type="text/template">## Tainted Examples

*Samples or labels reflect human bias*

![](amazon-hiring.png)

<aside class="notes"><ul>
<li>Bias in the dataset caused by humans</li>
<li>Some labels created manually by employers</li>
<li>Dataset &quot;tainted&quot; by biased human judgement</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">## Skewed Sample

*Crime prediction for policing strategy*

![](crime-map.jpg)

<aside class="notes"><p>Initial bias in the data set, amplified 
through feedback loop</p>
<p>Other example: Street Bump app in Boston (2012) to detect potholes while driving favors areas with higher smartphone adoption</p>
</aside></script></section><section data-markdown><script type="text/template">## Limited Features

*Features used are less informative/reliable for certain subpopulations*

![](performance-review.jpg)

Example: "Leave of absence" as feature in employee performance review

<aside class="notes"><ul>
<li>Features are less informative or reliable for certain parts of the population</li>
<li>Features that support accurate prediction for the majority may not do so
for a minority group</li>
<li>Example: Employee performance review<ul>
<li>&quot;Leave of absence&quot; as a feature (an indicator of poor performance)</li>
<li>Unfair bias against employees on parental leave</li>
</ul>
</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">## Sample Size Disparity

*Less training data available for certain subpopulations*

![](shirley-card.jpg)

Example: "Shirley Card" used for color calibration

<aside class="notes"><ul>
<li>Less data available for certain parts of the population</li>
<li>Example: &quot;Shirley Card&quot;<ul>
<li>Used by Kodak for color calibration in photo films</li>
<li>Most &quot;Shirley Cards&quot; used Caucasian models</li>
<li>Poor color quality for other skin tones</li>
</ul>
</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">
<div class="tweet" data-src="https://twitter.com/nke_ise/status/897756900753891328"></div>

</script></section><section data-markdown><script type="text/template">## Proxies

*Features correlate with protected attributes*

![](neighborhoods.png)

<aside class="notes"><ul>
<li>Certain features are correlated with class membership</li>
<li>Example: Neighborhood as a proxy for race</li>
<li>Even when sensitive attributes (e.g., race) are erased, bias may still occur</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">## Case Study: College Admission

![](college-admission.jpg)
<!-- .element: class="stretch" -->

* Classification: Is this student likely to succeed?
* Features: GPA, SAT, race, gender, household income, city, etc.,
* **Discuss:** Historical bias? Skewed sample? Tainted examples? Limited features? Sample size disparity? Proxies?





</script></section></section><section ><section data-markdown><script type="text/template"># Massive Potential Damage

![Book Cover: Weapons of math destruction](weaponsmath.jpg)
<!-- .element: class="stretch" -->

O'Neil, Cathy. [Weapons of math destruction: How big data increases inequality and threatens democracy](https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991016462699704436). Broadway Books, 2016.
</script></section><section data-markdown><script type="text/template">## Example: Predictive Policing

![](crime-map.jpg)

*with a few lines of code...*
</script></section><section data-markdown><script type="text/template">
> A person who scores as â€˜high riskâ€™ is likely to be unemployed and to come from a neighborhood where many of his friends and family have had run-ins with the law. Thanks in part to the resulting high score on the evaluation, he gets a longer sentence, locking him away for more years in a prison where heâ€™s surrounded by fellow criminalsâ€”which raises the likelihood that heâ€™ll return to prison. He is finally released into the same poor neighborhood, this time with a criminal record, which makes it that much harder to find a job. If he commits another crime, the recidivism model can claim another success. But in fact the model itself contributes to a toxic cycle and helps to sustain it. -- Cathy O'Neil in [Weapons of Math Destruction](https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991016462699704436)
</script></section><section data-markdown><script type="text/template">## Feedback Loops

```mermaid
graph LR
  t[biased training data] --> o[biased outcomes]
  o --> m[biased telemetry] 
  m --> t
```

> "Big Data processes codify the past.  They do not invent the future.  Doing that requires moral imagination, and thatâ€™s something only humans can provide. " -- Cathy O'Neil in [Weapons of Math Destruction](https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991016462699704436)
</script></section><section data-markdown><script type="text/template">## Key Problems

* We trust algorithms to be objective, may not question their predictions
* Often designed by and for privileged/majority group
* Algorithms often black box (technically opaque and kept secret from public)
* Predictions based on correlations, not causation; may depend on flawed statistics
* Potential for gaming/attacks
* Despite positive intent, feedback loops may undermine the original goals


<!-- references -->

O'Neil, Cathy. [Weapons of math destruction: How big data increases inequality and threatens democracy](https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991016462699704436). Broadway Books, 2016.

</script></section><section data-markdown><script type="text/template">## "Weapons of Math Destruction"

* Algorithm evaluates people
  - e.g., credit, hiring, admissions, recidivism, advertisement, insurance, healthcare
* Widely used for life-affecting decisions
* Opaque and not accountable, no path to complain
* Feedback loop 


<!-- references -->

O'Neil, Cathy. [Weapons of math destruction: How big data increases inequality and threatens democracy](https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991016462699704436). Broadway Books, 2016.












</script></section></section><section  data-markdown><script type="text/template"># Summary

* Many interrelated issues: ethics, fairness, justice, safety, security, ...
* Many many many potential issues
* Consider fairness when it's the law and because it's ethical
* Large potential for damage: Harm of allocation & harm of representation
* Sources of bias in ML: skewed sample, tainted examples, limited features, sample size, disparity, proxies
* Be aware of feedback loops
*
* Recommended readings: [Weapons of Math Destructions](https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991016462699704436) and [several](https://fairmlbook.org/tutorial1.html) [tutorials](https://sites.google.com/view/wsdm19-fairness-tutorial) on [ML fairness](https://sites.google.com/view/kdd19-fairness-tutorial)
* __Next__: Definitions of fairness, measurement, testing for fairness


</script></section></div>
    </div>

    <script src="./../js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './../plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './../plugin/zoom-js/zoom.js', async: true },
        { src: './../plugin/notes/notes.js', async: true },
        { src: './../plugin/math/math.js' },
        { src: './../rplugin/embed-tweet/embed-tweet.js' },
        { src: './../rplugin/menu/menu.js', async: true },
        { src: './../rplugin/spreadsheet/spreadsheet.js' },
        { src: './../rplugin/chalkboard/chalkboard.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps,
	chalkboard: { // font-awesome.min.css must be available
		toggleChalkboardButton: { left: "80px" },
		toggleNotesButton: { left: "130px" },
	},
	keyboard: {
	    67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle notes canvas when 'c' is pressed
	    66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
	    46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
	     8: function() { RevealChalkboard.reset() },	// reset chalkboard data on current slide when 'BACKSPACE' is pressed
	    68: function() { RevealChalkboard.download() },	// downlad recorded chalkboard drawing when 'd' is pressed
	    88: function() { RevealChalkboard.colorNext() },	// cycle colors forward when 'x' is pressed
	    89: function() { RevealChalkboard.colorPrev() },	// cycle colors backward when 'y' is pressed
	}
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"theme":"white","slideNumber":true,"hash":true,"center":false}, queryOptions);
    </script>

    <script src="./../_assets/_assets/mermaid.min.js"></script>
    <script src="./../_assets/_assets/loadmymarkdown.js"></script>

    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
