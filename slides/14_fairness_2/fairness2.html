<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>17-445: Fairness Continued</title>
    <link rel="stylesheet" href="./../css/reveal.css" />
    <link rel="stylesheet" href="./../css/theme/white.css" id="theme" />
    <link rel="stylesheet" href="./../css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./../css/print/paper.css" type="text/css" media="print" />
    <link rel="stylesheet" href="./../_assets/_assets/cmu.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><div id="footer">17-445 Software Engineering for AI-Enabled Systems, Christian Kaestner & Eunsuk Kang</div><section  data-markdown><script type="text/template">

# Fairness: Definitions and Measurements

Eunsuk Kang

<!-- references -->

Required reading:"Fairness and Machine Learning" by Barocas, Hardt,
and Narayanan (2019), Chapter 1.
</script></section><section  data-markdown><script type="text/template"># Learning Goals

* Understand different definitions of fairness
* Discuss methods for measuring fairness
</script></section><section ><section data-markdown><script type="text/template"># Fairness: Definitions
</script></section><section data-markdown><script type="text/template">### Fairness is still an actively studied & disputed concept!

![](fairness-papers.jpeg)
</script></section><section data-markdown><script type="text/template">## Fairness: Definitions

* Fairness through Blindness
* Group fairness
* Equalized odds
* Predictive rate parity
</script></section><section data-markdown><script type="text/template">## Fairness through Blindness

![](justice.jpeg)

* Ignore/eliminate sensitive attributes from dataset
* Example: Remove gender or race from a credit card scoring system
* __Q. Why is this potentially a bad idea__?
</script></section><section data-markdown><script type="text/template">## Fairness through Blindness

![](justice.jpeg)

* Ignore/eliminate sensitive attributes from dataset
* __Q. Why is this potentially a bad idea__?
  * Sensitive attributes may be correlated with other features
  * Some ML tasks need sensitive attributes (e.g., medical diagnosis)
</script></section><section data-markdown><script type="text/template">## Notations

* $X$: Feature set (e.g., age, race, education, region, income, etc.,)  
* $A$: Sensitive attribute (race)
* $R$: Regression score (predicted likelihood of recidivism)
  * $Y'$ = 1 if and only if $R$ is greater than some threshold $T$
* $Y$: Target variable (Did the person actually commit recidivism?)
</script></section><section data-markdown><script type="text/template">## Group Fairness

$P[R = 1 | A = a]  = P[R = 1 | A = b]$

* Also called _statistical parity_, _demographic parity_, or the
  _independence_ criterion
* Mathematically, $R \perp A$
  * Prediction must be independent of the sensitive attribute
* Example: The predicted rate of recidivism is the same across all races
  <!-- .element: class="fragment" -->
* Q. What are limitations of group fairness?
  <!-- .element: class="fragment" -->
  * Ignores possible correlation between $Y$ and $A$
    <!-- .element: class="fragment" -->
	* Rules out perfect predictor $R = Y$ when $Y$ & $A$ are correlated
  * Permits laziness: Intentionally give high ratings to
  random people in one group
    <!-- .element: class="fragment" -->
* Q. But does this mean group fairness should never be used?
  <!-- .element: class="fragment" -->
</script></section><section data-markdown><script type="text/template">## Equalized Odds

$P[R=0∣Y=1,A=a] = P[R=0∣Y=1,A=b]$
$P[R=1∣Y=0,A=a] = P[R=1∣Y=0,A=b]$

* Also called the _separation_ criterion
* $R \perp A | Y$
  * Prediction must be independent of the sensitive attribute
  _conditional_ on the target variable
</script></section><section data-markdown><script type="text/template">## Review: Confusion Matrix

![](confusion-matrix.jpg)

Can we explain equalize odds in terms of errors?

$P[R=0∣Y=1,A=a] = P[R=0∣Y=1,A=b]$
$P[R=1∣Y=0,A=a] = P[R=1∣Y=0,A=b]$
</script></section><section data-markdown><script type="text/template">## Equalized Odds

$P[R=0∣Y=1,A=a] = P[R=0∣Y=1,A=b]$
$P[R=1∣Y=0,A=a] = P[R=1∣Y=0,A=b]$

* Also called the _separation_ criterion
* $R \perp A | Y$
  * Prediction must be independent of the sensitive attribute
    _conditional_ on the target variable
* i.e., All groups experience the same false positive & negative rates
<!-- .element: class="fragment" -->
* Example: Credit card rating
<!-- .element: class="fragment" -->
  * R: Credit card score, A: Gender of applicant: Y: Actual credit behavior
  * A person with good (bad) credit behavior score should be assigned a
    good (bad) score, regardless of gender
* Q. What are advantages of equalized odds vs group fairness?
<!-- .element: class="fragment" -->
* Q. What are potential limitations of equalized odds?
<!-- .element: class="fragment" -->
</script></section><section data-markdown><script type="text/template">## Predictive Rate Parity

$P[Y=1∣R=1,A=a] = P[Y=1∣R=1,A=b]$
$P[Y=0∣R=0,A=a] = P[Y=0∣R=0,A=b]$

* Also called the _sufficiency_ criterion
* $Y \perp A | R$
  *  Target variable must be independent of the sensitive attribute
    _conditional_ on the prediction
* i.e., $R$ is alone sufficient to identify $Y$; no need to see $A$
<!-- .element: class="fragment" -->
</script></section><section data-markdown><script type="text/template">## Review: Recall vs Precision

![](confusion-matrix-full.jpg)

* Precision, or __Positive Predictive Value (PPV)__: How many of
  positively classified cases actually turned out to be positive?
</script></section><section data-markdown><script type="text/template">## Predictive Rate Parity

$P[Y=1∣R=1,A=a] = P[Y=1∣R=1,A=b]$
$P[Y=0∣R=0,A=a] = P[Y=0∣R=0,A=b]$

* Also called the _sufficiency_ criterion
* $Y \perp A | R$
  *  Target variable must be independent of the sensitive attribute
    _conditional_ on the prediction
* i.e., $R$ is alone sufficient to identify $Y$; no need to see $A$
  * or, __the model is equally precise across all groups__
* Example: Credit card rating
<!-- .element: class="fragment" -->
  * R: Credit card score, A: Gender of applicant: Y: Actual credit behavior
  * A person with a good (bad) predicted score should have good (bad)
    credit behavior, regardless of gender
* Q. What are potential limitations of predictive rate parity?
<!-- .element: class="fragment" -->
</script></section><section data-markdown><script type="text/template">## Impossibility Results

* Any two of the three definitions cannot be achieved simultaneously!
* e.g., Impossible to achieve equalized odds and predictive rate parity
  * $R \perp A | Y$ and $Y \perp A | R$ can't be true at the same time
  * Unless $A \perp Y$ 
  * Formal proofs: Chouldechova (2016), Kleinberg et al. (2016)
</script></section><section data-markdown><script type="text/template">## Case Study: Cancer Diagnosis

![](mri.jpg)
</script></section><section data-markdown><script type="text/template">## Exercise: Cancer Diagnosis

![](cancer-stats.jpg)

* 1000 data samples (500 male & 500 female patients)
* What's the overall recall & precision?
* Does the model achieve:
  * Group fairness? Equalized odds? Predictive rate parity?
* What can we conclude about the model & its usage?  
</script></section></section><section ><section data-markdown><script type="text/template"># Achieving Fairness Criteria
</script></section><section data-markdown><script type="text/template">## Can we achieve fairness during the learning process?

* Pre-processing:
  * Clean the dataset to reduce correlation between the feature set
    and sensitive attributes
* Training time constraint
  * ML is a constraint optimization problem (minimize errors)
  * Impose additional parity constraint into ML optimization process
* Post-processing
  * Adjust the learned model to be uncorrelated with sensitive attributes
* (Still active area of research! Many new techniques published each year)
</script></section><section data-markdown><script type="text/template">## Trade-offs: Accuracy vs Fairness

![](fairness-accuracy.jpeg)

* In general, accuracy is at odds with fairness
  * e.g., Impossible to achieve perfect accuracy ($R = Y$) while
  ensuring group parity

<!-- references -->

_Fairness Constraints: Mechanisms for Fair Classification_, Zafar et
al., AISTATS (2017).
  </script></section></section><section ><section data-markdown><script type="text/template"># Building Fair ML Systems
</script></section><section data-markdown><script type="text/template">## Fairness must be considered throughout the ML lifecycle!

![](fairness-lifecycle.jpg)

<!-- references -->

_Fairness-aware Machine Learning_, Bennett et al., WSDM Tutorial (2019).
</script></section></section><section ><section data-markdown><script type="text/template"># Requirements and Fairness
</script></section><section data-markdown><script type="text/template">## Machine Learning Cycle

![](ml-cycle.png)

<!-- references -->

"Fairness and Machine Learning" by Barocas, Hardt,
and Narayanan (2019), Chapter 1.
</script></section><section data-markdown><script type="text/template">## Recall: Machine vs World

![](machine-world.png)

* No ML/AI lives in vacuum; every system is deployed as part of the world
* A requirement describes a desired state of the world (i.e., environment)
* Machine (software) is _created_ to manipulate the environment into
  this state
</script></section><section data-markdown><script type="text/template">## Requirement vs Specification

![requirement-vs-spec](env-spec.png)

* Requirement (REQ): What the system should do, as desired effects on the environment
* Assumptions (ENV): What’s assumed about the behavior/properties of
  the environment (based on domain knowledge)
* Specification (SPEC): What the software must do in order to satisfy REQ
</script></section><section data-markdown><script type="text/template">## Case Study: College Admission

![](college-admission.jpg)
</script></section><section data-markdown><script type="text/template">## Requirements for Fair ML Systems

1. Identify all environmental entities
<!-- .element: class="fragment" -->
  * Consider all stakeholders, their backgrounds & characteristics
2. State requirement (REQ) over the environment
<!-- .element: class="fragment" -->
   * What functions should the system serve? Quality attributes?
   * But also: What kind of harms are possible & should be minimized?
   * Legal & policy requirements
</script></section><section data-markdown><script type="text/template">## "Four-fifth rule" (or "80% rule")

$(P[R = 1 | A = a]) / (P[R = 1 | A = b]) \geq 0.8$

* Selection rate for a protected group (e.g., $A = a$) <
80% of highest rate => selection procedure considered as having "adverse
impact"
* Guideline adopted by Federal agencies (Department of Justice, Equal
  Employment Opportunity Commission, etc.,) in 1978
* If violated, must justify business necessity (i.e., the selection procedure is
  essential to the safe & efficient operation)
* Example: Hiring
  * 50% of male applicants vs 20% female applicants hired
  (0.2/0.5 = 0.4)
  * Is there a business justification for hiring men at a higher rate?
</script></section><section data-markdown><script type="text/template">## Case Study: College Admission

![](college-admission.jpg)

* Who are the stakeholders?
* Types of harm?
* Legal & policy considerations?
</script></section><section data-markdown><script type="text/template">## Requirements for Fair ML Systems

1. Identify all environmental entities
2. State requirement (REQ) over the environment
3. Identify the interface between the environment & machine (ML)
<!-- .element: class="fragment" -->
  * What types of data will be sensed/measured by AI?
  * What types of actions will be performed by AI?
4. Identify the environmental assumptions (ENV)
<!-- .element: class="fragment" -->
	* How do stakeholders interact with the system?
	* Adversarial? Misuse? Unfair (dis-)advantages?
</script></section><section data-markdown><script type="text/template">## Case Study: College Admission

![](college-admission.jpg)

* Do certain groups of stakeholders have unfair (dis-)advantages that affect
their behavior?
* What types of data should the system measure?
  </script></section><section data-markdown><script type="text/template">## Requirements for Fair ML Systems

1. Identify all environmental entities
2. State requirement (REQ) over the environment
3. Identify the interface between the environment & machine (ML)
4. Identify the environmental assumptions (ENV)
5. Develop software specifications (SPEC) that are sufficient to
establish REQ
<!-- .element: class="fragment" -->
	* What type of fairness definition should we try to achieve?
6. Test whether ENV ∧ SPEC ⊧ REQ
<!-- .element: class="fragment" -->
	* Continually monitor the fairness metrics and user reports
</script></section><section data-markdown><script type="text/template">## Case Study: College Admission

![](college-admission.jpg)

* What type of fairness definition is appropriate?
  * Group fairness vs equalized odds? 
* How do we monitor if the system is being fair?
</script></section></section><section  data-markdown><script type="text/template"># Summary

* Definitions of fairness
  * Group fairness, equalized odds, predictive parity
* Achieving fairness
  * Trade-offs between accuracy & fairness
* Requirements for fair ML systems
</script></section></div>
    </div>

    <script src="./../js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './../lib/js/classList.js', condition: function() { return !document.body.classList; } },
        { src: './../plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './../plugin/zoom-js/zoom.js', async: true },
        { src: './../plugin/notes/notes.js', async: true },
        { src: './../plugin/math/math.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"theme":"white","slideNumber":true,"hash":true,"center":false}, queryOptions);
    </script>

    <script src="./../_assets/_assets/mermaid.min.js"></script>
    <script src="./../_assets/_assets/loadmymarkdown.js"></script>

    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
